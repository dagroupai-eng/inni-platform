"""
ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í—¬í¼ ëª¨ë“ˆ
ì—¬ëŸ¬ ê²€ìƒ‰ APIë¥¼ ì§€ì›í•˜ë©°, ê²€ìƒ‰ ê²°ê³¼ ìºì‹± ê¸°ëŠ¥ í¬í•¨
"""

import os
import json
import time
import hashlib
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import requests
from pathlib import Path

# ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = Path("cache/web_search")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# ìºì‹œ ìœ íš¨ ê¸°ê°„ (ì‹œê°„)
CACHE_EXPIRY_HOURS = 24


class WebSearchHelper:
    """ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, search_provider: str = "auto"):
        """
        Args:
            search_provider: ê²€ìƒ‰ ì œê³µì ("duckduckgo", "serper", "google", "auto")
        """
        self.search_provider = search_provider
        self._init_api_keys()
    
    def _init_api_keys(self):
        """API í‚¤ ì´ˆê¸°í™”"""
        try:
            import streamlit as st
            try:
                self.serper_api_key = st.secrets.get('SERPER_API_KEY') or os.environ.get('SERPER_API_KEY')
                self.google_api_key = st.secrets.get('GOOGLE_SEARCH_API_KEY') or os.environ.get('GOOGLE_SEARCH_API_KEY')
                self.google_cx = st.secrets.get('GOOGLE_SEARCH_CX') or os.environ.get('GOOGLE_SEARCH_CX')
            except (FileNotFoundError, AttributeError, KeyError):
                self.serper_api_key = os.environ.get('SERPER_API_KEY')
                self.google_api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
                self.google_cx = os.environ.get('GOOGLE_SEARCH_CX')
        except:
            self.serper_api_key = os.environ.get('SERPER_API_KEY')
            self.google_api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
            self.google_cx = os.environ.get('GOOGLE_SEARCH_CX')
        
        # ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ì œê³µì ì„ íƒ
        if self.search_provider == "auto":
            if self.serper_api_key:
                self.search_provider = "serper"
            elif self.google_api_key and self.google_cx:
                self.search_provider = "google"
            else:
                self.search_provider = "duckduckgo"
    
    def _get_cache_key(self, query: str, num_results: int = 5) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_str = f"{query}_{num_results}_{self.search_provider}"
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return CACHE_DIR / f"{cache_key}.json"
    
    def _load_cache(self, cache_key: str) -> Optional[List[Dict[str, Any]]]:
        """ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ"""
        cache_path = self._get_cache_path(cache_key)
        
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # ìºì‹œ ë§Œë£Œ í™•ì¸
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=CACHE_EXPIRY_HOURS):
                cache_path.unlink()  # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                return None
            
            return cache_data['results']
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_cache(self, cache_key: str, results: List[Dict[str, Any]]):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_path = self._get_cache_path(cache_key)
        
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'results': results
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def search_duckduckgo(self, query: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """DuckDuckGo ê²€ìƒ‰ (ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”)"""
        try:
            from duckduckgo_search import DDGS
            
            ddgs = DDGS()
            results = []
            
            # DuckDuckGo ê²€ìƒ‰
            search_results = ddgs.text(
                query,
                max_results=num_results,
                region='kr-ko'  # í•œêµ­ ì§€ì—­
            )
            
            for result in search_results:
                results.append({
                    'title': result.get('title', ''),
                    'snippet': result.get('body', ''),
                    'url': result.get('href', ''),
                    'source': 'DuckDuckGo'
                })
            
            return results
        except ImportError:
            print("DuckDuckGo ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'duckduckgo-search' íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return []
        except Exception as e:
            print(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_serper(self, query: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """Serper API ê²€ìƒ‰ (ìœ ë£Œ, í•˜ì§€ë§Œ ì •í™•í•¨)"""
        if not self.serper_api_key:
            print("Serper API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            url = "https://google.serper.dev/search"
            headers = {
                'X-API-KEY': self.serper_api_key,
                'Content-Type': 'application/json'
            }
            payload = {
                'q': query,
                'num': num_results
            }
            
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # organic ê²€ìƒ‰ ê²°ê³¼
            for item in data.get('organic', [])[:num_results]:
                results.append({
                    'title': item.get('title', ''),
                    'snippet': item.get('snippet', ''),
                    'url': item.get('link', ''),
                    'source': 'Serper API'
                })
            
            return results
        except Exception as e:
            print(f"Serper API ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_google(self, query: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """Google Custom Search API ê²€ìƒ‰"""
        if not self.google_api_key or not self.google_cx:
            print("Google Search API í‚¤ ë˜ëŠ” CXê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': query,
                'num': min(num_results, 10)  # Google APIëŠ” ìµœëŒ€ 10ê°œ
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for item in data.get('items', [])[:num_results]:
                results.append({
                    'title': item.get('title', ''),
                    'snippet': item.get('snippet', ''),
                    'url': item.get('link', ''),
                    'source': 'Google Custom Search'
                })
            
            return results
        except Exception as e:
            print(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search(self, query: str, num_results: int = 5, use_cache: bool = True) -> List[Dict[str, Any]]:
        """
        ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            num_results: ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ìºì‹œ í™•ì¸
        if use_cache:
            cache_key = self._get_cache_key(query, num_results)
            cached_results = self._load_cache(cache_key)
            if cached_results:
                print(f"âœ… ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ: {query}")
                return cached_results
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        print(f"ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query} (Provider: {self.search_provider})")
        
        if self.search_provider == "serper":
            results = self.search_serper(query, num_results)
        elif self.search_provider == "google":
            results = self.search_google(query, num_results)
        else:
            results = self.search_duckduckgo(query, num_results)
        
        # ìºì‹œ ì €ì¥
        if use_cache and results:
            cache_key = self._get_cache_key(query, num_results)
            self._save_cache(cache_key, results)
        
        return results
    
    def format_search_results(self, results: List[Dict[str, Any]], max_snippet_length: int = 300) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_snippet_length: ìŠ¤ë‹ˆí« ìµœëŒ€ ê¸¸ì´
        
        Returns:
            í¬ë§·íŒ…ëœ ë¬¸ìì—´
        """
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = "## ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼\n\n"
        
        for i, result in enumerate(results, 1):
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            snippet = result.get('snippet', 'ì„¤ëª… ì—†ìŒ')
            url = result.get('url', '')
            source = result.get('source', 'Unknown')
            
            # ìŠ¤ë‹ˆí« ê¸¸ì´ ì œí•œ
            if len(snippet) > max_snippet_length:
                snippet = snippet[:max_snippet_length] + "..."
            
            formatted += f"### {i}. {title}\n"
            formatted += f"**ì¶œì²˜:** {source}\n"
            formatted += f"**URL:** {url}\n"
            formatted += f"**ë‚´ìš©:** {snippet}\n\n"
        
        return formatted
    
    def get_citations_from_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ citations í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            citations ë¦¬ìŠ¤íŠ¸ (uri, title í¬í•¨)
        """
        citations = []
        for result in results:
            url = result.get('url', '')
            title = result.get('title', '')
            if url:  # URLì´ ìˆëŠ” ê²½ìš°ë§Œ citationsì— ì¶”ê°€
                citations.append({
                    'uri': url,
                    'title': title or url,  # ì œëª©ì´ ì—†ìœ¼ë©´ URL ì‚¬ìš©
                    'source': result.get('source', 'Web Search'),
                    'snippet': result.get('snippet', '')
                })
        return citations
    
    def search_multiple_queries(self, queries: List[str], num_results_per_query: int = 3) -> Dict[str, List[Dict[str, Any]]]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë™ì‹œì— ìˆ˜í–‰
        
        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            num_results_per_query: ì¿¼ë¦¬ë‹¹ ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results_dict = {}
        
        for query in queries:
            results = self.search(query, num_results=num_results_per_query)
            results_dict[query] = results
            # API í˜¸ì¶œ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.5)
        
        return results_dict


def get_web_search_context(block_id: str, project_info: Dict[str, Any], pdf_text: str = "") -> Optional[str]:
    """
    ë¸”ë¡ IDì— ë”°ë¼ ì ì ˆí•œ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ë° ê²€ìƒ‰ ìˆ˜í–‰
    
    Args:
        block_id: ë¶„ì„ ë¸”ë¡ ID
        project_info: í”„ë¡œì íŠ¸ ì •ë³´
        pdf_text: PDF ë¬¸ì„œ ë‚´ìš© (ì„ íƒì‚¬í•­)
    
    Returns:
        í¬ë§·íŒ…ëœ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´ ë˜ëŠ” None
    """
    web_search_queries = {
        'market_research_analysis': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ê¸€ë¡œë²Œ ì‹œì¥ ê·œëª¨",
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} êµ­ë‚´ ì‹œì¥ í˜„í™©",
            f"{project_info.get('location', '')} ìŠ¤í¬ì¸  ì‹œì„¤ ë²¤ì¹˜ë§ˆí‚¹",
            "ìŠ¤í¬ì¸  ì¬í™œ ê´€ê´‘ ì‚°ì—… íŠ¸ë Œë“œ"
        ],
        'business_model_development': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸",
            f"{project_info.get('location', '')} ì§€ì—­ ê²½ì œ ì—°ê³„ ì‚°ì—…",
            "ìŠ¤í¬ì¸  IP ì‚¬ì—… ì‚¬ë¡€",
            "ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸ ìˆ˜ìµ ëª¨ë¸"
        ],
        'revenue_model_design': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ìˆ˜ìµ êµ¬ì¡°",
            "ìŠ¤í¬ì¸  ì‹œì„¤ ìš´ì˜ ìˆ˜ìµ ëª¨ë¸",
            "ì „ì§€í›ˆë ¨ ìœ ì¹˜ ì „ëµ"
        ]
    }
    
    queries = web_search_queries.get(block_id)
    if not queries:
        return None
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    search_helper = WebSearchHelper()
    all_results = []
    
    for query in queries:
        results = search_helper.search(query, num_results=3)
        all_results.extend(results)
    
    if not all_results:
        return None
    
    # ê²°ê³¼ í¬ë§·íŒ…
    return search_helper.format_search_results(all_results, max_snippet_length=250)


def get_web_search_citations(block_id: str, project_info: Dict[str, Any], pdf_text: str = "") -> List[Dict[str, Any]]:
    """
    ë¸”ë¡ IDì— ë”°ë¼ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  citations í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    
    Args:
        block_id: ë¶„ì„ ë¸”ë¡ ID
        project_info: í”„ë¡œì íŠ¸ ì •ë³´
        pdf_text: PDF ë¬¸ì„œ ë‚´ìš© (ì„ íƒì‚¬í•­)
    
    Returns:
        citations ë¦¬ìŠ¤íŠ¸ (uri, title í¬í•¨)
    """
    web_search_queries = {
        'market_research_analysis': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ê¸€ë¡œë²Œ ì‹œì¥ ê·œëª¨",
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} êµ­ë‚´ ì‹œì¥ í˜„í™©",
            f"{project_info.get('location', '')} ìŠ¤í¬ì¸  ì‹œì„¤ ë²¤ì¹˜ë§ˆí‚¹",
            "ìŠ¤í¬ì¸  ì¬í™œ ê´€ê´‘ ì‚°ì—… íŠ¸ë Œë“œ"
        ],
        'business_model_development': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸",
            f"{project_info.get('location', '')} ì§€ì—­ ê²½ì œ ì—°ê³„ ì‚°ì—…",
            "ìŠ¤í¬ì¸  IP ì‚¬ì—… ì‚¬ë¡€",
            "ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸ ìˆ˜ìµ ëª¨ë¸"
        ],
        'revenue_model_design': [
            f"{project_info.get('project_type', 'ìŠ¤í¬ì¸  ì•„ì¹´ë°ë¯¸')} ìˆ˜ìµ êµ¬ì¡°",
            "ìŠ¤í¬ì¸  ì‹œì„¤ ìš´ì˜ ìˆ˜ìµ ëª¨ë¸",
            "ì „ì§€í›ˆë ¨ ìœ ì¹˜ ì „ëµ"
        ]
    }
    
    queries = web_search_queries.get(block_id)
    if not queries:
        return []
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    search_helper = WebSearchHelper()
    all_results = []
    
    for query in queries:
        results = search_helper.search(query, num_results=3)
        all_results.extend(results)
    
    if not all_results:
        return []
    
    # citations í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return search_helper.get_citations_from_results(all_results)

