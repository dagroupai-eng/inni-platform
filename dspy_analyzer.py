import os
# DSPy ìºì‹œ ë¹„í™œì„±í™” (import ì „ì— ì„¤ì •í•´ì•¼ í•¨)
os.environ['DSP_CACHEBOOL'] = 'false'
os.environ['DSPY_CACHEBOOL'] = 'false'

import dspy
import json
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Type, Callable, Tuple
from dotenv import load_dotenv
from web_search_helper import get_web_search_context, WebSearchHelper
# get_web_search_citationsëŠ” ì„ íƒì  import (í•¨ìˆ˜ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
try:
    from web_search_helper import get_web_search_citations
    WEB_SEARCH_CITATIONS_AVAILABLE = True
except ImportError:
    WEB_SEARCH_CITATIONS_AVAILABLE = False
    get_web_search_citations = None
from prompt_processor import process_prompt, UNIFIED_PROMPT_TEMPLATE

# Pydantic ì§€ì› (ì„ íƒì )
try:
    from pydantic import BaseModel
    PYDANTIC_AVAILABLE = True
except ImportError:
    PYDANTIC_AVAILABLE = False
    BaseModel = None

# RAG ê¸°ëŠ¥ (ì„ íƒì  ì‚¬ìš©)
try:
    from rag_helper import (
        build_rag_system_for_documents,
        query_rag_system,
        retrieve_relevant_contexts
    )
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("âš ï¸ RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ embedding_helper.pyì™€ rag_helper.pyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

try:  # Python 3.11+
    import tomllib
except ModuleNotFoundError:  # pragma: no cover
    import tomli as tomllib

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ í•¨ìˆ˜
def _load_streamlit_secrets_into_env():
    """
    Streamlit secrets.toml ê°’ì„ ì¼ë°˜ ì‹¤í–‰ í™˜ê²½ ë³€ìˆ˜ë¡œ ì£¼ì…í•©ë‹ˆë‹¤.
    Streamlitì´ ì•„ë‹Œ CLI/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œë„ ë™ì¼í•œ API í‚¤ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    """
    secrets_path = Path(__file__).resolve().parent / ".streamlit" / "secrets.toml"
    if not secrets_path.exists():
        return

    try:
        with secrets_path.open("rb") as f:
            data = tomllib.load(f)
    except Exception:
        return

    # Streamlit ê¸°ë³¸ êµ¬ì¡°ëŠ” [secrets] ì„¹ì…˜ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ë£¨íŠ¸ í‚¤ì¼ ìˆ˜ë„ ìˆìŒ
    secrets_block = data.get("secrets", data)
    if not isinstance(secrets_block, dict):
        return

    for key, value in secrets_block.items():
        if isinstance(value, str) and not os.environ.get(key):
            os.environ[key] = value

# í™˜ê²½ë³€ìˆ˜ ë° secrets ë¡œë“œ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
_load_streamlit_secrets_into_env()
try:
    load_dotenv()
except UnicodeDecodeError:
    # .env íŒŒì¼ì— ì¸ì½”ë”© ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ë¬´ì‹œ
    pass

# API ì œê³µìë³„ ì„¤ì • ì •ë³´ (Gemini 2.5 Pro ê³ ì„±ëŠ¥ ëª¨ë¸)
PROVIDER_CONFIG = {
    'gemini': {
        'api_key_env': 'GEMINI_API_KEY',  # Google AI Studio API í‚¤
        'model': 'gemini-2.5-pro',  # LiteLLM í˜•ì‹: gemini/gemini-2.5-pro
        'provider': 'gemini',  # Google AI Studio (API Key)
        'display_name': 'Gemini 2.5 Pro'
    },
    'gemini_3pro': {
        'api_key_env': 'GEMINI_API_KEY',  # Google AI Studio API í‚¤
        'model': 'gemini-3-pro-preview',  # ê°€ì¥ ì§€ëŠ¥ì ì¸ ëª¨ë¸
        'provider': 'gemini',  # Google AI Studio (API Key)
        'display_name': 'Gemini 3 Pro'
    },
    'gemini_25flash': {
        'api_key_env': 'GEMINI_API_KEY',  # Google AI Studio API í‚¤
        'model': 'gemini-2.5-flash',  # ë¹ ë¥´ê³  ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ ìš°ìˆ˜
        'provider': 'gemini',  # Google AI Studio (API Key)
        'display_name': 'Gemini 2.5 Flash'
    },
    'gemini_3flash': {
        'api_key_env': 'GEMINI_API_KEY',  # Google AI Studio API í‚¤
        'model': 'gemini-3-flash-preview',  # Gemini 3.0 Flash
        'provider': 'gemini',  # Google AI Studio (API Key)
        'display_name': 'Gemini 3.0 Flash'
    }
}

# í”¼ë“œë°± ìœ í˜• ë¶„ë¥˜
FEEDBACK_TYPES = {
    'perspective_shift': {
        'name': 'ê´€ì  ë¶€ì¡±',
        'description': 'ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬ë¶„ì„ (í™˜ê²½, ê²½ì œ, ì‚¬íšŒ, ê¸°ìˆ  ë“±)',
        'hint': 'ì˜ˆ: "í™˜ê²½ì  ì¸¡ë©´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤", "ê²½ì œì„± ë¶„ì„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”"',
        'keywords': ['ê´€ì ', 'ì¸¡ë©´', 'ì‹œê°', 'ê³ ë ¤', 'ë¶€ì¡±', 'ëˆ„ë½', 'ì¶”ê°€']
    },
    'constraint_addition': {
        'name': 'ì œì•½ì¡°ê±´ ì¶”ê°€',
        'description': 'ì˜ˆì‚°, ê·œëª¨, ë²•ê·œ, ì¼ì • ë“± ì œì•½ì‚¬í•­ ë°˜ì˜',
        'hint': 'ì˜ˆ: "ì˜ˆì‚° 30ì–µ ì´í•˜ë¡œ", "6ê°œì›” ë‚´ ì™„ê³µ ê°€ëŠ¥í•˜ë„ë¡"',
        'keywords': ['ì˜ˆì‚°', 'ë¹„ìš©', 'ê·œëª¨', 'ë²•ê·œ', 'ê·œì •', 'ì¼ì •', 'ê¸°í•œ', 'ì œí•œ', 'ì´í•˜', 'ì´ìƒ', 'ë²”ìœ„']
    },
    'scope_expansion': {
        'name': 'ë²”ìœ„ í™•ì¥',
        'description': 'ì¶”ê°€ ë¶„ì„ ì˜ì—­ ìš”ì²­',
        'hint': 'ì˜ˆ: "ì£¼ë³€ êµí†µ ì˜í–¥ë„ ë¶„ì„í•´ì£¼ì„¸ìš”", "ì¸ê·¼ ì‹œì„¤ê³¼ì˜ ì—°ê³„ë°©ì•ˆ"',
        'keywords': ['ì¶”ê°€', 'í™•ì¥', 'í¬í•¨', 'í•¨ê»˜', 'ì—°ê³„', 'ì£¼ë³€', 'ë”']
    },
    'correction': {
        'name': 'ì˜¤ë¥˜ ìˆ˜ì •',
        'description': 'ì˜ëª»ëœ ë‚´ìš© ìˆ˜ì • ë˜ëŠ” ì‚¬ì‹¤ê´€ê³„ ì •ì •',
        'hint': 'ì˜ˆ: "ë©´ì ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤", "ë²•ê·œ í•´ì„ì´ í‹€ë ¸ìŠµë‹ˆë‹¤"',
        'keywords': ['ì˜ëª»', 'ì˜¤ë¥˜', 'í‹€ë¦¼', 'ìˆ˜ì •', 'ì •ì •', 'ì•„ë‹™ë‹ˆë‹¤', 'ì•„ë‹ˆë¼']
    },
    'direction_change': {
        'name': 'ë°©í–¥ ì „í™˜',
        'description': 'ë¶„ì„ ë°©í–¥ ìì²´ë¥¼ ë³€ê²½',
        'hint': 'ì˜ˆ: "ì´ ë°©í–¥ì€ ì–´ë µìŠµë‹ˆë‹¤. ëŒ€ì•ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”"',
        'keywords': ['ì•ˆë©ë‹ˆë‹¤', 'ì–´ë µ', 'ë¶ˆê°€ëŠ¥', 'ëŒ€ì•ˆ', 'ë‹¤ë¥¸', 'ë°©í–¥', 'ì „í™˜']
    }
}


def parse_feedback_intent(feedback_text: str, feedback_type: Optional[str] = None) -> Dict[str, Any]:
    """
    í”¼ë“œë°± í…ìŠ¤íŠ¸ì—ì„œ ì˜ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        feedback_text: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”¼ë“œë°± í…ìŠ¤íŠ¸
        feedback_type: ì„ íƒëœ í”¼ë“œë°± ìœ í˜• (Noneì´ë©´ ìë™ ê°ì§€)

    Returns:
        ë¶„ì„ëœ í”¼ë“œë°± ì˜ë„ ì •ë³´:
        - type: í”¼ë“œë°± ìœ í˜•
        - missing_perspectives: ë¶€ì¡±í•œ ê´€ì  ëª©ë¡
        - constraints: ì¶”ì¶œëœ ì œì•½ì¡°ê±´
        - rejection_reason: ê±°ë¶€ ì´ìœ  (ìˆëŠ” ê²½ìš°)
        - additional_scope: ì¶”ê°€ ë¶„ì„ ë²”ìœ„
        - correction_points: ìˆ˜ì • í•„ìš” ì‚¬í•­
    """
    result = {
        'type': feedback_type,
        'original_text': feedback_text,
        'missing_perspectives': [],
        'constraints': [],
        'rejection_reason': None,
        'additional_scope': [],
        'correction_points': []
    }

    if not feedback_text:
        return result

    feedback_lower = feedback_text.lower()

    # ìë™ ìœ í˜• ê°ì§€ (feedback_typeì´ ì—†ëŠ” ê²½ìš°)
    if not feedback_type:
        max_score = 0
        detected_type = None
        for ftype, info in FEEDBACK_TYPES.items():
            score = sum(1 for kw in info['keywords'] if kw in feedback_lower)
            if score > max_score:
                max_score = score
                detected_type = ftype
        result['type'] = detected_type or 'general'

    # ê´€ì  í‚¤ì›Œë“œ ì¶”ì¶œ
    perspective_keywords = {
        'í™˜ê²½': 'í™˜ê²½ì  ì˜í–¥ ë¶„ì„',
        'ê²½ì œ': 'ê²½ì œì„± ë¶„ì„',
        'ì‚¬íšŒ': 'ì‚¬íšŒì  ì˜í–¥ ë¶„ì„',
        'ê¸°ìˆ ': 'ê¸°ìˆ ì  íƒ€ë‹¹ì„±',
        'ë²•ê·œ': 'ë²•ê·œ ê²€í† ',
        'ì•ˆì „': 'ì•ˆì „ì„± ë¶„ì„',
        'ì ‘ê·¼ì„±': 'ì ‘ê·¼ì„± ë¶„ì„',
        'ì§€ì†ê°€ëŠ¥': 'ì§€ì†ê°€ëŠ¥ì„± ë¶„ì„',
        'êµí†µ': 'êµí†µ ì˜í–¥ ë¶„ì„',
        'ë¬¸í™”': 'ë¬¸í™”ì  ê°€ì¹˜ ë¶„ì„'
    }
    for kw, perspective in perspective_keywords.items():
        if kw in feedback_text:
            result['missing_perspectives'].append(perspective)

    # ì œì•½ì¡°ê±´ ì¶”ì¶œ (ìˆ«ì + ë‹¨ìœ„ íŒ¨í„´)
    import re
    constraint_patterns = [
        r'(\d+(?:,\d+)*(?:\.\d+)?)\s*(ì–µ|ë§Œì›|ì›|í‰|ã¡|mÂ²|ì¸µ|ê°œì›”|ë…„|ì¼)',
        r'(ì˜ˆì‚°|ë¹„ìš©|ë©´ì |ê·œëª¨|ë†’ì´|ê¸°ê°„)\s*[:ï¼š]?\s*(\d+(?:,\d+)*(?:\.\d+)?)\s*(ì–µ|ë§Œì›|ì›|í‰|ã¡|mÂ²|ì¸µ|ê°œì›”|ë…„|ì¼)?',
        r'(\d+(?:,\d+)*(?:\.\d+)?)\s*(ì–µ|ë§Œì›|ì›)\s*(ì´í•˜|ì´ìƒ|ë¯¸ë§Œ|ì´ˆê³¼|ë‚´ì™¸)'
    ]
    for pattern in constraint_patterns:
        matches = re.findall(pattern, feedback_text)
        for match in matches:
            constraint_str = ' '.join(str(m) for m in match if m)
            if constraint_str and constraint_str not in result['constraints']:
                result['constraints'].append(constraint_str)

    # ê±°ë¶€/ë°©í–¥ì „í™˜ ì´ìœ  ì¶”ì¶œ
    rejection_keywords = ['ì•ˆë©ë‹ˆë‹¤', 'ì–´ë µìŠµë‹ˆë‹¤', 'ë¶ˆê°€ëŠ¥', 'ëª»í•©ë‹ˆë‹¤', 'ì•ˆ ë©ë‹ˆë‹¤']
    for kw in rejection_keywords:
        if kw in feedback_text:
            # í•´ë‹¹ ë¬¸ì¥ ì¶”ì¶œ
            sentences = feedback_text.split('.')
            for sent in sentences:
                if kw in sent:
                    result['rejection_reason'] = sent.strip()
                    break
            break

    # ì¶”ê°€ ë²”ìœ„ ì¶”ì¶œ
    scope_keywords = ['ì¶”ê°€ë¡œ', 'í•¨ê»˜', 'í¬í•¨í•´ì„œ', 'ì—°ê³„í•˜ì—¬', 'ë”ë¶ˆì–´']
    for kw in scope_keywords:
        if kw in feedback_text:
            idx = feedback_text.find(kw)
            # í‚¤ì›Œë“œ ì´í›„ ë¬¸ì¥ ì¶”ì¶œ
            remainder = feedback_text[idx:].split('.')[0]
            if remainder:
                result['additional_scope'].append(remainder.strip())

    return result


def build_contextual_feedback_prompt(
    feedback_intent: Dict[str, Any],
    previous_result: str,
    block_info: Dict[str, Any]
) -> str:
    """
    í”¼ë“œë°± ì˜ë„ì— ë”°ë¥¸ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        feedback_intent: parse_feedback_intent()ì˜ ê²°ê³¼
        previous_result: ì´ì „ ë¶„ì„ ê²°ê³¼
        block_info: í˜„ì¬ ë¸”ë¡ ì •ë³´

    Returns:
        ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”¼ë“œë°± í”„ë¡¬í”„íŠ¸
    """
    feedback_type = feedback_intent.get('type', 'general')
    original_feedback = feedback_intent.get('original_text', '')

    prompt_parts = []

    # ê¸°ë³¸ í”¼ë“œë°± ì„¹ì…˜
    prompt_parts.append("### ğŸ” ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„\n")

    # í”¼ë“œë°± ìœ í˜•ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­
    if feedback_type == 'perspective_shift':
        perspectives = feedback_intent.get('missing_perspectives', [])
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ê´€ì  ë¶€ì¡± - ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬ë¶„ì„ í•„ìš”\n")
        if perspectives:
            prompt_parts.append(f"**ì¶”ê°€ í•„ìš” ê´€ì **: {', '.join(perspectives)}\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ì´ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ìœ„ì—ì„œ ì§€ì ëœ ê´€ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¬ë¶„ì„í•˜ì„¸ìš”.
2. ê¸°ì¡´ ë¶„ì„ì—ì„œ ëˆ„ë½ëœ ì¸¡ë©´ì„ ë³´ì™„í•˜ì—¬ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
3. ìƒˆë¡œìš´ ê´€ì ì—ì„œ ë°œê²¬ë˜ëŠ” ì´ìŠˆì™€ ê¸°íšŒë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”.
""")

    elif feedback_type == 'constraint_addition':
        constraints = feedback_intent.get('constraints', [])
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ì œì•½ì¡°ê±´ ì¶”ê°€ - ìƒˆë¡œìš´ ì œì•½ì‚¬í•­ ë°˜ì˜ í•„ìš”\n")
        if constraints:
            prompt_parts.append(f"**ì ìš©í•  ì œì•½ì¡°ê±´**: {', '.join(constraints)}\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ìœ„ì˜ ì œì•½ì¡°ê±´ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ë¶„ì„ì„ ìˆ˜ì •í•˜ì„¸ìš”.
2. ì œì•½ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ë³€ê²½ë˜ëŠ” ë¶€ë¶„ì„ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.
3. ì œì•½ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ê³ , ì¶©ì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
""")

    elif feedback_type == 'scope_expansion':
        additional_scope = feedback_intent.get('additional_scope', [])
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ë²”ìœ„ í™•ì¥ - ì¶”ê°€ ë¶„ì„ ì˜ì—­ ìš”ì²­\n")
        if additional_scope:
            prompt_parts.append(f"**ì¶”ê°€ ë¶„ì„ ë²”ìœ„**: {'; '.join(additional_scope)}\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìš”ì²­ëœ ì¶”ê°€ ë²”ìœ„ë¥¼ ë¶„ì„ì— í¬í•¨í•˜ì„¸ìš”.
2. ì¶”ê°€ëœ ë²”ìœ„ì™€ ê¸°ì¡´ ë¶„ì„ ê°„ì˜ ì—°ê´€ì„±ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
3. í™•ì¥ëœ ë²”ìœ„ì—ì„œ ìƒˆë¡­ê²Œ ë°œê²¬ë˜ëŠ” ì‹œì‚¬ì ì„ ì œì‹œí•˜ì„¸ìš”.
""")

    elif feedback_type == 'correction':
        correction_points = feedback_intent.get('correction_points', [])
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ì˜¤ë¥˜ ìˆ˜ì • - ì˜ëª»ëœ ë‚´ìš© ì •ì • í•„ìš”\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ì‚¬ìš©ìê°€ ì§€ì í•œ ì˜¤ë¥˜ ì‚¬í•­ì„ ì£¼ì˜ ê¹Šê²Œ ê²€í† í•˜ì„¸ìš”.
2. ì˜ëª»ëœ ë¶€ë¶„ì„ ì •í™•í•œ ì •ë³´ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ìˆ˜ì •ëœ ë‚´ìš©ì´ ì „ì²´ ë¶„ì„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë°˜ì˜í•˜ì„¸ìš”.
4. ìˆ˜ì • ì‚¬í•­ì„ ëª…í™•íˆ í‘œì‹œí•˜ì—¬ ë³€ê²½ì ì„ ì•Œ ìˆ˜ ìˆê²Œ í•˜ì„¸ìš”.
""")

    elif feedback_type == 'direction_change':
        rejection_reason = feedback_intent.get('rejection_reason', '')
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ë°©í–¥ ì „í™˜ - ë¶„ì„ ë°©í–¥ ë³€ê²½ ìš”ì²­\n")
        if rejection_reason:
            prompt_parts.append(f"**ì‚¬ìš©ìê°€ ì œì‹œí•œ ì´ìœ **: {rejection_reason}\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ì‚¬ìš©ìê°€ í˜„ì¬ ë°©í–¥ì´ ì–´ë ¤ìš´ ì´ìœ ë¥¼ ì´í•´í•˜ê³  ìˆ˜ìš©í•˜ì„¸ìš”.
2. ì™„ì „íˆ ìƒˆë¡œìš´ ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ì„¸ìš”.
3. ëŒ€ì•ˆì´ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.
4. ì—¬ëŸ¬ ëŒ€ì•ˆì´ ìˆë‹¤ë©´ ê°ê°ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•˜ì„¸ìš”.
""")

    else:  # general
        prompt_parts.append("**í”¼ë“œë°± ìœ í˜•**: ì¼ë°˜ í”¼ë“œë°±\n")
        prompt_parts.append("""
**ì¬ë¶„ì„ ì§€ì‹œì‚¬í•­**:
1. ì‚¬ìš©ì í”¼ë“œë°±ì˜ í•µì‹¬ ìš”ì²­ì‚¬í•­ì„ íŒŒì•…í•˜ì„¸ìš”.
2. ìš”ì²­ì— ë”°ë¼ ë¶„ì„ì„ ë³´ì™„í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ì„¸ìš”.
3. ë³€ê²½ëœ ë‚´ìš©ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
""")

    # ì›ë³¸ í”¼ë“œë°± í¬í•¨
    prompt_parts.append(f"\n**ì›ë³¸ í”¼ë“œë°±**: {original_feedback}\n")

    # ì´ì „ ê²°ê³¼ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
    if previous_result:
        summary_length = min(len(previous_result), 1500)
        prompt_parts.append(f"\n**ì´ì „ ë¶„ì„ ê²°ê³¼ ìš”ì•½**:\n{previous_result[:summary_length]}")
        if len(previous_result) > summary_length:
            prompt_parts.append("\n[ì´ì „ ê²°ê³¼ ì¼ë¶€ ìƒëµ...]")

    return '\n'.join(prompt_parts)


# API í‚¤ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_api_key(provider: str) -> Optional[str]:
    """
    ì„ íƒëœ ì œê³µìì— ë§ëŠ” API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: 1) ì„¸ì…˜ ìƒíƒœ -> 2) DB ì €ì¥ í‚¤ -> 3) Streamlit secrets -> 4) í™˜ê²½ë³€ìˆ˜

    Args:
        provider: ì œê³µì ì´ë¦„ ('anthropic', 'openai', 'gemini', 'deepseek')

    Returns:
        API í‚¤ ë¬¸ìì—´ ë˜ëŠ” None (Vertex AIëŠ” None ë°˜í™˜)
    """
    if provider not in PROVIDER_CONFIG:
        return None

    config = PROVIDER_CONFIG[provider]
    api_key_env = config.get('api_key_env')

    # Vertex AIëŠ” API í‚¤ ë¶ˆí•„ìš” (ADC ì‚¬ìš©)
    if not api_key_env:
        return None

    try:
        import streamlit as st
        # 1. ë¨¼ì € ì„¸ì…˜ ìƒíƒœì—ì„œ í™•ì¸ (ì‚¬ìš©ìê°€ ì›¹ì—ì„œ ì…ë ¥í•œ í‚¤)
        session_key = f'user_api_key_{api_key_env}'
        if session_key in st.session_state and st.session_state[session_key]:
            return st.session_state[session_key]

        # 2. DBì— ì €ì¥ëœ ì‚¬ìš©ìë³„ API í‚¤ í™•ì¸
        try:
            from security.api_key_manager import get_api_key_for_current_user
            db_api_key = get_api_key_for_current_user(api_key_env)
            if db_api_key:
                return db_api_key
        except ImportError:
            pass

        # 3. Streamlit secretsì—ì„œ í™•ì¸ (secrets íŒŒì¼ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        # 4. í™˜ê²½ë³€ìˆ˜ì—ì„œ í™•ì¸
        try:
            api_key = st.secrets.get(api_key_env) or os.environ.get(api_key_env)
        except (FileNotFoundError, AttributeError, KeyError):
            api_key = os.environ.get(api_key_env)
    except Exception:
        # Streamlitì´ ì—†ëŠ” í™˜ê²½ (ì˜ˆ: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
        api_key = os.environ.get(api_key_env)

    return api_key

# í˜„ì¬ ì œê³µì ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_current_provider() -> str:
    """
    í˜„ì¬ ì„ íƒëœ ì œê³µìë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ 'gemini_25flash'ì…ë‹ˆë‹¤.
    
    Returns:
        ì œê³µì ì´ë¦„
    """
    env_override = os.environ.get('LLM_PROVIDER')
    
    # í™˜ê²½ë³€ìˆ˜ ìš°ì„  (CLI/í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë“±ì„ ìœ„í•´)
    if env_override and env_override in PROVIDER_CONFIG:
        return env_override
    
    try:
        import streamlit as st
        provider = st.session_state.get('llm_provider', env_override or 'gemini_25flash')
        if provider not in PROVIDER_CONFIG:
            provider = 'gemini_25flash'
        return provider
    except Exception:
        # Streamlitì´ ì—†ëŠ” í™˜ê²½
        if env_override and env_override in PROVIDER_CONFIG:
            return env_override
        return 'gemini'

# ê°œì„ ëœ Signature ì •ì˜
class SimpleAnalysisSignature(dspy.Signature):
    """Chain of Thought ê¸°ë°˜ ì¢…í•© ë¶„ì„ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="í”„ë¡œì íŠ¸ ë¬¸ì„œ ë° ë¶„ì„ ìš”êµ¬ì‚¬í•­")
    output = dspy.OutputField(desc="ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ í¬í•¨í•œ ì²´ê³„ì ì¸ ë¶„ì„ ê²°ê³¼")

class BasicInfoSignature(dspy.Signature):
    """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="í”„ë¡œì íŠ¸ëª…, ìœ„ì¹˜, ê·œëª¨, ëª©í‘œ, ì£¼ìš” íŠ¹ì§•ì„ í¬í•¨í•œ ì²´ê³„ì ì¸ ê¸°ë³¸ ì •ë³´ í‘œ")

class RequirementsSignature(dspy.Signature):
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ê±´ì¶• ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ìš”êµ¬ì‚¬í•­ ë§¤íŠ¸ë¦­ìŠ¤, ìš°ì„ ìˆœìœ„ ë„í‘œ, ì„¤ê³„ ë°©í–¥ì„ í¬í•¨í•œ ì¢…í•© ë¶„ì„")

class DesignSignature(dspy.Signature):
    """ì„¤ê³„ ì œì•ˆì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ì„¤ê³„ ë°©í–¥ì„ ì œì•ˆí•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ì„¤ê³„ ì›ì¹™, ê³µê°„ êµ¬ì„±ì•ˆ, ì‹¤í–‰ ë‹¨ê³„ë¥¼ í¬í•¨í•œ êµ¬ì²´ì ì¸ ì„¤ê³„ ì œì•ˆ")

class AccessibilitySignature(dspy.Signature):
    """ì ‘ê·¼ì„± í‰ê°€ë¥¼ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ì ‘ê·¼ì„±ì„ í‰ê°€í•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ì ‘ê·¼ì„± ë§¤íŠ¸ë¦­ìŠ¤, ê°œì„  ë°©ì•ˆ, ì ìˆ˜ í‰ê°€ë¥¼ í¬í•¨í•œ ì¢…í•© í‰ê°€")

class ZoningSignature(dspy.Signature):
    """ë²•ê·œ ê²€ì¦ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ë²•ê·œë¥¼ ê²€ì¦í•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ë²•ê·œ ì¤€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸, ìœ„í—˜ìš”ì†Œ ë¶„ì„, ëŒ€ì‘ ë°©ì•ˆì„ í¬í•¨í•œ ê²€ì¦ ê²°ê³¼")

class CapacitySignature(dspy.Signature):
    """ìˆ˜ìš©ë ¥ ì¶”ì •ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ìˆ˜ìš©ë ¥ì„ ì¶”ì •í•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ë¬¼ë¦¬ì /ë²•ì /ì‚¬íšŒì /ê²½ì œì  ìˆ˜ìš©ë ¥ ë¶„ì„í‘œì™€ ìµœì  ê°œë°œ ê·œëª¨ ì œì•ˆ")

class FeasibilitySignature(dspy.Signature):
    """ì‚¬ì—…ì„± í‰ê°€ë¥¼ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ì‚¬ì—…ì„±ì„ í‰ê°€í•  ë¬¸ì„œ")
    output = dspy.OutputField(desc="ì‹œì¥ì„±, ê¸°ìˆ ì„±, ê²½ì œì„±, ë²•ê·œì„± í‰ê°€í‘œì™€ ì¢…í•© ì‚¬ì—…ì„± ì ìˆ˜")

class AnalysisQualityValidator(dspy.Signature):
    """ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ì„ ìœ„í•œ Signature"""
    analysis_result = dspy.InputField(desc="ê²€ì¦í•  ë¶„ì„ ê²°ê³¼")
    validation_criteria = dspy.InputField(desc="í’ˆì§ˆ ê²€ì¦ ê¸°ì¤€")
    output = dspy.OutputField(desc="í’ˆì§ˆ ì ìˆ˜, ê°œì„  ì‚¬í•­, ì™„ì„±ë„ í‰ê°€ë¥¼ í¬í•¨í•œ ê²€ì¦ ê²°ê³¼")

class ê±´ì¶•ìš”êµ¬ì‚¬í•­ë¶„ì„CotSignature(dspy.Signature):
    """ê±´ì¶• ìš”êµ¬ì‚¬í•­ ë¶„ì„ (CoT)ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ê±´ì¶• ìš”êµ¬ì‚¬í•­ ë¶„ì„ (CoT)ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„°")
    output = dspy.OutputField(desc="Chain of Thoughtë¡œ ê±´ì¶• ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³  ì •ë¦¬í•œ ê²°ê³¼")

class ê±´ì¶•ìš”êµ¬ì‚¬í•­ë¶„ì„22Signature(dspy.Signature):
    """ê±´ì¶• ìš”êµ¬ì‚¬í•­ ë¶„ì„22ì„ ìœ„í•œ Signature"""
    input = dspy.InputField(desc="ê±´ì¶• ìš”êµ¬ì‚¬í•­ ë¶„ì„22ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„°")
    output = dspy.OutputField(desc="Chain of Thoughtë¡œ ê±´ì¶• ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³  ì •ë¦¬í•œ ê²°ê³¼")

class EnhancedArchAnalyzer:
    """dA_AIì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ DSPyë¥¼ ì‚¬ìš©í•˜ëŠ” ê±´ì¶• ë¶„ì„ê¸°"""
    
    _lm_initialized = False
    _last_provider = None
    
    def __init__(self, use_gemini_native_pdf: bool = False):
        """
        DSPy ì„¤ì • ì´ˆê¸°í™” (dA_AIì™€ ë™ì¼í•œ ë°©ì‹)
        
        Args:
            use_gemini_native_pdf: PDF ì²˜ë¦¬ë¥¼ Gemini ë„¤ì´í‹°ë¸Œ ë°©ì‹ìœ¼ë¡œ í• ì§€ ì—¬ë¶€
                                  (ì´ë¯¸ì§€, ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸ê¹Œì§€ ì´í•´)
        """
        self._provider_lms: Dict[str, Any] = {}
        self._active_provider: Optional[str] = None
        self.use_gemini_native_pdf = use_gemini_native_pdf
        try:
            self.setup_dspy()
        except Exception as e:
            # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê°ì²´ëŠ” ìƒì„± (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥)
            print(f"âš ï¸ DSPy ì´ˆê¸°í™” ê²½ê³ : {e}")
            # ì—ëŸ¬ë¥¼ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— í™•ì¸ ê°€ëŠ¥í•˜ë„ë¡
            self._init_error = str(e)
    
    @classmethod
    def reset_lm(cls):
        """LM ì´ˆê¸°í™” ìƒíƒœë¥¼ ì™„ì „íˆ ë¦¬ì…‹í•©ë‹ˆë‹¤. ì œê³µìê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        cls._last_provider = None
        cls._lm_initialized = False
    
    def _get_current_model_info(self, suffix: str = "") -> str:
        """
        í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            suffix: ëª¨ë¸ ì´ë¦„ ë’¤ì— ì¶”ê°€í•  ì ‘ë¯¸ì‚¬ (ì˜ˆ: " (DSPy)")

        Returns:
            ëª¨ë¸ ì •ë³´ ë¬¸ìì—´
        """
        provider = get_current_provider()
        provider_config = PROVIDER_CONFIG.get(provider, {})
        model_name = provider_config.get('model', 'unknown')
        display_name = provider_config.get('display_name', provider)

        if suffix:
            return f"{display_name} {model_name}{suffix}"
        return f"{display_name} {model_name}"

    def _is_long_context_model(self) -> bool:
        """
        í˜„ì¬ í™œì„±í™”ëœ ëª¨ë¸ì´ Long Context ëª¨ë¸ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Returns:
            Long Context ëª¨ë¸ì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        current_provider = self._active_provider or get_current_provider()
        return current_provider in ['gemini', 'gemini_3pro', 'gemini_25pro', 'gemini_25flash']

    def _get_pdf_content_for_context(self, pdf_text: str, max_length: int = 4000, use_long_context: bool = False) -> str:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ìë¥´ê±°ë‚˜ ì „ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            pdf_text: PDF í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´ (Long Context ëª¨ë¸ì´ ì•„ë‹ ë•Œ ì‚¬ìš©)
            use_long_context: Long Context ëª¨ë¸ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ëœ PDF í…ìŠ¤íŠ¸
        """
        if not pdf_text:
            return "PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        if use_long_context:
            # Long Context ëª¨ë¸: ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            return pdf_text
        else:
            # ì¼ë°˜ ëª¨ë¸: ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
            if len(pdf_text) > max_length:
                return pdf_text[:max_length] + "\n\n[ë¬¸ì„œ ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤...]"
            return pdf_text

    def _get_output_format_template(self):
        """ì¶œë ¥ í˜•ì‹ í…œí”Œë¦¿ì„ ë°˜í™˜í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
        return """
## ğŸ“ ì¶œë ¥ í˜•ì‹ ìš”êµ¬ì‚¬í•­

**âš ï¸ ì¤‘ìš”**: í‘œë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.

### í•„ìˆ˜ êµ¬ì¡°:
1. **ì†Œì œëª©** (ì˜ˆ: ### í•„ìˆ˜ ì‹œì„¤ ë¶„ì„)
2. **ì†Œì œëª© í•´ì„¤** (3-5ë¬¸ì¥, 200-400ì) - í‘œ ì´ì „ì— ë°˜ë“œì‹œ ìƒì„¸í•œ ì„œìˆ í˜• ë¶„ì„ ì‘ì„±
3. **í‘œ** (ì •ë³´ ì •ë¦¬)
4. **í‘œ í•´ì„¤** (4-8ë¬¸ì¥, 300-600ì) - í‘œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•´ì„

### í˜•ì‹ ì˜ˆì‹œ:

### [ì†Œì œëª© 1]
[ì†Œì œëª©ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ì„ ë°˜ë“œì‹œ ì‘ì„±í•˜ì„¸ìš”. ì´ ì„¹ì…˜ì€ í‘œ ì´ì „ì— í•„ìˆ˜ì…ë‹ˆë‹¤. ìµœì†Œ 200ì ì´ìƒì˜ ì„œìˆ í˜• ë¶„ì„ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤" ê°™ì€ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê·¼ê±°, ë¶„ì„ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.]

| í•­ëª© | ë‚´ìš© | ë¹„ê³  |
|------|------|------|
| í•­ëª©1 | ë‚´ìš©1 | ë¹„ê³ 1 |
| í•­ëª©2 | ë‚´ìš©2 | ë¹„ê³ 2 |

**[í‘œ í•´ì„¤]**
ìœ„ í‘œì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ì„ 4-8ë¬¸ì¥(300-600ì)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. í‘œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•´ì„í•˜ë©°, ê° í•­ëª©ì˜ ì˜ë¯¸ì™€ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë‹¨ìˆœíˆ í‘œì˜ ë‚´ìš©ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í‘œì—ì„œ ë“œëŸ¬ë‚˜ëŠ” íŒ¨í„´, ê´€ê³„, ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.

### [ì†Œì œëª© 2]
[ì†Œì œëª©ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ (3-5ë¬¸ì¥, 200-400ì)]

| í•­ëª© | ë‚´ìš© | ë¹„ê³  |
|------|------|------|
| í•­ëª©1 | ë‚´ìš©1 | ë¹„ê³ 1 |
| í•­ëª©2 | ë‚´ìš©2 | ë¹„ê³ 2 |

**[í‘œ í•´ì„¤]**
ìœ„ í‘œì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ì„ 4-8ë¬¸ì¥(300-600ì)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

### [ì†Œì œëª© 3]
[ì†Œì œëª©ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ (3-5ë¬¸ì¥, 200-400ì)]

| í•­ëª© | ë‚´ìš© | ë¹„ê³  |
|------|------|------|
| í•­ëª©1 | ë‚´ìš©1 | ë¹„ê³ 1 |
| í•­ëª©2 | ë‚´ìš©2 | ë¹„ê³ 2 |

**[í‘œ í•´ì„¤]**
ìœ„ í‘œì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ì„ 4-8ë¬¸ì¥(300-600ì)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

### [ì†Œì œëª© 4]
[ì†Œì œëª©ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ (3-5ë¬¸ì¥, 200-400ì)]

| í•­ëª© | ë‚´ìš© | ë¹„ê³  |
|------|------|------|
| í•­ëª©1 | ë‚´ìš©1 | ë¹„ê³ 1 |
| í•­ëª©2 | ë‚´ìš©2 | ë¹„ê³ 2 |

**[í‘œ í•´ì„¤]**
ìœ„ í‘œì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤ì„ 4-8ë¬¸ì¥(300-600ì)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

## âš ï¸ í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­

1. **í‘œ ì´ì „ ì„œìˆ  í•„ìˆ˜**: ëª¨ë“  í‘œ ì•ì— ë°˜ë“œì‹œ 200-400ìì˜ ìƒì„¸í•œ ì„œìˆ í˜• ë¶„ì„ì„ ì‘ì„±í•˜ì„¸ìš”. í‘œë§Œìœ¼ë¡œëŠ” ì ˆëŒ€ ë¶€ì¡±í•©ë‹ˆë‹¤.

2. **êµ¬ì²´ì  ìˆ˜ì¹˜ ì œì‹œ**: ëª¨ë“  ë¶„ì„ì— êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš” (ë©´ì , ì¸ì›, ë¹„ìš©, ì´ìš©ë¥ , ì‹œê°„, ë¹„ìœ¨ ë“±).

3. **ê·¼ê±° ëª…ì‹œ**: ëª¨ë“  ê²°ë¡ ê³¼ íŒë‹¨ì— ëª…í™•í•œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”. "~ë¼ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤"ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ë©°, ì™œ ê·¸ë ‡ê²Œ íŒë‹¨í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

4. **ë¶„ëŸ‰ ì¤€ìˆ˜**: ê° ì†Œì œëª© í•´ì„¤ì€ ìµœì†Œ 200ì, í‘œ í•´ì„¤ì€ ìµœì†Œ 300ì ì´ìƒ ì‘ì„±í•˜ì„¸ìš”.

5. **ì„œìˆ í˜• ë¬¸ì¥**: ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ í‚¤ì›Œë“œ ë‚˜ì—´ì´ ì•„ë‹Œ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
"""
    
    def _get_extended_thinking_template(self):
        """í™•ì¥ ì‚¬ê³ (Extended Thinking) ì§€ì‹œì‚¬í•­ í…œí”Œë¦¿ì„ ë°˜í™˜í•˜ëŠ” ì‹œìŠ¤í…œ ë ˆë²¨ í•¨ìˆ˜"""
        return """

## ğŸ§  í™•ì¥ ì‚¬ê³  (Extended Thinking) ì§€ì‹œì‚¬í•­

ì´ ë¶„ì„ì€ ë³µì¡í•œ ë‹¤ì°¨ì›ì  ë¬¸ì œë¥¼ ë‹¤ë£¨ë¯€ë¡œ, **í™•ì¥ ì‚¬ê³ (Extended Thinking)** ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ ìˆëŠ” ì¶”ë¡ ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì • ëª…ì‹œ**: ê° ë¶„ì„ ë‹¨ê³„ì—ì„œ ì–´ë–»ê²Œ ìƒê°í–ˆëŠ”ì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡
2. **ë‹¤ê°ë„ ë¶„ì„**: ê° ë¬¸ì œë¥¼ ë‹¤ì–‘í•œ ê´€ì (ê²½ì œì , ë²•ì , ì‚¬íšŒì , ê¸°ìˆ ì , í™˜ê²½ì )ì—ì„œ ë¶„ì„
3. **ê°€ì •ê³¼ ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ**: ë¶„ì„ì— ì‚¬ìš©ëœ ê°€ì •ê³¼ ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì„ ëª…í™•íˆ í‘œì‹œ
4. **ëŒ€ì•ˆ ê²€í† **: ì£¼ìš” ê²°ì •ì‚¬í•­ì— ëŒ€í•´ ëŒ€ì•ˆì„ ê²€í† í•˜ê³  ë¹„êµ ë¶„ì„
5. **ê²€ì¦ ê°€ëŠ¥í•œ ê²°ë¡ **: ëª¨ë“  ê²°ë¡ ì´ ê²€ì¦ ê°€ëŠ¥í•œ ê·¼ê±°ë¥¼ ê°€ì§€ë„ë¡ í•¨

**í™•ì¥ ì‚¬ê³  í˜•ì‹ ì˜ˆì‹œ:**
- **ì‚¬ê³  ë‹¨ê³„ 1**: [ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •]
- **ê°€ì •**: [ë¶„ì„ì— ì‚¬ìš©ëœ ê°€ì •]
- **ë¶ˆí™•ì‹¤ì„±**: [ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ê³¼ í•´ê²° ë°©ì•ˆ]
- **ëŒ€ì•ˆ ê²€í† **: [ê³ ë ¤í•œ ëŒ€ì•ˆê³¼ ë¹„êµ]
- **ê²°ë¡ **: [ê²€ì¦ ê°€ëŠ¥í•œ ê²°ë¡ ]
"""
    
    def _build_signature_map(self) -> Dict[str, Type]:
        """
        ë¸”ë¡ IDì™€ Signature í´ë˜ìŠ¤ë¥¼ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        ê¸°ë³¸ ë¸”ë¡ì€ í•˜ë“œì½”ë”©ëœ ë§¤í•‘ì„ ì‚¬ìš©í•˜ê³ , ì»¤ìŠ¤í…€ ë¸”ë¡ì€ blocks.jsonì—ì„œ ì½ì–´ì„œ ë™ì ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
        
        Returns:
            ë¸”ë¡ IDë¥¼ í‚¤ë¡œ, Signature í´ë˜ìŠ¤ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ë¸”ë¡ë“¤ì˜ í•˜ë“œì½”ë”©ëœ ë§¤í•‘ (ê¸°ì¡´ ë¸”ë¡ í˜¸í™˜ì„± ìœ ì§€)
        signature_map = {            'basic_info': BasicInfoSignature,
            'requirements': RequirementsSignature,
            'design_suggestions': DesignSignature,
            'accessibility_analysis': AccessibilitySignature,
            'zoning_verification': ZoningSignature,
            'capacity_estimation': CapacitySignature,
            'feasibility_analysis': FeasibilitySignature,
            'phase1_facility_program': SimpleAnalysisSignature,
            'phase1_facility_area_reference': SimpleAnalysisSignature,
            'phase1_facility_area_calculation': SimpleAnalysisSignature,
            'phase1_candidate_generation': SimpleAnalysisSignature,
            'phase1_candidate_evaluation': SimpleAnalysisSignature
}
        
        # blocks.jsonì—ì„œ ë¸”ë¡ì„ ì½ì–´ì„œ ë™ì ìœ¼ë¡œ Signature í´ë˜ìŠ¤ ë§¤í•‘ ì¶”ê°€
        try:
            from prompt_processor import load_blocks
            blocks = load_blocks()
            
            # í˜„ì¬ ëª¨ë“ˆì˜ globals()ì—ì„œ Signature í´ë˜ìŠ¤ ì°¾ê¸°
            current_module_globals = globals()
            
            for block in blocks:
                block_id = block.get('id')
                if not block_id or block_id in signature_map:
                    # ì´ë¯¸ ë§¤í•‘ëœ ë¸”ë¡ì€ ê±´ë„ˆë›°ê¸°
                    continue
                
                # ë¸”ë¡ IDì—ì„œ Signature í´ë˜ìŠ¤ëª… ìƒì„± (Block Generatorì™€ ë™ì¼í•œ ê·œì¹™)
                # ì˜ˆ: "my_analysis" -> "MyAnalysisSignature"
                signature_name = ''.join(word.capitalize() for word in block_id.split('_')) + 'Signature'
                
                # globals()ì—ì„œ Signature í´ë˜ìŠ¤ ì°¾ê¸°
                signature_class = current_module_globals.get(signature_name)
                if signature_class and issubclass(signature_class, dspy.Signature):
                    signature_map[block_id] = signature_class
                    print(f"âœ… ë™ì  Signature ë§¤í•‘: {block_id} -> {signature_name}")
                else:
                    # Signature í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ SimpleAnalysisSignature ì‚¬ìš©
                    signature_map[block_id] = SimpleAnalysisSignature
                    if block.get('created_by') == 'user':
                        # ì‚¬ìš©ìê°€ ìƒì„±í•œ ë¸”ë¡ì¸ ê²½ìš°ì—ë§Œ ë¡œê·¸ ì¶œë ¥
                        print(f"âš ï¸ Signature í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ ({signature_name}), SimpleAnalysisSignature ì‚¬ìš©: {block_id}")
                        print(f"   ğŸ’¡ íŒ: ìƒˆë¡œ ìƒì„±í•œ ë¸”ë¡ì˜ ê²½ìš° Streamlit í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ë©´ Signatureê°€ ë¡œë“œë©ë‹ˆë‹¤.")
                        print(f"   ğŸ’¡ ë˜ëŠ” dspy_analyzer.py íŒŒì¼ì— '{signature_name}' í´ë˜ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        except Exception as e:
            # blocks.json ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë§¤í•‘ë§Œ ì‚¬ìš©
            print(f"âš ï¸ blocks.json ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ Signature ë§¤í•‘ë§Œ ì‚¬ìš©: {e}")
        
        return signature_map
    
    def setup_dspy(self):
        """ì„ íƒëœ ì œê³µìì— ë”°ë¼ DSPy ì„¤ì •"""
        current_provider = get_current_provider()
        
        # Providerê°€ ë³€ê²½ëœ ê²½ìš° ê¸°ì¡´ ìºì‹œ í´ë¦¬ì–´
        if hasattr(self, '_active_provider') and self._active_provider != current_provider:
            if current_provider in self._provider_lms:
                # Providerê°€ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ í•´ë‹¹ providerì˜ ìºì‹œë¥¼ ì¬ìƒì„±
                del self._provider_lms[current_provider]
        
        self._active_provider = current_provider
        
        # ê¸°ì¡´ LM ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (ê°™ì€ providerì¸ ê²½ìš°)
        if current_provider in self._provider_lms:
            return
        
        temperature = 0.2
        max_tokens = 16000
        thinking_budget = None  # Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš© (Gemini 2.5ëŠ” thinking í™œì„±í™”)
        thinking_level = None  # Gemini 3 Proìš©: "low" ë˜ëŠ” "high"
        include_thoughts = False  # Thought Summaries í¬í•¨ ì—¬ë¶€
        try:
            import streamlit as st
            temperature = float(st.session_state.get('llm_temperature', temperature))
            max_tokens = int(st.session_state.get('llm_max_tokens', max_tokens))
            thinking_budget = st.session_state.get('llm_thinking_budget', None)
            thinking_level = st.session_state.get('llm_thinking_level', None)
            include_thoughts = st.session_state.get('llm_include_thoughts', False)
        except Exception:
            temperature = float(os.environ.get('LLM_TEMPERATURE', temperature))
            max_tokens = int(os.environ.get('LLM_MAX_TOKENS', max_tokens))
            thinking_budget_str = os.environ.get('LLM_THINKING_BUDGET')
            if thinking_budget_str:
                try:
                    thinking_budget = int(thinking_budget_str)
                except ValueError:
                    thinking_budget = None
            thinking_level = os.environ.get('LLM_THINKING_LEVEL', None)
            include_thoughts_str = os.environ.get('LLM_INCLUDE_THOUGHTS', 'false').lower()
            include_thoughts = include_thoughts_str == 'true'

        temperature = max(0.0, min(1.0, temperature))
        max_tokens = max(1000, min(16000, max_tokens))

        # ì„ íƒëœ ì œê³µì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        provider_config = PROVIDER_CONFIG.get(current_provider)
        if not provider_config:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µìì…ë‹ˆë‹¤: {current_provider}")

        # API í‚¤ ê°€ì ¸ì˜¤ê¸° (Vertex AIëŠ” ADC ì‚¬ìš©, API í‚¤ ë¶ˆí•„ìš”)
        api_key = None
        if provider_config.get('api_key_env'):
            api_key = get_api_key(current_provider)
            if not api_key:
                provider_name = provider_config.get('display_name', current_provider)
                api_key_env = provider_config['api_key_env']
                raise ValueError(f"{provider_name} API í‚¤({api_key_env})ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

        # DSPy LM ì„¤ì •
        try:
            provider_name = provider_config['provider']
            base_model_name = provider_config['model']
            
            # Google AI Studio (Gemini)ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if current_provider in ['gemini', 'gemini_3pro', 'gemini_25pro', 'gemini_25flash']:
                # Google AI Studio API í‚¤ ë°©ì‹
                # models/ ì ‘ë‘ì‚¬ ì œê±°
                clean_model = base_model_name.replace('models/', '').replace('model/', '')
                
                # LiteLLM Google AI Studio ëª¨ë¸ ì´ë¦„ í˜•ì‹
                # ì—¬ëŸ¬ ëª¨ë¸ ì´ë¦„ ì‹œë„: gemini-proê°€ ê°€ì¥ ì•ˆì •ì 
                # gemini-1.5-flash, gemini-1.5-proëŠ” ì¼ë¶€ ì§€ì—­ì—ì„œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŒ
                litellm_model = f"gemini/{clean_model}"
                
                print(f"ğŸ”§ Google AI Studio Gemini ì„¤ì •:")
                print(f"   base_model_name: {base_model_name}")
                print(f"   clean_model: {clean_model}")
                print(f"   litellm_model: {litellm_model}")
                print(f"   api_key ì„¤ì •ë¨: {bool(api_key)}")
                if api_key:
                    print(f"   api_key ì‹œì‘: {api_key[:10]}...")
                
                lm_kwargs = {
                    'model': litellm_model,
                    'api_key': api_key,
                    'max_tokens': max_tokens,
                    'temperature': temperature
                }
                extra_body = {
                    # Gemini ê²€ìƒ‰ ì—°ë™ì„ ê¸°ë³¸ í™œì„±í™” (grounded response ê¸°ëŒ€)
                    'tools': [{'google_search': {}}]
                }
                print("   Google Search Grounding: enabled (tools.google_search)")
                
                # Gemini 2.5 ë° 3 ëª¨ë¸ì˜ Thinking Config ì§€ì›
                # gemini-2.5-pro, gemini-2.5-flash, gemini-3-pro-preview ëª¨ë¸ ê°ì§€
                is_thinking_model = (
                    'gemini-2.5' in clean_model or 
                    'gemini-3' in clean_model or
                    'gemini-2.0' in clean_model
                )
                
                if is_thinking_model:
                    # Thinking Config ì„¤ì •
                    # ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ Thinking íŒŒë¼ë¯¸í„° ì§€ì›
                    is_gemini_3 = 'gemini-3' in clean_model
                    is_gemini_25_pro = 'gemini-2.5-pro' in clean_model
                    is_gemini_25_flash = 'gemini-2.5-flash' in clean_model or 'gemini-2.5-flash-lite' in clean_model
                    
                    thinking_config = {}
                    
                    # Gemini 3 Pro: thinking_level ìš°ì„  ì‚¬ìš©
                    if is_gemini_3:
                        if thinking_level:
                            if thinking_level.lower() in ['low', 'high']:
                                thinking_config['thinking_level'] = thinking_level.lower()
                                print(f"   Thinking Level: {thinking_level.lower()}")
                            else:
                                print(f"   ê²½ê³ : thinking_levelì€ 'low' ë˜ëŠ” 'high'ë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
                        # Gemini 3 ProëŠ” thinking ë¹„í™œì„±í™” ë¶ˆê°€
                        # thinking_budgetì€ í˜¸í™˜ì„±ì„ ìœ„í•´ ì§€ì›í•˜ì§€ë§Œ thinking_level ì‚¬ìš© ê¶Œì¥
                        if thinking_budget is not None and not thinking_level:
                            print(f"   ê²½ê³ : Gemini 3 ProëŠ” thinking_level ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                            if thinking_budget > 0:
                                thinking_config['thinking_budget'] = thinking_budget
                                print(f"   Thinking Budget: {thinking_budget} (í˜¸í™˜ì„± ëª¨ë“œ)")
                    # Gemini 2.5 Pro: thinking_budgetë§Œ ì§€ì›, ë¹„í™œì„±í™” ë¶ˆê°€
                    elif is_gemini_25_pro:
                        if thinking_budget is not None:
                            if thinking_budget == 0:
                                print(f"   ê²½ê³ : Gemini 2.5 Proì—ì„œëŠ” thinkingì„ ë¹„í™œì„±í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            elif thinking_budget == -1:
                                thinking_config['thinking_budget'] = -1
                                print(f"   Thinking: ë™ì  ì‚¬ê³  (thinking_budget=-1)")
                            elif 128 <= thinking_budget <= 32768:
                                thinking_config['thinking_budget'] = thinking_budget
                                print(f"   Thinking Budget: {thinking_budget}")
                            else:
                                print(f"   ê²½ê³ : Gemini 2.5 Proì˜ thinking_budget ë²”ìœ„ëŠ” 128-32768ì…ë‹ˆë‹¤.")
                        else:
                            print(f"   Thinking: ê¸°ë³¸ê°’ ì‚¬ìš© (ë™ì  ì‚¬ê³ )")
                    # Gemini 2.5 Flash: thinking_budget ì§€ì›, ë¹„í™œì„±í™” ê°€ëŠ¥
                    elif is_gemini_25_flash:
                        if thinking_budget is not None:
                            if thinking_budget == 0:
                                thinking_config['thinking_budget'] = 0
                                print(f"   Thinking: ë¹„í™œì„±í™” (thinking_budget=0)")
                            elif thinking_budget == -1:
                                thinking_config['thinking_budget'] = -1
                                print(f"   Thinking: ë™ì  ì‚¬ê³  (thinking_budget=-1)")
                            elif 0 <= thinking_budget <= 24576:
                                thinking_config['thinking_budget'] = thinking_budget
                                print(f"   Thinking Budget: {thinking_budget}")
                            else:
                                print(f"   ê²½ê³ : Gemini 2.5 Flashì˜ thinking_budget ë²”ìœ„ëŠ” 0-24576ì…ë‹ˆë‹¤.")
                        else:
                            print(f"   Thinking: ê¸°ë³¸ê°’ ì‚¬ìš© (ë™ì  ì‚¬ê³ )")
                    
                    # Thought Summaries ì§€ì› (ëª¨ë“  thinking ëª¨ë¸)
                    if include_thoughts:
                        thinking_config['include_thoughts'] = True
                        print(f"   Thought Summaries: í™œì„±í™”")
                    
                    # extra_bodyì— thinking_config ì¶”ê°€
                    if thinking_config:
                        extra_body['thinking_config'] = thinking_config
                    else:
                        print(f"   Thinking: ê¸°ë³¸ê°’ ì‚¬ìš© (ëª¨ë¸ ê¸°ë³¸ ì„¤ì •)")
                
                if extra_body:
                    lm_kwargs['extra_body'] = extra_body
                
                # providerëŠ” ëª…ì‹œí•˜ì§€ ì•ŠìŒ (ëª¨ë¸ ì´ë¦„ì—ì„œ ìë™ ì¸ì‹)
                print(f"ğŸ”§ Google AI Studio LM kwargs: model={lm_kwargs['model']}")
            else:
                litellm_model = (
                    f"{provider_name}/{base_model_name}"
                    if provider_name and "/" not in base_model_name
                    else base_model_name
                )
                lm_kwargs = {
                    'model': litellm_model,
                    'provider': provider_name,
                    'api_key': api_key,
                    'max_tokens': max_tokens,
                    'temperature': temperature
                }
            
            if current_provider == 'deepseek' and 'base_url' in provider_config:
                lm_kwargs['base_url'] = provider_config['base_url']
            
            lm = dspy.LM(**lm_kwargs)
            self._provider_lms[current_provider] = lm
            
            if not EnhancedArchAnalyzer._lm_initialized:
                try:
                    dspy.configure(lm=lm, track_usage=True, cache=False)
                    print("DSPy ì „ì—­ LMì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ìºì‹± ë¹„í™œì„±í™”)")
                except RuntimeError as thread_error:
                    print(f"ì „ì—­ LM ì„¤ì • ê²½ê³ : {thread_error}. í™œì„± ì»¨í…ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                EnhancedArchAnalyzer._lm_initialized = True
            
            provider_name = provider_config.get('display_name', current_provider)
            model_name = provider_config['model']
            print(f"{provider_name} ({model_name}) ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            EnhancedArchAnalyzer._last_provider = current_provider
            try:
                import streamlit as st
                st.session_state['_last_llm_provider'] = current_provider
            except Exception:
                pass
                
        except Exception as e:
            provider_name = provider_config.get('display_name', current_provider)
            print(f"{provider_name} ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            if current_provider != 'gemini':
                fallback_provider = 'gemini'
                fallback_config = PROVIDER_CONFIG.get(fallback_provider)
                fallback_api_key = get_api_key(fallback_provider)
                
                if fallback_config and fallback_api_key:
                    try:
                        lm = dspy.LM(
                            model=fallback_config['model'],
                            provider=fallback_config['provider'],
                            api_key=fallback_api_key,
                            max_tokens=max_tokens,
                            temperature=temperature
                        )
                        self._provider_lms[fallback_provider] = lm
                        self._active_provider = fallback_provider
                        if not EnhancedArchAnalyzer._lm_initialized:
                            try:
                                dspy.configure(lm=lm, track_usage=True, cache=False)
                                print("DSPy ì „ì—­ LMì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (í´ë°±, ìºì‹± ë¹„í™œì„±í™”)")
                            except RuntimeError as thread_error:
                                print(f"ì „ì—­ LM ì„¤ì • ê²½ê³ : {thread_error}. í™œì„± ì»¨í…ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                            EnhancedArchAnalyzer._lm_initialized = True
                        print(f"í´ë°±: {fallback_config.get('display_name')} ëª¨ë¸ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        return
                    except Exception as e2:
                        print(f"í´ë°± ëª¨ë¸ ì„¤ì •ë„ ì‹¤íŒ¨: {e2}")
            
            raise
    
    @contextmanager
    def _lm_context(self, provider: Optional[str] = None):
        """ì„ íƒëœ LMì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì ìš©"""
        target_provider = provider or self._active_provider or get_current_provider()
        lm = None
        if hasattr(self, "_provider_lms"):
            lm = self._provider_lms.get(target_provider)
            if lm is None and self._provider_lms:
                lm = next(iter(self._provider_lms.values()))
        if lm is None:
            yield
            return
        try:
            ctx = dspy.settings.context(lm=lm)
        except Exception:
            yield
        else:
            with ctx:
                yield
    
    @contextmanager
    def _lm_context_with_system_instruction(self, system_instruction: str, provider: Optional[str] = None):
        """System Instructionì„ í¬í•¨í•œ LM ì»¨í…ìŠ¤íŠ¸"""
        target_provider = provider or self._active_provider or get_current_provider()
        lm = None
        if hasattr(self, "_provider_lms"):
            lm = self._provider_lms.get(target_provider)
            if lm is None and self._provider_lms:
                lm = next(iter(self._provider_lms.values()))
        
        if lm is None:
            yield
            return
        
        # System Instructionì„ LMì— ì ìš©
        original_extra_body = None
        try:
            # LiteLLMì˜ extra_bodyì— system_instruction ì¶”ê°€
            if hasattr(lm, 'kwargs') and 'extra_body' in lm.kwargs:
                original_extra_body = lm.kwargs.get('extra_body', {}).copy()
                if not isinstance(lm.kwargs.get('extra_body'), dict):
                    lm.kwargs['extra_body'] = {}
                lm.kwargs['extra_body']['system_instruction'] = {
                    "parts": [{"text": system_instruction}]
                }
            elif hasattr(lm, 'kwargs'):
                lm.kwargs['extra_body'] = {
                    'system_instruction': {
                        "parts": [{"text": system_instruction}]
                    }
                }
            
            # System instructionì„ extra_bodyì— ì§ì ‘ ì¶”ê°€ (LiteLLM í˜•ì‹)
            if hasattr(lm, 'kwargs'):
                if 'extra_body' not in lm.kwargs:
                    lm.kwargs['extra_body'] = {}
                # LiteLLMì€ system_instructionì„ extra_bodyì— ì¶”ê°€
                lm.kwargs['extra_body']['system_instruction'] = system_instruction
            
            try:
                ctx = dspy.settings.context(lm=lm)
            except Exception:
                yield
            else:
                with ctx:
                    yield
        finally:
            # ì›ë˜ ìƒíƒœë¡œ ë³µì›
            if original_extra_body is not None and hasattr(lm, 'kwargs'):
                lm.kwargs['extra_body'] = original_extra_body
            elif hasattr(lm, 'kwargs') and 'extra_body' in lm.kwargs:
                # system_instructionë§Œ ì œê±°
                if 'system_instruction' in lm.kwargs['extra_body']:
                    del lm.kwargs['extra_body']['system_instruction']
    
    @contextmanager
    def _lm_context_with_params(self, thinking_budget: Optional[int] = None, temperature: Optional[float] = None, system_instruction: Optional[str] = None, provider: Optional[str] = None):
        """Thinking Budget, Temperature, System Instructionì„ í¬í•¨í•œ LM ì»¨í…ìŠ¤íŠ¸"""
        """Thinking Budgetê³¼ System Instructionì„ í¬í•¨í•œ LM ì»¨í…ìŠ¤íŠ¸"""
        target_provider = provider or self._active_provider or get_current_provider()
        lm = None
        if hasattr(self, "_provider_lms"):
            lm = self._provider_lms.get(target_provider)
            if lm is None and self._provider_lms:
                lm = next(iter(self._provider_lms.values()))
        
        if lm is None:
            yield
            return
        
        # í˜„ì¬ ëª¨ë¸ì´ thinkingì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
        current_provider = get_current_provider()
        provider_config = PROVIDER_CONFIG.get(current_provider, {})
        model_name = provider_config.get('model', '')
        clean_model = model_name.replace('models/', '').replace('model/', '')
        
        is_thinking_model = (
            'gemini-2.5' in clean_model or 
            'gemini-3' in clean_model or
            'gemini-2.0' in clean_model
        )
        
        # Temperature ì ìš©
        original_temperature = None
        if temperature is not None and hasattr(lm, 'kwargs'):
            original_temperature = lm.kwargs.get('temperature')
            lm.kwargs['temperature'] = max(0.0, min(1.0, temperature))
        
        if not is_thinking_model:
            # Thinkingì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° system_instructionê³¼ temperatureë§Œ ì ìš©
            try:
                if system_instruction:
                    with self._lm_context_with_system_instruction(system_instruction, provider):
                        yield
                else:
                    with self._lm_context(provider):
                        yield
            finally:
                # Temperature ë³µì›
                if original_temperature is not None and hasattr(lm, 'kwargs'):
                    lm.kwargs['temperature'] = original_temperature
            return
        
        # Thinking Budgetê³¼ System Instructionì„ LMì— ì ìš©
        original_extra_body = None
        try:
            if hasattr(lm, 'kwargs'):
                if 'extra_body' in lm.kwargs:
                    original_extra_body = lm.kwargs.get('extra_body', {}).copy()
                else:
                    lm.kwargs['extra_body'] = {}
                
                # System Instruction ì¶”ê°€
                if system_instruction:
                    lm.kwargs['extra_body']['system_instruction'] = system_instruction
                
                # Thinking Config ì¶”ê°€
                is_gemini_3 = 'gemini-3' in clean_model
                is_gemini_25_pro = 'gemini-2.5-pro' in clean_model
                is_gemini_25_flash = 'gemini-2.5-flash' in clean_model or 'gemini-2.5-flash-lite' in clean_model
                
                thinking_config = {}
                
                if is_gemini_3:
                    # Gemini 3ëŠ” thinking_level ì‚¬ìš© ê¶Œì¥, í•˜ì§€ë§Œ thinking_budgetë„ ì§€ì›
                    if thinking_budget > 0:
                        thinking_config['thinking_budget'] = thinking_budget
                elif is_gemini_25_pro:
                    # Gemini 2.5 ProëŠ” thinking_budgetë§Œ ì§€ì›
                    if thinking_budget > 0:
                        thinking_config['thinking_budget'] = max(128, min(32768, thinking_budget))
                elif is_gemini_25_flash:
                    # Gemini 2.5 FlashëŠ” thinking_budget ì§€ì›, 0ìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥
                    thinking_config['thinking_budget'] = max(0, min(24576, thinking_budget))
                
                if thinking_config:
                    if 'thinking_config' not in lm.kwargs['extra_body']:
                        lm.kwargs['extra_body']['thinking_config'] = {}
                    lm.kwargs['extra_body']['thinking_config'].update(thinking_config)
            
            # Temperature ì ìš©
            if temperature is not None:
                original_temperature = lm.kwargs.get('temperature')
                lm.kwargs['temperature'] = max(0.0, min(1.0, temperature))
            
            try:
                ctx = dspy.settings.context(lm=lm)
            except Exception:
                yield
            else:
                with ctx:
                    yield
        finally:
            # ì›ë˜ ìƒíƒœë¡œ ë³µì›
            if original_extra_body is not None and hasattr(lm, 'kwargs'):
                lm.kwargs['extra_body'] = original_extra_body
            elif hasattr(lm, 'kwargs') and 'extra_body' in lm.kwargs:
                # ì¶”ê°€í•œ í•­ëª©ë§Œ ì œê±°
                if 'system_instruction' in lm.kwargs['extra_body']:
                    del lm.kwargs['extra_body']['system_instruction']
                if 'thinking_config' in lm.kwargs['extra_body']:
                    del lm.kwargs['extra_body']['thinking_config']
            
            # Temperature ë³µì›
            if 'original_temperature' in locals() and original_temperature is not None and hasattr(lm, 'kwargs'):
                lm.kwargs['temperature'] = original_temperature
    
    def _get_structured_output_config(self, schema: Optional[Union[Type, Dict[str, Any]]]) -> Optional[Dict[str, Any]]:
        """
        Pydantic ëª¨ë¸ ë˜ëŠ” JSON ìŠ¤í‚¤ë§ˆë¥¼ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •ìœ¼ë¡œ ë³€í™˜
        
        Args:
            schema: Pydantic ëª¨ë¸ í´ë˜ìŠ¤ ë˜ëŠ” JSON ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        if schema is None:
            return None
        
        try:
            # Pydantic ëª¨ë¸ í´ë˜ìŠ¤ì¸ ê²½ìš°
            if PYDANTIC_AVAILABLE and isinstance(schema, type) and issubclass(schema, BaseModel):
                return {
                    "response_mime_type": "application/json",
                    "response_json_schema": schema.model_json_schema()
                }
            # JSON ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            elif isinstance(schema, dict):
                return {
                    "response_mime_type": "application/json",
                    "response_json_schema": schema
                }
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë³€í™˜ ì˜¤ë¥˜: {e}")
        
        return None
    
    def _parse_structured_response(self, response_text: str, schema: Optional[Union[Type, Dict[str, Any]]]) -> Any:
        """
        êµ¬ì¡°í™”ëœ ì‘ë‹µì„ Pydantic ëª¨ë¸ë¡œ íŒŒì‹±
        
        Args:
            response_text: JSON ë¬¸ìì—´ ì‘ë‹µ
            schema: Pydantic ëª¨ë¸ í´ë˜ìŠ¤ ë˜ëŠ” None
        
        Returns:
            Pydantic ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬
        """
        if schema is None:
            return response_text
        
        try:
            # JSON íŒŒì‹±
            response_data = json.loads(response_text)
            
            # Pydantic ëª¨ë¸ì´ ì œê³µëœ ê²½ìš° ê²€ì¦ ë° ë³€í™˜
            if PYDANTIC_AVAILABLE and isinstance(schema, type) and issubclass(schema, BaseModel):
                return schema.model_validate(response_data)
            else:
                # JSON ìŠ¤í‚¤ë§ˆë§Œ ì œê³µëœ ê²½ìš° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
                return response_data
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return response_text
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°í™”ëœ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return response_text
    
    def _convert_function_declarations(self, function_declarations: List[Union[Dict[str, Any], Callable]]) -> List[Dict[str, Any]]:
        """
        Function declarationsë¥¼ LiteLLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            function_declarations: Function declaration ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” Python í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ë³€í™˜ëœ function declaration ë¦¬ìŠ¤íŠ¸
        """
        converted = []
        
        for func_decl in function_declarations:
            if isinstance(func_decl, dict):
                # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ì¸ ê²½ìš°
                converted.append(func_decl)
            elif callable(func_decl):
                # Python í•¨ìˆ˜ì¸ ê²½ìš° - Google GenAI SDK ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ ì‹œë„
                try:
                    # í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ì™€ docstringì„ ê¸°ë°˜ìœ¼ë¡œ declaration ìƒì„±
                    import inspect
                    
                    sig = inspect.signature(func_decl)
                    docstring = inspect.getdoc(func_decl) or ""
                    
                    # Function declaration ìƒì„±
                    func_decl_dict = {
                        "name": func_decl.__name__,
                        "description": docstring,
                        "parameters": {
                            "type": "object",
                            "properties": {},
                            "required": []
                        }
                    }
                    
                    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                    for param_name, param in sig.parameters.items():
                        param_type = param.annotation
                        param_desc = ""
                        
                        # íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                        type_str = "string"
                        if param_type == int:
                            type_str = "integer"
                        elif param_type == float:
                            type_str = "number"
                        elif param_type == bool:
                            type_str = "boolean"
                        elif param_type == list or getattr(param_type, '__origin__', None) == list:
                            type_str = "array"
                        
                        func_decl_dict["parameters"]["properties"][param_name] = {
                            "type": type_str,
                            "description": param_desc
                        }
                        
                        if param.default == inspect.Parameter.empty:
                            func_decl_dict["parameters"]["required"].append(param_name)
                    
                    converted.append(func_decl_dict)
                except Exception as e:
                    print(f"âš ï¸ í•¨ìˆ˜ë¥¼ function declarationìœ¼ë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                    continue
            else:
                print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” function declaration í˜•ì‹: {type(func_decl)}")
        
        return converted
    
    def _extract_function_calls(self, response) -> List[Dict[str, Any]]:
        """
        LiteLLM ì‘ë‹µì—ì„œ function calls ì¶”ì¶œ
        
        Args:
            response: LiteLLM completion ì‘ë‹µ
        
        Returns:
            Function call ë¦¬ìŠ¤íŠ¸
        """
        function_calls = []
        
        try:
            if hasattr(response, 'choices') and len(response.choices) > 0:
                message = response.choices[0].message
                
                # tool_calls í™•ì¸ (OpenAI í˜•ì‹)
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tool_call in message.tool_calls:
                        if hasattr(tool_call, 'function'):
                            function_calls.append({
                                "id": getattr(tool_call, 'id', None),
                                "name": tool_call.function.name,
                                "arguments": json.loads(tool_call.function.arguments) if isinstance(tool_call.function.arguments, str) else tool_call.function.arguments
                            })
                
                # function_call í™•ì¸ (êµ¬í˜• í˜•ì‹)
                elif hasattr(message, 'function_call') and message.function_call:
                    func_call = message.function_call
                    try:
                        # name ì•ˆì „ ì¶”ì¶œ
                        try:
                            func_name = func_call.name if hasattr(func_call, 'name') else func_call.get('name') if hasattr(func_call, 'get') else 'unknown'
                        except (AttributeError, Exception) as e:
                            print(f"[WARNING] func_call.name ì ‘ê·¼ ì‹¤íŒ¨ (êµ¬í˜•): {e}, íƒ€ì…: {type(func_call)}")
                            func_name = 'unknown'

                        # arguments ì•ˆì „ ì¶”ì¶œ
                        try:
                            func_args = func_call.arguments if hasattr(func_call, 'arguments') else func_call.get('arguments', {}) if hasattr(func_call, 'get') else {}
                        except (AttributeError, Exception) as e:
                            print(f"[WARNING] func_call.arguments ì ‘ê·¼ ì‹¤íŒ¨ (êµ¬í˜•): {e}, íƒ€ì…: {type(func_call)}")
                            func_args = {}

                        function_calls.append({
                            "id": None,
                            "name": func_name,
                            "arguments": func_args
                        })
                    except Exception as e:
                        print(f"[WARNING] function_call ì²˜ë¦¬ ì‹¤íŒ¨ (êµ¬í˜•): {e}, íƒ€ì…: {type(func_call)}")
        except Exception as e:
            print(f"âš ï¸ Function calls ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return function_calls
    
    def _execute_function_call(self, function_name: str, arguments: Dict[str, Any], function_implementations: Dict[str, Callable]) -> Any:
        """
        Function call ì‹¤í–‰
        
        Args:
            function_name: ì‹¤í–‰í•  í•¨ìˆ˜ ì´ë¦„
            arguments: í•¨ìˆ˜ ì¸ì
            function_implementations: í•¨ìˆ˜ ì´ë¦„ -> í•¨ìˆ˜ êµ¬í˜„ ë§¤í•‘
        
        Returns:
            í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼
        """
        if function_name not in function_implementations:
            return {"error": f"Function '{function_name}' not found in implementations"}
        
        try:
            func = function_implementations[function_name]
            result = func(**arguments)
            return result
        except Exception as e:
            return {"error": f"Function execution failed: {str(e)}"}
    
    def _analyze_with_function_calling(
        self,
        prompt: str,
        pdf_text: str = None,
        function_declarations: List[Union[Dict[str, Any], Callable]] = None,
        function_implementations: Dict[str, Callable] = None,
        automatic_function_calling: bool = False,
        function_calling_mode: str = "AUTO",
        max_iterations: int = 10,
        response_schema: Optional[Union[Type, Dict[str, Any]]] = None,
        block_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Function callingì„ ì‚¬ìš©í•œ ë¶„ì„
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            pdf_text: PDF í…ìŠ¤íŠ¸
            function_declarations: Function declaration ë¦¬ìŠ¤íŠ¸
            function_implementations: í•¨ìˆ˜ ì´ë¦„ -> í•¨ìˆ˜ êµ¬í˜„ ë§¤í•‘
            automatic_function_calling: ìë™ í•¨ìˆ˜ ì‹¤í–‰ ì—¬ë¶€
            function_calling_mode: Function calling ëª¨ë“œ
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (compositional calling)
            response_schema: êµ¬ì¡°í™”ëœ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (ì„ íƒì‚¬í•­)
            block_id: ë¸”ë¡ ID
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import litellm
        
        current_provider = self._active_provider or get_current_provider()
        provider_config = PROVIDER_CONFIG.get(current_provider)
        api_key = get_api_key(current_provider)
        base_model_name = provider_config['model']
        clean_model = base_model_name.replace('models/', '').replace('model/', '')
        litellm_model = f"gemini/{clean_model}"
        
        # temperatureì™€ max_tokens ê°€ì ¸ì˜¤ê¸°
        temp_value = 0.2
        max_tokens_value = 16000
        try:
            import streamlit as st
            temp_value = float(st.session_state.get('llm_temperature', 0.2))
            max_tokens_value = int(st.session_state.get('llm_max_tokens', 16000))
        except Exception:
            temp_value = float(os.environ.get('LLM_TEMPERATURE', 0.2))
            max_tokens_value = int(os.environ.get('LLM_MAX_TOKENS', 16000))
        
        # Function declarations ë³€í™˜
        converted_declarations = self._convert_function_declarations(function_declarations)
        if not converted_declarations:
            raise ValueError("ìœ íš¨í•œ function declarationsê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ”§ Function Calling ì‚¬ìš©: {len(converted_declarations)}ê°œ í•¨ìˆ˜")
        
        # LiteLLM tools í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        tools = [{
            "type": "function",
            "function": decl
        } for decl in converted_declarations]
        
        # Tool config ì„¤ì •
        tool_config = None
        if function_calling_mode != "AUTO":
            tool_config = {
                "tool_choice": {
                    "type": "function" if function_calling_mode == "ANY" else "none" if function_calling_mode == "NONE" else "auto"
                }
            }
        
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì • (ìˆëŠ” ê²½ìš°)
        extra_body = {}
        structured_output_config = self._get_structured_output_config(response_schema)
        if structured_output_config:
            extra_body.update(structured_output_config)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        messages = [{"role": "user", "content": prompt}]
        
        # Compositional function callingì„ ìœ„í•œ ë£¨í”„
        iteration = 0
        function_calls_history = []
        
        while iteration < max_iterations:
            iteration += 1
            print(f"ğŸ”„ Function calling ë°˜ë³µ {iteration}/{max_iterations}")
            
            # API í˜¸ì¶œ
            try:
                call_kwargs = {
                    "model": litellm_model,
                    "messages": messages,
                    "api_key": api_key,
                    "temperature": temp_value,
                    "max_tokens": max_tokens_value
                }
                
                # ì²« ë²ˆì§¸ ìš”ì²­ì—ë§Œ tools ì „ë‹¬
                if iteration == 1:
                    call_kwargs["tools"] = tools
                    if tool_config:
                        call_kwargs["tool_choice"] = tool_config.get("tool_choice")
                
                if extra_body:
                    call_kwargs["extra_body"] = extra_body
                
                response = litellm.completion(**call_kwargs)
            except Exception as e:
                return {
                    "success": False,
                    "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}",
                    "model": self._get_current_model_info(" (Function Calling)"),
                    "block_id": block_id
                }
            
            # ì‘ë‹µì—ì„œ function calls ì¶”ì¶œ
            function_calls = self._extract_function_calls(response)
            
            if not function_calls:
                # Function callì´ ì—†ìœ¼ë©´ ìµœì¢… í…ìŠ¤íŠ¸ ì‘ë‹µ
                response_text = response.choices[0].message.content if response.choices[0].message.content else ""
                
                # êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹±
                if response_schema and response_text:
                    parsed_response = self._parse_structured_response(response_text, response_schema)
                    return {
                        "success": True,
                        "analysis": parsed_response if PYDANTIC_AVAILABLE and isinstance(parsed_response, BaseModel) else response_text,
                        "parsed_data": parsed_response if response_schema else None,
                        "model": self._get_current_model_info(" (Function Calling)"),
                        "method": "Function Calling",
                        "block_id": block_id,
                        "function_calls": function_calls_history
                    }
                else:
                    return {
                        "success": True,
                        "analysis": response_text,
                        "model": self._get_current_model_info(" (Function Calling)"),
                        "method": "Function Calling",
                        "block_id": block_id,
                        "function_calls": function_calls_history
                    }
            
            # Function calls ì‹¤í–‰ (Parallel function calling ì§€ì›)
            function_responses = []
            for func_call in function_calls:
                function_name = func_call["name"]
                arguments = func_call.get("arguments", {})
                
                print(f"ğŸ”§ Function í˜¸ì¶œ: {function_name}({arguments})")
                
                if automatic_function_calling and function_implementations:
                    # ìë™ ì‹¤í–‰
                    result = self._execute_function_call(function_name, arguments, function_implementations)
                    function_responses.append({
                        "role": "tool",
                        "tool_call_id": func_call.get("id"),
                        "name": function_name,
                        "content": json.dumps(result) if isinstance(result, (dict, list)) else str(result)
                    })
                    function_calls_history.append({
                        "name": function_name,
                        "arguments": arguments,
                        "result": result
                    })
                else:
                    # ìˆ˜ë™ ì‹¤í–‰ - function calls ë°˜í™˜
                    return {
                        "success": True,
                        "analysis": f"Function call required: {function_name}",
                        "function_calls": [func_call],
                        "model": self._get_current_model_info(" (Function Calling)"),
                        "method": "Function Calling (Manual)",
                        "block_id": block_id,
                        "requires_manual_execution": True
                    }
            
            # Function responsesë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            # ì´ì „ ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
            messages.append({
                "role": "assistant",
                "content": None,
                "tool_calls": [
                    {
                        "id": fc.get("id", f"call_{i}"),
                        "type": "function",
                        "function": {
                            "name": fc["name"],
                            "arguments": json.dumps(fc["arguments"])
                        }
                    }
                    for i, fc in enumerate(function_calls)
                ]
            })
            
            # Function responses ì¶”ê°€ (Parallel calling - ëª¨ë“  ì‘ë‹µì„ í•œ ë²ˆì— ì¶”ê°€)
            messages.extend(function_responses)
        
        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬
        return {
            "success": False,
            "error": f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜({max_iterations})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.",
            "model": self._get_current_model_info(" (Function Calling)"),
            "block_id": block_id,
            "function_calls": function_calls_history
        }
    
    def analyze_project(self, project_info, pdf_text):
        """í”„ë¡œì íŠ¸ ë¶„ì„ - ì¼ê´€ëœ êµ¬ì¡°, ë¸”ë¡ë³„ ë‚´ìš© ì°¨ë³„í™”"""
        prompt = f"""
ë‹¤ìŒ ê±´ì¶• í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ Chain of Thought ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

**í”„ë¡œì íŠ¸ ì •ë³´:**
- í”„ë¡œì íŠ¸ëª…: {project_info.get('project_name', 'N/A')}
- í”„ë¡œì íŠ¸ ìœ í˜•: {project_info.get('project_type', 'N/A')}
- ìœ„ì¹˜: {project_info.get('location', 'N/A')}
- ê·œëª¨: {project_info.get('scale', 'N/A')}

**PDF ë¬¸ì„œ ë‚´ìš©:**
{self._get_pdf_content_for_context(pdf_text, max_length=4000, use_long_context=self._is_long_context_model()) if pdf_text else "PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}

## í™•ì¥ ì‚¬ê³  (Extended Thinking) ë° Chain of Thought ë¶„ì„

**ì¤‘ìš”**: ì´ ë¶„ì„ì€ í™•ì¥ ì‚¬ê³ (Extended Thinking) ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ê¹Šì´ ìˆê²Œ ì‚¬ê³ í•˜ê³ , ê° ë‹¨ê³„ì—ì„œ ë°œê²¬í•œ ë‚´ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.

### í™•ì¥ ì‚¬ê³  ë‹¨ê³„ë³„ ê°€ì´ë“œ:

#### 1ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ë¥˜ (í™•ì¥ ì‚¬ê³ )
- **ëª…ì‹œì  ì •ë³´ ì‹ë³„**: ë¬¸ì„œì—ì„œ ì§ì ‘ ì–¸ê¸‰ëœ ëª¨ë“  ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë‚˜ì—´
- **ì•”ì‹œì  ì •ë³´ ì¶”ë¡ **: ë¬¸ì„œì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ ì¶”ë¡  ê°€ëŠ¥í•œ ì •ë³´ ì‹ë³„
  - ê° ì¶”ë¡ ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œ
  - ì¶”ë¡ ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)
- **ì •ë³´ ì‹ ë¢°ë„ í‰ê°€**: ê° ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹ ë¢°ë„ë¥¼ 3ë‹¨ê³„ë¡œ í‰ê°€
  - **ë†’ìŒ**: ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë¨
  - **ì¤‘ê°„**: ë¬¸ì„œì˜ ë§¥ë½ì—ì„œ í•©ë¦¬ì ìœ¼ë¡œ ì¶”ë¡  ê°€ëŠ¥
  - **ë‚®ìŒ**: ì¼ë°˜ì ì¸ ì§€ì‹ì´ë‚˜ ê°€ì •ì— ê¸°ë°˜

#### 2ë‹¨ê³„: í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (í™•ì¥ ì‚¬ê³ )
- **í”„ë¡œì íŠ¸ ëª©í‘œ ë° ë¹„ì „ ë¶„ì„**
  - ëª…ì‹œì  ëª©í‘œì™€ ì•”ì‹œì  ëª©í‘œë¥¼ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬
  - ê° ëª©í‘œì˜ ìš°ì„ ìˆœìœ„ì™€ ì¤‘ìš”ë„ í‰ê°€
  - ëª©í‘œ ê°„ ìƒí˜¸ê´€ê³„ ë¶„ì„
- **ì£¼ìš” ì œì•½ì¡°ê±´ ë° ê¸°íšŒìš”ì†Œ ë¶„ì„**
  - ê° ì œì•½ì¡°ê±´ì´ í”„ë¡œì íŠ¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
  - ê¸°íšŒìš”ì†Œì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ê³¼ ê¸°ëŒ€ íš¨ê³¼ í‰ê°€
  - ì œì•½ì¡°ê±´ê³¼ ê¸°íšŒìš”ì†Œ ê°„ì˜ ìƒí˜¸ì‘ìš© ë¶„ì„
- **ì´í•´ê´€ê³„ì ë° ì˜í–¥ ë²”ìœ„ ë¶„ì„**
  - ì´í•´ê´€ê³„ìë³„ ê´€ì‹¬ì‚¬ì™€ ê¸°ëŒ€ íš¨ê³¼ ë¶„ì„
  - í”„ë¡œì íŠ¸ê°€ ê° ì´í•´ê´€ê³„ìì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ ë²”ìœ„ í‰ê°€

#### 3ë‹¨ê³„: ë¶„ì„ ë° ì¢…í•© (í™•ì¥ ì‚¬ê³ )
- **ê° ìš”ì†Œì˜ ì¤‘ìš”ë„ í‰ê°€**
  - ì •ëŸ‰ì  ì ìˆ˜(1-5ì )ì™€ ì •ì„±ì  í‰ê°€ë¥¼ í•¨ê»˜ ì œì‹œ
  - ì¤‘ìš”ë„ í‰ê°€ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œ
- **ìš”ì†Œ ê°„ ìƒí˜¸ê´€ê³„ ë¶„ì„**
  - ìš”ì†Œ ê°„ ìƒí˜¸ì‘ìš©ì„ ë‹¤ì´ì–´ê·¸ë¨ì´ë‚˜ í‘œë¡œ ì‹œê°í™”
  - ê¸ì •ì /ë¶€ì •ì  ìƒí˜¸ì‘ìš©ì„ êµ¬ë¶„í•˜ì—¬ ë¶„ì„
- **ì¢…í•©ì  í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**
  - ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© í•´ì„
  - í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3-5ê°œë¥¼ ëª…í™•íˆ ì œì‹œ
  - ê° ì¸ì‚¬ì´íŠ¸ì˜ ì‹¤ë¬´ì  ì˜ë¯¸ì™€ ì ìš© ë°©ì•ˆ ì œì‹œ

## ğŸ“‹ í’ˆì§ˆ ê¸°ì¤€ ë° ì œì•½ì¡°ê±´

### í•„ìˆ˜ ì œì•½ì¡°ê±´
- **AI ì¶”ë¡  í‘œì‹œ**: ëª¨ë“  AI ê¸°ë°˜ ì¶”ë¡ ì€ ë°˜ë“œì‹œ '(AI ì¶”ë¡ )' í‘œì‹œ í›„ ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ
- **êµ¬ì²´ì  ê·¼ê±°**: ëª¨ë“  ë¶„ì„ ê²°ê³¼ëŠ” êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì¶œì²˜ í˜ì´ì§€/ì›ë¬¸ ì¸ìš© í•„ìˆ˜
- **í‘œ í•´ì„¤**: ê° ì„¹ì…˜ì˜ í‘œ í•˜ë‹¨ì— í•´ì„¤ ì¶”ê°€ (ìµœì†Œ 4ë¬¸ì¥, ìµœëŒ€ 8ë¬¸ì¥, 300-600ì)
- **ì†Œì œëª© í•´ì„¤**: ëª¨ë“  ì†Œì œëª© ì•„ë˜ì— ìƒì„¸í•œ í•´ì„¤ ì¤„ê¸€ í•„ìˆ˜ (ìµœì†Œ 3-5ë¬¸ì¥, 200-400ì)
- **ë¶„ëŸ‰ ìš”êµ¬**: ì „ì²´ ë¬¸ì„œ ë¶„ëŸ‰ 2000ì ì´ìƒ ì‘ì„± (ê¸°ì¡´ 1200ìì—ì„œ í™•ëŒ€)
- **í‘œì™€ ì„œìˆ **: í‘œì™€ ì„œìˆ ì‹ ì¤„ê¸€ì˜ ì ì ˆí•œ ì¡°í•© í•„ìˆ˜
- **ìƒì„¸ ë¶„ì„**: ê° í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ê°„, ê·œëª¨, ë¹„ìš© ë“±ì„ ìƒì„¸íˆ ì œì‹œ
- **ë‹¤ê°ë„ ë¶„ì„**: ë¬¼ë¦¬ì , ë²•ì , ê²½ì œì , ì‚¬íšŒì  ì¸¡ë©´ì„ ëª¨ë‘ ê³ ë ¤í•œ ì¢…í•© ë¶„ì„
- **ë¹„êµ ë¶„ì„**: ëŒ€ì•ˆì´ ìˆëŠ” ê²½ìš° ìƒì„¸í•œ ë¹„êµ ë¶„ì„í‘œì™€ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„ í•„ìˆ˜

### ë¶„ì„ ê°€ì´ë“œë¼ì¸
- **êµ¬ì²´ì„±**: í‚¤ì›Œë“œë‚˜ ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ êµ¬ì²´ì ì´ê³  ì„œìˆ ì ì¸ ì„¤ëª… ì œê³µ
- **ê·¼ê±° ì œì‹œ**: ëª¨ë“  ê²°ë¡ ì—ëŠ” ëª…í™•í•œ ê·¼ê±°ì™€ ì¶œì²˜ë¥¼ ì œì‹œ
- **í‘œ í™œìš©**: ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ê¸° ìœ„í•´ í‘œ í˜•ì‹ì„ ì ê·¹ í™œìš©
- **ë¬¸ì¥ í˜•íƒœ**: ë¶ˆë¦¿ í¬ì¸íŠ¸ë³´ë‹¤ëŠ” ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…
- **ì‹¤ìš©ì„±**: ì¶”ìƒì ì¸ ë‚´ìš©ë³´ë‹¤ëŠ” ì‹¤ì œ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ë°©ì•ˆ ì œì‹œ
- **ì •ëŸ‰ì  ì •ë³´**: ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ê°„, ê·œëª¨ ë“±ì„ í¬í•¨
- **ë§¥ë½ ì œê³µ**: ê° ì •ë³´ê°€ í”„ë¡œì íŠ¸ ì „ì²´ì—ì„œ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ë§¥ë½ ì„¤ëª…
- **ì‹¬í™” ë¶„ì„**: í‘œë©´ì  ì •ë³´ë¥¼ ë„˜ì–´ì„œ ì‹¬ì¸µì ì´ê³  ì „ë¬¸ì ì¸ ë¶„ì„ ìˆ˜í–‰
- **ì˜ˆì‹œ ì œì‹œ**: êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œë¥¼ í†µí•œ ì„¤ëª… ê°•í™”
- **ë‹¨ê³„ë³„ ë¶„ì„**: ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸íˆ ë¶„ì„
- **ì‹œê°í™” ê³ ë ¤**: ë³µì¡í•œ ë°ì´í„°ëŠ” í‘œë‚˜ ì°¨íŠ¸ í˜•íƒœë¡œ ì •ë¦¬

{self._get_output_format_template()}
"""
        
        try:
            with self._lm_context():
                result = dspy.Predict(SimpleAnalysisSignature)(input=prompt)
            
            return {
                "success": True,
                "analysis": result.output,
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "DSPy + CoT"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "DSPy + CoT"
            }
    
    def analyze_custom_block(
        self, 
        prompt, 
        pdf_text, 
        block_id=None, 
        project_info=None, 
        pdf_path=None, 
        response_schema=None,
        function_declarations: Optional[List[Union[Dict[str, Any], Callable]]] = None,
        function_implementations: Optional[Dict[str, Callable]] = None,
        automatic_function_calling: bool = False,
        function_calling_mode: str = "AUTO",
        max_iterations: int = 10,
        many_shot_examples: Optional[List[str]] = None
    ):
        """
        ì‚¬ìš©ì ì •ì˜ ë¸”ë¡ ë¶„ì„ - ë¸”ë¡ë³„ ê³ ìœ  í”„ë¡¬í”„íŠ¸ì™€ Signature ì‚¬ìš©
        
        Args:
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            pdf_text: PDF í…ìŠ¤íŠ¸ (ê¸°ì¡´ ë°©ì‹)
            block_id: ë¸”ë¡ ID
            project_info: í”„ë¡œì íŠ¸ ì •ë³´
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ (Gemini ë„¤ì´í‹°ë¸Œ ì²˜ë¦¬ìš©, ì„ íƒì‚¬í•­)
            response_schema: êµ¬ì¡°í™”ëœ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (Pydantic ëª¨ë¸ í´ë˜ìŠ¤ ë˜ëŠ” JSON ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬, ì„ íƒì‚¬í•­)
            function_declarations: Function declaration ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” Python í•¨ìˆ˜)
            function_implementations: í•¨ìˆ˜ ì´ë¦„ -> í•¨ìˆ˜ êµ¬í˜„ ë§¤í•‘ (ìë™ ì‹¤í–‰ìš©)
            automatic_function_calling: ìë™ í•¨ìˆ˜ ì‹¤í–‰ ì—¬ë¶€
            function_calling_mode: Function calling ëª¨ë“œ ("AUTO", "ANY", "NONE")
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (compositional calling)
            many_shot_examples: Many-shot learning ì˜ˆì œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        """
        try:
            # Gemini ë„¤ì´í‹°ë¸Œ PDF ì²˜ë¦¬ ì‚¬ìš© (pdf_pathê°€ ì œê³µë˜ê³  ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
            if self.use_gemini_native_pdf and pdf_path:
                try:
                    from pdf_analyzer import extract_text_with_gemini_pdf
                    
                    print(f"ğŸ“„ Gemini ë„¤ì´í‹°ë¸Œ PDF ì²˜ë¦¬ ì‚¬ìš©: {pdf_path}")
                    gemini_result = extract_text_with_gemini_pdf(
                        pdf_path=pdf_path,
                        prompt="ì´ PDF ë¬¸ì„œë¥¼ ì „ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸, í…Œì´ë¸”ì„ í¬í•¨í•œ ëª¨ë“  ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                    )
                    
                    if gemini_result.get("success"):
                        pdf_text = gemini_result.get("text", pdf_text)
                        print(f"âœ… Gemini PDF ì²˜ë¦¬ ì™„ë£Œ (ë°©ì‹: {gemini_result.get('method', 'unknown')})")
                    else:
                        print(f"âš ï¸ Gemini PDF ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ì¡´ í…ìŠ¤íŠ¸ ì‚¬ìš©: {gemini_result.get('error')}")
                except Exception as e:
                    print(f"âš ï¸ Gemini PDF ì²˜ë¦¬ ì˜¤ë¥˜, ê¸°ì¡´ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
            
            # ë¸”ë¡ IDì— ë”°ë¼ ì ì ˆí•œ Signature ì„ íƒ (ë™ì  ìƒì„±)
            signature_map = self._build_signature_map()
            
            # ê¸°ë³¸ Signature ì‚¬ìš© (ë¸”ë¡ IDê°€ ì—†ê±°ë‚˜ ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš°)
            signature_class = signature_map.get(block_id, SimpleAnalysisSignature)
            
            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print(f"ğŸ” DSPy ë¶„ì„ ë””ë²„ê¹…:")
            print(f"   ë¸”ë¡ ID: {block_id}")
            print(f"   ì‚¬ìš©í•  Signature: {signature_class.__name__}")
            print(f"   í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            print(f"   PDF í…ìŠ¤íŠ¸ ê¸¸ì´: {len(pdf_text) if pdf_text else 0}ì")
            print(f"   í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:200]}...")
            
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (íŠ¹ì • ë¸”ë¡ì— ëŒ€í•´ì„œë§Œ)
            web_search_context = ""
            if block_id and project_info:
                try:
                    web_search_context = get_web_search_context(block_id, project_info, pdf_text)
                    if web_search_context:
                        print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {block_id}")
                except Exception as e:
                    print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            
            # RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (PDF í…ìŠ¤íŠ¸ê°€ ê¸¸ê±°ë‚˜ ì°¸ê³  ë¬¸ì„œê°€ ìˆì„ ë•Œ)
            rag_context = ""
            if RAG_AVAILABLE and pdf_text and len(pdf_text) > 5000:
                try:
                    # í”„ë¡¬í”„íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
                    query_keywords = prompt[:500] if prompt else ""
                    
                    # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ë¶€ë¶„ ê²€ìƒ‰
                    rag_system = build_rag_system_for_documents(
                        documents=[pdf_text],
                        chunk_size=1000,
                        overlap=200
                    )
                    
                    # í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    relevant_contexts = query_rag_system(
                        rag_system=rag_system,
                        query=query_keywords,
                        top_k=3,
                        build_prompt=False
                    )
                    
                    if relevant_contexts:
                        context_texts = [ctx for ctx, _ in relevant_contexts]
                        rag_context = "\n\n".join(context_texts)
                        print(f"ğŸ“š RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_contexts)}ê°œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë°œê²¬")
                except Exception as e:
                    print(f"âš ï¸ RAG ê²€ìƒ‰ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ PDF í…ìŠ¤íŠ¸ë¡œ ì¹˜í™˜ (ë‹¨ì¼ ë¸”ë¡ ë¶„ì„ìš©)
            formatted_prompt = prompt
            if "{pdf_text}" in prompt:
                # ì‹¤ì œ PDF í…ìŠ¤íŠ¸ë¥¼ ì‚½ì… (ê¸¸ì´ ì œí•œ ê³ ë ¤)
                pdf_content = pdf_text if pdf_text else "PDF ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                # RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                if rag_context:
                    pdf_content = rag_context
                    print("ğŸ“š RAGë¡œ ì¶”ì¶œí•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (í† í° ì œí•œ ê³ ë ¤)
                else:
                    # Long Context ëª¨ë¸ ê°ì§€ ë° ì œí•œ ì™„í™”
                    current_provider = self._active_provider or get_current_provider()
                    is_long_context_model = current_provider in ['gemini', 'gemini_3pro', 'gemini_25pro', 'gemini_25flash']

                    # Long Context ëª¨ë¸ì˜ ê²½ìš° ë” ê¸´ í…ìŠ¤íŠ¸ í—ˆìš©
                    # 1M í† í° â‰ˆ 750,000 ë¬¸ì (ëŒ€ëµì ì¸ ë³€í™˜: 1 í† í° â‰ˆ 0.75 ë¬¸ì)
                    # ì•ˆì „ ë§ˆì§„ì„ ë‘ê³  500,000 ë¬¸ìë¡œ ì œí•œ
                    long_context_limit = 500000 if is_long_context_model else 12000

                    if len(pdf_content) > long_context_limit:
                        if is_long_context_model:
                            # Long Context ëª¨ë¸: ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ì „ì²´ ì‚¬ìš©
                            print(f"ğŸ“„ ê¸´ ë¬¸ì„œ ê°ì§€ ({len(pdf_content):,}ì). Long Context ëª¨ë¸ì´ë¯€ë¡œ ì „ì²´ ë‚´ìš©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            print(f"   ì°¸ê³ : ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” ë¹„ìš©ê³¼ ì§€ì—°ì‹œê°„ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            # ì¼ë°˜ ëª¨ë¸: ê¸°ì¡´ ì œí•œ ì ìš©
                            pdf_content = pdf_content[:12000] + "\n\n[ë¬¸ì„œ ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ìœ„ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”...]"
                            print(f"âš ï¸ ë¬¸ì„œê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(pdf_content)}ì). ì•ë¶€ë¶„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                formatted_prompt = prompt.replace("{pdf_text}", pdf_content)
            
            # ë¬¸ì„œ ê¸°ë°˜ ì¶”ë¡  ê°•ì¡° ì§€ì‹œì‚¬í•­ ì¶”ê°€
            document_based_instruction = f"""

## ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ í•„ìˆ˜ ì§€ì‹œì‚¬í•­

**âš ï¸ ë§¤ìš° ì¤‘ìš”**: ì•„ë˜ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

### 1. ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì¶”ë¡  í•„ìˆ˜
- **ìœ„ì— ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•íˆ ì½ê³  ì´í•´í•œ í›„ ë¶„ì„í•˜ì„¸ìš”**
- **ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„ì— í™œìš©í•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ í…œí”Œë¦¿ì´ë‚˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì´ ì•„ë‹Œ, ì´ íŠ¹ì • í”„ë¡œì íŠ¸ ë¬¸ì„œì˜ ì‹¤ì œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”**

### 2. ë¬¸ì„œ ì¸ìš© ë° ê·¼ê±° ì œì‹œ í•„ìˆ˜
- **ë¶„ì„ ê²°ê³¼ì˜ ëª¨ë“  ì£¼ìš” ì£¼ì¥ì€ ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë’·ë°›ì¹¨í•˜ì„¸ìš”**
- **ì˜ˆì‹œ**: "ë¬¸ì„œ 3í˜ì´ì§€ì— 'ëŒ€ì§€ë©´ì  5,000ã¡'ë¼ê³  ëª…ì‹œë˜ì–´ ìˆì–´..." í˜•ì‹ìœ¼ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
- **ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ì„ ì œì‹œí•  ë•ŒëŠ” ë°˜ë“œì‹œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”**

### 3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ìƒì„±í•˜ì§€ ë§ ê²ƒ
- **ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”**
- **ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ë˜ëŠ” 'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¡œ í‘œì‹œí•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ ê±´ì¶• í”„ë¡œì íŠ¸ì˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”**

### 4. ë¬¸ì„œ ë‚´ìš©ì˜ êµ¬ì²´ì  í™œìš©
- **ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ëª…ì¹­, ìœ„ì¹˜, ê·œëª¨ ë“±ì„ ë¶„ì„ì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”**
- **ë¬¸ì„œì˜ ë§¥ë½ê³¼ ë°°ê²½ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”**
- **ë¬¸ì„œì˜ ì•”ì‹œì  ì˜ë¯¸ë‚˜ ì—°ê´€ëœ ìš”êµ¬ì‚¬í•­ì„ ì¶”ë¡ í•˜ì—¬ ë¶„ì„ì„ í’ë¶€í•˜ê²Œ ë§Œë“¤ë˜, ì¶”ë¡ ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”**

### 5. ë¶„ì„ ê²°ê³¼ì˜ ì°¨ë³„í™”
- **ì´ í”„ë¡œì íŠ¸ë§Œì˜ ê³ ìœ í•œ íŠ¹ì§•ê³¼ ìš”êµ¬ì‚¬í•­ì„ ê°•ì¡°í•˜ì„¸ìš”**
- **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì™€ êµ¬ë³„ë˜ëŠ” íŠ¹ë³„í•œ ë‚´ìš©ì„ ì°¾ì•„ ë¶„ì„í•˜ì„¸ìš”**
- **ë¬¸ì„œì—ì„œ ë°œê²¬í•œ íŠ¹ì´ì‚¬í•­, ì œì•½ì¡°ê±´, ê¸°íšŒìš”ì†Œ ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”**

**ìœ„ ì§€ì‹œì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶„ì„ì´ ë°˜ë³µë˜ê±°ë‚˜ ì¼ë°˜ë¡ ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.**
"""
            
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            if web_search_context:
                formatted_prompt = f"""{formatted_prompt}

{web_search_context}

**ì¤‘ìš”**: ìœ„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì‹  ì •ë³´ì™€ ì‹œì¥ ë™í–¥ì„ ë°˜ì˜í•œ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ë‹¨, ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ë³´ì™„í•˜ëŠ” ì—­í• ì´ë©°, ë¶„ì„ì˜ ì£¼ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–»ì€ ì •ë³´ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³ , ë¬¸ì„œ ë‚´ìš©ê³¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
"""
            else:
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ ê°•ì¡°
                formatted_prompt = f"""{formatted_prompt}{document_based_instruction}"""
            
            # í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì¶”ê°€ (ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ ì ìš©)
            # ë¸”ë¡ í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ Chain of Thought ì§€ì‹œì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ë¸”ë¡ ëª©ë¡
            # (ì´ ë¸”ë¡ë“¤ì€ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ë ˆë²¨ ì§€ì‹œì‚¬í•­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
            blocks_with_builtin_cot = ['phase1_facility_program']
            
            # ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ì ìœ¼ë¡œ í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì ìš© (ì¤‘ë³µ ë°©ì§€ ì œì™¸)
            extended_thinking_note = ""
            if block_id and block_id not in blocks_with_builtin_cot:
                # ì‹œìŠ¤í…œ ë ˆë²¨ í™•ì¥ ì‚¬ê³  í…œí”Œë¦¿ ì‚¬ìš©
                extended_thinking_note = self._get_extended_thinking_template()
            
            # Many-shot Learning ì§€ì›
            many_shot_section = ""
            if many_shot_examples:
                is_long_context_model = self._is_long_context_model()
                if is_long_context_model:
                    print(f"ğŸ“š Many-shot Learning í™œì„±í™”: {len(many_shot_examples)}ê°œ ì˜ˆì œ")
                    many_shot_section = "\n\n## ì˜ˆì œ (Many-shot Learning)\n\n"
                    for i, example in enumerate(many_shot_examples, 1):
                        many_shot_section += f"### ì˜ˆì œ {i}\n{example}\n\n"
            
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”: Long Contextì—ì„œëŠ” ì¿¼ë¦¬ë¥¼ ëì— ë°°ì¹˜
            # Long Contextì—ì„œëŠ” ì¿¼ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ì˜ ëì— ë°°ì¹˜í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì 
            is_long_context = self._is_long_context_model()
            
            if is_long_context and "{pdf_text}" in prompt:
                # Long Context ìµœì í™”: ì¿¼ë¦¬(í”„ë¡¬í”„íŠ¸)ë¥¼ ëì— ë°°ì¹˜
                # formatted_promptëŠ” ì´ë¯¸ PDF í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŒ
                # í”„ë¡¬í”„íŠ¸ì—ì„œ PDF í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ì¿¼ë¦¬ ë¶€ë¶„ ì¶”ì¶œ
                prompt_without_pdf = prompt.replace("{pdf_text}", "").strip()
                if not prompt_without_pdf:
                    prompt_without_pdf = "ìœ„ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
                
                # ì¶œë ¥ í˜•ì‹ ìš”êµ¬ì‚¬í•­ ì¶”ê°€
                enhanced_prompt = f"""
{formatted_prompt}

{extended_thinking_note}

{many_shot_section}

{self._get_output_format_template() if response_schema is None else ""}

---
## ë¶„ì„ ìš”ì²­
{prompt_without_pdf}
"""
            else:
                # ê¸°ì¡´ ë°©ì‹: í”„ë¡¬í”„íŠ¸ â†’ ì§€ì‹œì‚¬í•­
                enhanced_prompt = f"""
{formatted_prompt}{extended_thinking_note}

{many_shot_section}

{self._get_output_format_template() if response_schema is None else ""}
"""
            
            # Function Calling ì§€ì› (Gemini ëª¨ë¸ì—ì„œ function_declarationsê°€ ì œê³µëœ ê²½ìš°)
            if function_declarations:
                current_provider = self._active_provider or get_current_provider()
                if current_provider in ['gemini', 'gemini_3pro', 'gemini_25pro', 'gemini_25flash']:
                    try:
                        return self._analyze_with_function_calling(
                            prompt=enhanced_prompt,
                            pdf_text=pdf_text,
                            function_declarations=function_declarations,
                            function_implementations=function_implementations or {},
                            automatic_function_calling=automatic_function_calling,
                            function_calling_mode=function_calling_mode,
                            max_iterations=max_iterations,
                            response_schema=response_schema,
                            block_id=block_id
                        )
                    except Exception as e:
                        print(f"âš ï¸ Function calling ì‚¬ìš© ì‹¤íŒ¨, ê¸°ì¡´ DSPy ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}")
                        # í´ë°±: ê¸°ì¡´ DSPy ë°©ì‹ ì‚¬ìš©
            
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ ì§€ì› (Gemini ëª¨ë¸ì—ì„œ response_schemaê°€ ì œê³µëœ ê²½ìš°)
            structured_output_config = self._get_structured_output_config(response_schema)
            if structured_output_config:
                current_provider = self._active_provider or get_current_provider()
                if current_provider in ['gemini', 'gemini_3pro', 'gemini_25pro', 'gemini_25flash']:
                    try:
                        # LiteLLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©
                        import litellm
                        
                        provider_config = PROVIDER_CONFIG.get(current_provider)
                        api_key = get_api_key(current_provider)
                        base_model_name = provider_config['model']
                        clean_model = base_model_name.replace('models/', '').replace('model/', '')
                        litellm_model = f"gemini/{clean_model}"
                        
                        # temperatureì™€ max_tokens ê°€ì ¸ì˜¤ê¸°
                        temp_value = 0.2
                        max_tokens_value = 16000
                        try:
                            import streamlit as st
                            temp_value = float(st.session_state.get('llm_temperature', 0.2))
                            max_tokens_value = int(st.session_state.get('llm_max_tokens', 16000))
                        except Exception:
                            temp_value = float(os.environ.get('LLM_TEMPERATURE', 0.2))
                            max_tokens_value = int(os.environ.get('LLM_MAX_TOKENS', 16000))
                        
                        schema_name = 'JSON Schema'
                        if isinstance(response_schema, dict):
                            schema_name = 'JSON Schema'
                        elif PYDANTIC_AVAILABLE and hasattr(response_schema, '__name__'):
                            schema_name = response_schema.__name__
                        
                        print(f"ğŸ”· êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©: {schema_name}")
                        
                        # Context Caching ì§€ì› (ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                        use_context_caching = False
                        if pdf_text and len(pdf_text) > 50000:  # 50,000ì ì´ìƒì¼ ë•Œ ìºì‹± ê³ ë ¤
                            try:
                                import hashlib
                                cache_key = hashlib.md5(pdf_text.encode()).hexdigest()
                                use_context_caching = True
                                print(f"ğŸ’¾ Context Caching í™œì„±í™”: ê¸´ ë¬¸ì„œ ({len(pdf_text):,}ì)")
                            except Exception:
                                pass
                        
                        # LiteLLM í˜¸ì¶œ
                        call_kwargs = {
                            "model": litellm_model,
                            "messages": [{"role": "user", "content": enhanced_prompt}],
                            "api_key": api_key,
                            "extra_body": structured_output_config,
                            "temperature": temp_value,
                            "max_tokens": max_tokens_value
                        }
                        
                        # Context Caching í™œì„±í™”
                        if use_context_caching:
                            call_kwargs["caching"] = True
                        
                        response = litellm.completion(**call_kwargs)
                        
                        # ì‘ë‹µ íŒŒì‹±
                        response_text = response.choices[0].message.content
                        parsed_response = self._parse_structured_response(response_text, response_schema)
                        
                        return {
                            "success": True,
                            "analysis": parsed_response if PYDANTIC_AVAILABLE and isinstance(parsed_response, BaseModel) else response_text,
                            "parsed_data": parsed_response,
                            "model": self._get_current_model_info(" (Structured Output)"),
                            "method": f"Structured Output + {signature_class.__name__}",
                            "block_id": block_id
                        }
                    except Exception as e:
                        print(f"âš ï¸ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš© ì‹¤íŒ¨, ê¸°ì¡´ DSPy ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}")
                        # í´ë°±: ê¸°ì¡´ DSPy ë°©ì‹ ì‚¬ìš©
            
            # DSPy Predict ì‚¬ìš© (ë¸”ë¡ë³„ íŠ¹í™” signature í¬í•¨)
            result = dspy.Predict(signature_class)(input=enhanced_prompt)
            
            return {
                "success": True,
                "analysis": result.output,
                "model": self._get_current_model_info(" (DSPy)"),
                "method": f"DSPy + {signature_class.__name__}",
                "block_id": block_id
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": f"DSPy + {signature_class.__name__ if 'signature_class' in locals() else 'Unknown'}",
                "block_id": block_id
            }
    
    def validate_analysis_quality(self, analysis_result, block_type="general"):
        """ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ - ê°œì„ ëœ ë²„ì „"""
        try:
            # ë¸”ë¡ë³„ íŠ¹í™” ê²€ì¦ ê¸°ì¤€
            validation_criteria = {
                "basic_info": {
                    "name": "ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ",
                    "criteria": [
                        "í”„ë¡œì íŠ¸ëª…, ìœ„ì¹˜, ê·œëª¨ ë“± í•µì‹¬ ì •ë³´ê°€ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ê°€?",
                        "í‘œ í˜•íƒœë¡œ ì •ë³´ê°€ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬ë˜ì—ˆëŠ”ê°€?",
                        "ê° í‘œ í•˜ë‹¨ì— ìƒì„¸í•œ í•´ì„¤ì´ ìˆëŠ”ê°€?",
                        "ì†Œì œëª©ë³„ë¡œ ì„œìˆ í˜• ì„¤ëª…ì´ ìˆëŠ”ê°€?",
                        "ë¬¸ì„œ ì¶œì²˜ì™€ ê·¼ê±°ê°€ ëª…ì‹œë˜ì—ˆëŠ”ê°€?"
                    ],
                    "weights": [0.25, 0.25, 0.2, 0.15, 0.15]
                },
                "requirements": {
                    "name": "ê±´ì¶• ìš”êµ¬ì‚¬í•­ ë¶„ì„",
                    "criteria": [
                        "ìš”êµ¬ì‚¬í•­ì´ ì²´ê³„ì ìœ¼ë¡œ ì‹ë³„ë˜ê³  ë¶„ë¥˜ë˜ì—ˆëŠ”ê°€?",
                        "ìš°ì„ ìˆœìœ„ê°€ ëª…í™•í•˜ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ê°€?",
                        "ìš”êµ¬ì‚¬í•­ ë§¤íŠ¸ë¦­ìŠ¤ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?",
                        "ì„¤ê³„ ë°©í–¥ì´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?",
                        "í‘œ í•´ì„¤ê³¼ ì„œìˆ í˜• ì„¤ëª…ì´ ì¶©ë¶„í•œê°€?"
                    ],
                    "weights": [0.3, 0.2, 0.2, 0.2, 0.1]
                },
                "design_suggestions": {
                    "name": "ì„¤ê³„ ì œì•ˆ",
                    "criteria": [
                        "í˜„í™© ë¶„ì„ì´ ì •í™•í•˜ê³  í¬ê´„ì ì¸ê°€?",
                        "ì„¤ê³„ ì»¨ì…‰ì´ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ê°€?",
                        "ê³µê°„ êµ¬ì„±ì•ˆì´ ì‹¤í˜„ ê°€ëŠ¥í•œê°€?",
                        "ì‹¤í–‰ ê³„íšì´ ë‹¨ê³„ë³„ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?",
                        "ì „ì²´ì ì¸ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì„±ì´ ìˆëŠ”ê°€?"
                    ],
                    "weights": [0.2, 0.3, 0.25, 0.15, 0.1]
                },
                "accessibility_analysis": {
                    "name": "ì ‘ê·¼ì„± í‰ê°€",
                    "criteria": [
                        "êµí†µ, ë³´í–‰, ì‹œì„¤, ì¥ì• ì¸ ì ‘ê·¼ì„±ì´ ëª¨ë‘ í‰ê°€ë˜ì—ˆëŠ”ê°€?",
                        "5ì  ì²™ë„ë¡œ ê°ê´€ì ì¸ ì ìˆ˜ê°€ ì‚°ì¶œë˜ì—ˆëŠ”ê°€?",
                        "ê°œì„  ë°©ì•ˆì´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?",
                        "ì ìˆ˜ ì‚°ì¶œ ê·¼ê±°ê°€ ëª…í™•í•œê°€?",
                        "ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë¡œë“œë§µì´ ìˆëŠ”ê°€?"
                    ],
                    "weights": [0.25, 0.2, 0.25, 0.15, 0.15]
                },
                "zoning_verification": {
                    "name": "ë²•ê·œ ê²€ì¦",
                    "criteria": [
                        "ìš©ë„ì§€ì—­, ê±´ì¶•ë²•ê·œ, íŠ¹ë³„ë²•ì´ ëª¨ë‘ ê²€í† ë˜ì—ˆëŠ”ê°€?",
                        "ë²•ì  ìœ„í—˜ìš”ì†Œê°€ ì •í™•í•˜ê²Œ ì‹ë³„ë˜ì—ˆëŠ”ê°€?",
                        "ìœ„í—˜ë„ë³„ ë¶„ë¥˜ê°€ ì ì ˆí•œê°€?",
                        "ëŒ€ì‘ë°©ì•ˆì´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?",
                        "ë²•ë ¹ ì¡°í•­ê³¼ ê·¼ê±°ê°€ ëª…í™•í•œê°€?"
                    ],
                    "weights": [0.25, 0.25, 0.15, 0.2, 0.15]
                },
                "capacity_estimation": {
                    "name": "ìˆ˜ìš©ë ¥ ì¶”ì •",
                    "criteria": [
                        "ë¬¼ë¦¬ì , ë²•ì , ì‚¬íšŒì , ê²½ì œì  ìˆ˜ìš©ë ¥ì´ ëª¨ë‘ ë¶„ì„ë˜ì—ˆëŠ”ê°€?",
                        "ì •ëŸ‰ì  ê³„ì‚°ê³¼ ìˆ˜ì¹˜ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?",
                        "ìµœì  ê°œë°œ ê·œëª¨ê°€ ì œì‹œë˜ì—ˆëŠ”ê°€?",
                        "ë‹¨ê³„ë³„ ê°œë°œ ë°©ì•ˆì´ êµ¬ì²´ì ì¸ê°€?",
                        "ê³„ì‚° ê³¼ì •ê³¼ ê·¼ê±°ê°€ ëª…í™•í•œê°€?"
                    ],
                    "weights": [0.3, 0.25, 0.2, 0.15, 0.1]
                },
                "feasibility_analysis": {
                    "name": "ì‚¬ì—…ì„± í‰ê°€",
                    "criteria": [
                        "ì‹œì¥ì„±, ìˆ˜ìµì„±, ìœ„í—˜ì„±, ìê¸ˆì¡°ë‹¬ì„±ì´ ëª¨ë‘ í‰ê°€ë˜ì—ˆëŠ”ê°€?",
                        "ê° ê¸°ì¤€ë³„ 1-5ì  í‰ê°€ê°€ ê°ê´€ì ì¸ê°€?",
                        "ì¢…í•© ì ìˆ˜ ì‚°ì¶œì´ ì ì ˆí•œê°€?",
                        "Go/No-Go ê²°ì • ê·¼ê±°ê°€ ëª…í™•í•œê°€?",
                        "íˆ¬ì ê¶Œê³ ì•ˆì´ ì‹¤ìš©ì ì¸ê°€?"
                    ],
                    "weights": [0.3, 0.2, 0.2, 0.15, 0.15]
                }
            }
            
            # ê¸°ë³¸ ê²€ì¦ ê¸°ì¤€ (ì¼ë°˜ì ì¸ ê²½ìš°)
            general_criteria = {
                "name": "ì¼ë°˜ ë¶„ì„",
                "criteria": [
                    "ë¶„ì„ ì™„ì„±ë„ê°€ ë†’ì€ê°€?",
                    "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê²°ë¡ ì´ ìˆëŠ”ê°€?",
                    "ì²´ê³„ì ì¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ê°€?",
                    "ê·¼ê±°ì™€ ì¶œì²˜ê°€ ëª…ì‹œë˜ì—ˆëŠ”ê°€?",
                    "ì‹¤ìš©ì ì¸ ì •ë³´ì¸ê°€?"
                ],
                "weights": [0.2, 0.2, 0.2, 0.2, 0.2]
            }
            
            # ë¸”ë¡ë³„ ê²€ì¦ ê¸°ì¤€ ì„ íƒ
            criteria_info = validation_criteria.get(block_type, general_criteria)
            
            validation_prompt = f"""
ë‹¤ìŒ {criteria_info['name']} ë¶„ì„ ê²°ê³¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•´ì£¼ì„¸ìš”:

**ë¶„ì„ ê²°ê³¼:**
{analysis_result}

**ê²€ì¦ ê¸°ì¤€:**
{chr(10).join([f"{i+1}. {criterion}" for i, criterion in enumerate(criteria_info['criteria'])])}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê²€ì¦ ê²°ê³¼ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”:

## ğŸ“Š í’ˆì§ˆ ê²€ì¦ ê²°ê³¼

### ğŸ“‹ í•­ëª©ë³„ ì ìˆ˜ í‰ê°€ (ê° í•­ëª© 1-5ì )
{chr(10).join([f"- **í•­ëª© {i+1}**: [ì ìˆ˜]/5 - [ê°„ë‹¨í•œ í‰ê°€ ê·¼ê±°]" for i in range(len(criteria_info['criteria']))])}

### ğŸ“ˆ ì¢…í•© ì ìˆ˜: [ì´ì ]/25ì 
### ğŸ† í’ˆì§ˆ ë“±ê¸‰: [ìš°ìˆ˜/ì–‘í˜¸/ë³´í†µ/ë¯¸í¡/ë¶€ì¡±]

### âœ… ìš°ìˆ˜í•œ ë¶€ë¶„
- [ì˜ëœ ë¶€ë¶„ë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´]

### ğŸ”§ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„
- [ê°œì„ ì´ í•„ìš”í•œ í•­ëª©ë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´]

### ğŸ“ êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ
- [ê° ê°œì„  í•­ëª©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì œì•ˆì‚¬í•­]
"""
            
            with self._lm_context():
                result = dspy.Predict(AnalysisQualityValidator)(
                    analysis_result=validation_prompt,
                    validation_criteria=str(criteria_info['criteria'])
                )
            
            return {
                "success": True,
                "validation": result.output,
                "block_type": block_type,
                "criteria_info": criteria_info,
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "DSPy + Enhanced AnalysisQualityValidator"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "block_type": block_type,
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "DSPy + AnalysisQualityValidator"
            }
    
    def enhanced_analyze_with_validation(self, project_info, pdf_text, block_type="general"):
        """ê²€ì¦ì´ í¬í•¨ëœ í–¥ìƒëœ ë¶„ì„"""
        try:
            # ë¶„ì„ ìˆ˜í–‰
            analysis_result = self.analyze_project(project_info, pdf_text)
            
            if not analysis_result["success"]:
                return analysis_result
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "success": True,
                "analysis": analysis_result["analysis"],
                "model": analysis_result["model"],
                "method": analysis_result["method"],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "Enhanced Analysis with Validation"
            }
    
    def _extract_quality_score(self, validation_text):
        """ê²€ì¦ ê²°ê³¼ì—ì„œ í’ˆì§ˆ ì ìˆ˜ ì¶”ì¶œ"""
        import re
        try:
            # "ì¢…í•© ì ìˆ˜: [ì´ì ]/25ì " íŒ¨í„´ ì°¾ê¸°
            score_pattern = r'ì¢…í•© ì ìˆ˜:\s*(\d+)/25'
            match = re.search(score_pattern, validation_text)
            if match:
                return int(match.group(1))
            return None
        except:
            return None
    
    def _extract_quality_grade(self, validation_text):
        """ê²€ì¦ ê²°ê³¼ì—ì„œ í’ˆì§ˆ ë“±ê¸‰ ì¶”ì¶œ"""
        import re
        try:
            # "í’ˆì§ˆ ë“±ê¸‰: [ë“±ê¸‰]" íŒ¨í„´ ì°¾ê¸°
            grade_pattern = r'í’ˆì§ˆ ë“±ê¸‰:\s*([ê°€-í£]+)'
            match = re.search(grade_pattern, validation_text)
            if match:
                return match.group(1)
            return "ë¯¸í‰ê°€"
        except:
            return "ë¯¸í‰ê°€"
    
    def initialize_cot_session(self, project_info: Dict[str, Any], pdf_text: str, total_blocks: int) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ CoT ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        return {
            "project_info": project_info,
            "pdf_text": pdf_text or "",
            "previous_results": {},
            "cot_history": [],
            "total_blocks": total_blocks
        }

    def run_cot_step(
        self,
        block_id: str,
        block_info: Dict[str, Any],
        cot_session: Dict[str, Any],
        progress_callback=None,
        step_index: Optional[int] = None,
        feedback: Optional[str] = None,
        feedback_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¸”ë¡ì— ëŒ€í•œ CoT ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

        Args:
            block_id: ë¸”ë¡ ID
            block_info: ë¸”ë¡ ì •ë³´
            cot_session: CoT ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ ì½œë°± í•¨ìˆ˜
            step_index: í˜„ì¬ ë‹¨ê³„ ì¸ë±ìŠ¤
            feedback: í”¼ë“œë°± í…ìŠ¤íŠ¸
            feedback_type: í”¼ë“œë°± ìœ í˜• (perspective_shift, constraint_addition ë“±)
        """
        try:
            print(f"[DEBUG] run_cot_step ì‹œì‘: block_id={block_id}")
            print(f"[DEBUG] cot_session previous_results keys: {list(cot_session.get('previous_results', {}).keys())}")
            current_step = step_index if step_index is not None else len(cot_session["previous_results"]) + 1
            print(f"[DEBUG] current_step={current_step}")
            context_for_current_block = self._build_cot_context(
                cot_session,
                block_info,
                current_step,
                feedback_notes=feedback,
                feedback_type=feedback_type
            )
            project_info = cot_session.get("project_info")
            
            # ìµœì í™”ëœ thinking_budget ê³„ì‚°
            current_provider = get_current_provider()
            provider_config = PROVIDER_CONFIG.get(current_provider, {})
            model_name = provider_config.get('model', '')
            optimal_thinking_budget = self._get_optimal_thinking_budget(block_id, block_info, model_name)
            
            # ìµœì í™”ëœ temperature ê³„ì‚°
            optimal_temperature = self._get_optimal_temperature(block_id, block_info)
            
            print(f"[DEBUG] _analyze_block_with_cot_context í˜¸ì¶œ ì‹œì‘...")
            import time
            start_time = time.time()

            result = self._analyze_block_with_cot_context(
                context_for_current_block,
                block_info,
                block_id,
                project_info,
                thinking_budget=optimal_thinking_budget,
                temperature=optimal_temperature,
                enable_streaming=True,  # CoT ë¶„ì„ì—ì„œëŠ” ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                progress_callback=progress_callback
            )

            elapsed_time = time.time() - start_time
            print(f"[DEBUG] _analyze_block_with_cot_context ì™„ë£Œ. ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            print(f"[DEBUG] result success: {result.get('success')}, method: {result.get('method')}")

            if not result.get("success"):
                return result

            key_insights = self._extract_key_insights(result['analysis'])
            cot_session["previous_results"][block_id] = result['analysis']
            cot_session["cot_history"] = [
                entry for entry in cot_session["cot_history"] if entry.get('block_id') != block_id
            ]
            cot_session["cot_history"].append({
                "block_id": block_id,
                "block_name": block_info.get('name', 'Unknown'),
                "step": current_step,
                "key_insights": key_insights
            })
            cot_session["cot_history"].sort(key=lambda entry: entry.get('step', 0))

            if progress_callback:
                block_name = block_info.get('name', block_id)
                progress_callback(f"âœ… {block_name} ë¸”ë¡ ë¶„ì„ ì™„ë£Œ")

            return {
                "success": True,
                "analysis": result['analysis'],
                "cot_session": cot_session,
                "key_insights": key_insights,
                "feedback": feedback,
                "model": result.get("model"),
                "method": result.get("method"),
                "all_citations": result.get("all_citations", [])
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "Block Chain of Thought Analysis"
            }

    def analyze_blocks_with_cot(self, selected_blocks, project_info, pdf_text, block_infos, progress_callback=None):
        """ë¸”ë¡ ê°„ Chain of Thought ë¶„ì„"""
        try:
            cumulative_context = self.initialize_cot_session(project_info, pdf_text, len(selected_blocks))
            
            analysis_results = {}
            
            print(f"ğŸ”— ë¸”ë¡ ê°„ Chain of Thought ë¶„ì„ ì‹œì‘: {len(selected_blocks)}ê°œ ë¸”ë¡")
            if progress_callback:
                progress_callback(f"ğŸ”— ë¸”ë¡ ê°„ Chain of Thought ë¶„ì„ ì‹œì‘: {len(selected_blocks)}ê°œ ë¸”ë¡")
            
            for i, block_id in enumerate(selected_blocks):
                block_name = block_infos.get(block_id, {}).get('name', block_id)
                print(f"ğŸ“Š {i+1}/{len(selected_blocks)} ë¸”ë¡ ë¶„ì„ ì¤‘: {block_id}")
                if progress_callback:
                    progress_callback(f"ğŸ“Š {i+1}/{len(selected_blocks)} ë¸”ë¡ ë¶„ì„ ì¤‘: {block_name}")
                
                # í˜„ì¬ ë¸”ë¡ ì •ë³´ ì°¾ê¸°
                block_info = block_infos.get(block_id)
                if not block_info:
                    print(f"[X] ë¸”ë¡ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {block_id}")
                    if progress_callback:
                        progress_callback(f"[X] ë¸”ë¡ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {block_id}")
                    continue
                
                step_result = self.run_cot_step(
                    block_id,
                    block_info,
                    cumulative_context,
                    progress_callback=progress_callback,
                    step_index=i + 1
                )
                
                if step_result.get('success'):
                    analysis_results[block_id] = step_result['analysis']
                    cumulative_context = step_result['cot_session']
                    print(f"âœ… {block_id} ë¸”ë¡ ì™„ë£Œ")
                else:
                    print(f"[X] {block_id} ë¸”ë¡ ì‹¤íŒ¨: {step_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    if progress_callback:
                        progress_callback(f"[X] {block_name} ë¸”ë¡ ì‹¤íŒ¨: {step_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            print("ğŸ‰ ëª¨ë“  ë¸”ë¡ ë¶„ì„ ì™„ë£Œ!")
            if progress_callback:
                progress_callback("ğŸ‰ ëª¨ë“  ë¸”ë¡ ë¶„ì„ ì™„ë£Œ!")
            
            return {
                "success": True,
                "analysis_results": analysis_results,
                "cot_history": cumulative_context["cot_history"],
                "model": self._get_current_model_info(" (DSPy + Block CoT)"),
                "method": "Block Chain of Thought Analysis"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "Block Chain of Thought Analysis"
            }
    
    def _build_cot_context(self, cumulative_context, block_info, current_step, feedback_notes: Optional[str] = None, feedback_type: Optional[str] = None):
        """í˜„ì¬ ë¸”ë¡ì„ ìœ„í•œ CoT ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±

        Args:
            cumulative_context: ëˆ„ì  ì»¨í…ìŠ¤íŠ¸
            block_info: ë¸”ë¡ ì •ë³´
            current_step: í˜„ì¬ ë‹¨ê³„
            feedback_notes: í”¼ë“œë°± í…ìŠ¤íŠ¸
            feedback_type: í”¼ë“œë°± ìœ í˜• (perspective_shift, constraint_addition ë“±)
        """

        # ì´ì „ ë¸”ë¡ë“¤ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        previous_insights_summary = ""
        if cumulative_context["previous_results"]:
            previous_insights_summary = "\n### ğŸ”— ì´ì „ ë¸”ë¡ë“¤ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n"

            for i, history_item in enumerate(cumulative_context["cot_history"]):
                previous_insights_summary += f"""
**{i+1}ë‹¨ê³„ - {history_item['block_name']}:**
{history_item['key_insights'][:300]}...

"""

        project_info = cumulative_context.get('project_info', {})

        summary_section = ""
        if isinstance(project_info, dict):
            preprocessed_summary = project_info.get('preprocessed_summary')
            preprocessing_meta = project_info.get('preprocessing_meta', {})
            if preprocessed_summary:
                summary_section += "\n### ğŸ§¾ ì •ì œëœ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸\n"
                summary_section += preprocessed_summary.strip() + "\n"
            if isinstance(preprocessing_meta, dict) and preprocessing_meta:
                stats_parts = []
                original_chars = preprocessing_meta.get('original_chars')
                processed_chars = preprocessing_meta.get('processed_chars')
                if original_chars is not None and processed_chars is not None:
                    stats_parts.append(f"{original_chars}ì â†’ {processed_chars}ì")
                keyword_total = preprocessing_meta.get('keyword_total')
                if keyword_total:
                    stats_parts.append(f"í•µì‹¬ í‚¤ì›Œë“œ {keyword_total}ê°œ")
                if stats_parts:
                    summary_section += "\n**ì „ì²˜ë¦¬ í†µê³„:** " + ", ".join(stats_parts) + "\n"

        # í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        if isinstance(project_info, dict):
            project_info_text = f"""
- í”„ë¡œì íŠ¸ëª…: {project_info.get('project_name', 'N/A')}
- ìœ„ì¹˜: {project_info.get('location', 'N/A')}
- í”„ë¡œì íŠ¸ ëª©í‘œ: {project_info.get('project_goals', 'N/A')[:200]}
- ì¶”ê°€ ì •ë³´: {project_info.get('additional_info', 'N/A')[:200]}
"""
        else:
            project_info_text = str(project_info)

        # ì‚¬ìš©ì í”¼ë“œë°± ì„¹ì…˜ êµ¬ì„± (í”¼ë“œë°± ê³ ë„í™” ì ìš©)
        feedback_section = ""
        if feedback_notes:
            # í”¼ë“œë°± ì˜ë„ ë¶„ì„
            feedback_intent = parse_feedback_intent(feedback_notes, feedback_type)

            # ì´ì „ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            block_id = block_info.get('id', '')
            previous_result = cumulative_context.get('previous_results', {}).get(block_id, '')

            # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”¼ë“œë°± í”„ë¡¬í”„íŠ¸ ìƒì„±
            feedback_section = build_contextual_feedback_prompt(
                feedback_intent,
                previous_result,
                block_info
            )

        # í˜„ì¬ ë¸”ë¡ì„ ìœ„í•œ íŠ¹ë³„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        cot_context = f"""
## ğŸ”— ë¸”ë¡ ê°„ Chain of Thought ë¶„ì„ ì»¨í…ìŠ¤íŠ¸

### ğŸ“Š ë¶„ì„ ì§„í–‰ ìƒí™©
- í˜„ì¬ ë‹¨ê³„: {current_step}/{cumulative_context['total_blocks']}
- ì™„ë£Œëœ ë¸”ë¡: {len(cumulative_context['previous_results'])}ê°œ
- ë‚¨ì€ ë¸”ë¡: {cumulative_context['total_blocks'] - current_step + 1}ê°œ

{previous_insights_summary}
{summary_section}

### ğŸ¯ í˜„ì¬ ë¸”ë¡ ì •ë³´
- ë¸”ë¡ëª…: {block_info.get('name', 'Unknown')}
- ë¸”ë¡ ì„¤ëª…: {block_info.get('description', 'N/A')}

### ğŸ“„ ì›ë³¸ í”„ë¡œì íŠ¸ ì •ë³´
{project_info_text}

### ğŸ“„ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©
{cumulative_context['pdf_text'][:3000] if cumulative_context['pdf_text'] else 'PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.'}

{feedback_section}

## ğŸ”— ë¸”ë¡ ê°„ ì—°ê²°ì„± ì§€ì‹œì‚¬í•­

**ì¤‘ìš”**: ì´ì „ ë¸”ë¡ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ë¸”ë¡ì„ ë¶„ì„í•˜ì„¸ìš”:

1. **ì´ì „ ê²°ê³¼ í™œìš©**: ìœ„ì˜ ì´ì „ ë¸”ë¡ ì¸ì‚¬ì´íŠ¸ë“¤ì„ í˜„ì¬ ë¶„ì„ì˜ ê·¼ê±°ë¡œ í™œìš©
2. **ì—°ê´€ì„± ëª…ì‹œ**: ì´ì „ ê²°ê³¼ì™€ í˜„ì¬ ë¶„ì„ ê²°ê³¼ ê°„ì˜ ì—°ê²°ì ì„ ëª…í™•íˆ ì œì‹œ
3. **ëˆ„ì  ì¸ì‚¬ì´íŠ¸**: ì´ì „ ë¸”ë¡ë“¤ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ì„ í˜„ì¬ ë¶„ì„ì— ë°˜ì˜
4. **ì¼ê´€ì„± ìœ ì§€**: ì „ì²´ ë¶„ì„ ë°©í–¥ì„±ì˜ ì¼ê´€ì„±ì„ ìœ ì§€
5. **ìƒí˜¸ ë³´ì™„**: ì´ì „ ë¸”ë¡ ê²°ê³¼ë¥¼ ë³´ì™„í•˜ê³  ë°œì „ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ë¶„ì„

### ğŸ“‹ í˜„ì¬ ë¸”ë¡ ë¶„ì„ í”„ë¡¬í”„íŠ¸
"""
        
        return cot_context
    
    def _format_prompt_template(self, block_info, cot_context, pdf_text: str = ""):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜"""
        try:
            # ì‹¤ì œ PDF í…ìŠ¤íŠ¸ ì‚¬ìš© (cot_contextì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ì§ì ‘ ì „ë‹¬ë°›ì€ ê°’)
            if not pdf_text:
                # cot_contextì—ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                if "### ğŸ“„ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©" in cot_context:
                    pdf_start = cot_context.find("### ğŸ“„ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©") + len("### ğŸ“„ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©")
                    pdf_end = cot_context.find("\n\n", pdf_start)
                    if pdf_end > pdf_start:
                        pdf_text = cot_context[pdf_start:pdf_end].strip()
            
            # PDF í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©
            if not pdf_text:
                pdf_text = ""
            
            # ë¸”ë¡ì˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‹¤ì œ PDF í…ìŠ¤íŠ¸ í¬í•¨)
            formatted_prompt = process_prompt(block_info, pdf_text)
            
            # ë””ë²„ê¹…: ë¸”ë¡ ë‚´ìš©ì´ ì œëŒ€ë¡œ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            print(f"ğŸ” ë¸”ë¡ í”„ë¡¬í”„íŠ¸ ìƒì„± í™•ì¸:")
            print(f"  - ë¸”ë¡ ID: {block_info.get('id', 'unknown')}")
            print(f"  - ë¸”ë¡ëª…: {block_info.get('name', 'unknown')}")
            if 'role' in block_info:
                print(f"  - ì—­í• (Role): {block_info.get('role', '')[:50]}...")
            if 'instructions' in block_info:
                print(f"  - ì§€ì‹œ(Instructions): {block_info.get('instructions', '')[:50]}...")
            if 'steps' in block_info:
                print(f"  - ë‹¨ê³„ ìˆ˜: {len(block_info.get('steps', []))}ê°œ")
            print(f"  - ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(formatted_prompt)}ì")
            
            return formatted_prompt
        except Exception as e:
            print(f"[X] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            return UNIFIED_PROMPT_TEMPLATE.replace("{pdf_text}", pdf_text if pdf_text else "")
    
    def _analyze_block_with_cot_context(self, cot_context, block_info, block_id, project_info=None, thinking_budget: Optional[int] = None, temperature: Optional[float] = None, enable_streaming: bool = False, progress_callback=None, use_pdf_direct: bool = True):
        """CoT ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë¸”ë¡ ë¶„ì„"""
        try:
            # PDF ì§ì ‘ ì „ë‹¬ ì‹œë„ (ì˜µì…˜ì´ í™œì„±í™”ë˜ê³  PDFê°€ ìˆëŠ” ê²½ìš°)
            if use_pdf_direct:
                pdf_bytes, pdf_path, file_size = self._extract_pdf_data(project_info)
                if pdf_bytes is not None:
                    print(f"ğŸ“„ PDF ì§ì ‘ ì „ë‹¬ ëª¨ë“œ ì‚¬ìš©: {file_size} bytes")
                    # PDF ì§ì ‘ ì „ë‹¬ ë°©ì‹ ì‚¬ìš©
                    return self._analyze_block_with_pdf_direct_wrapper(
                        cot_context, block_info, block_id, project_info,
                        pdf_bytes, pdf_path, thinking_budget, temperature,
                        enable_streaming, progress_callback
                    )
            
            # ê¸°ì¡´ ë°©ì‹: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_text = ""
            if isinstance(project_info, dict):
                pdf_text = project_info.get('file_text', '') or project_info.get('pdf_text', '')
            
            # ìµœì í™”ëœ thinking_budget ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if thinking_budget is None:
                current_provider = get_current_provider()
                provider_config = PROVIDER_CONFIG.get(current_provider, {})
                model_name = provider_config.get('model', '')
                thinking_budget = self._get_optimal_thinking_budget(block_id, block_info, model_name)
                if thinking_budget:
                    print(f"ğŸ§  ë¸”ë¡ë³„ ìµœì í™”ëœ Thinking Budget: {thinking_budget} (ë¸”ë¡: {block_id})")
            
            # ìµœì í™”ëœ temperature ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if temperature is None:
                temperature = self._get_optimal_temperature(block_id, block_info)
                print(f"ğŸŒ¡ï¸ ë¸”ë¡ë³„ ìµœì í™”ëœ Temperature: {temperature:.2f} (ë¸”ë¡: {block_id})")
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜ (ì‹¤ì œ PDF í…ìŠ¤íŠ¸ ì „ë‹¬)
            formatted_prompt = self._format_prompt_template(block_info, cot_context, pdf_text)
            
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (íŠ¹ì • ë¸”ë¡ì— ëŒ€í•´ì„œë§Œ)
            web_search_context = ""
            if block_id and project_info:
                try:
                    web_search_context = get_web_search_context(block_id, project_info, "")
                    if web_search_context:
                        print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ (CoT): {block_id}")
                except Exception as e:
                    print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            
            # ë¬¸ì„œ ê¸°ë°˜ ì¶”ë¡  ê°•ì¡° ì§€ì‹œì‚¬í•­ ì¶”ê°€
            document_based_instruction = f"""

## ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ í•„ìˆ˜ ì§€ì‹œì‚¬í•­

**âš ï¸ ë§¤ìš° ì¤‘ìš”**: ì•„ë˜ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

### 1. ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì¶”ë¡  í•„ìˆ˜
- **ìœ„ì— ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•íˆ ì½ê³  ì´í•´í•œ í›„ ë¶„ì„í•˜ì„¸ìš”**
- **ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„ì— í™œìš©í•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ í…œí”Œë¦¿ì´ë‚˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì´ ì•„ë‹Œ, ì´ íŠ¹ì • í”„ë¡œì íŠ¸ ë¬¸ì„œì˜ ì‹¤ì œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”**

### 2. ë¬¸ì„œ ì¸ìš© ë° ê·¼ê±° ì œì‹œ í•„ìˆ˜
- **ë¶„ì„ ê²°ê³¼ì˜ ëª¨ë“  ì£¼ìš” ì£¼ì¥ì€ ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë’·ë°›ì¹¨í•˜ì„¸ìš”**
- **ì˜ˆì‹œ**: "ë¬¸ì„œì— 'ëŒ€ì§€ë©´ì  5,000ã¡'ë¼ê³  ëª…ì‹œë˜ì–´ ìˆì–´..." í˜•ì‹ìœ¼ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
- **ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ì„ ì œì‹œí•  ë•ŒëŠ” ë°˜ë“œì‹œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”**

### 3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ìƒì„±í•˜ì§€ ë§ ê²ƒ
- **ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”**
- **ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ë˜ëŠ” 'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¡œ í‘œì‹œí•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ ê±´ì¶• í”„ë¡œì íŠ¸ì˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”**

### 4. ë¬¸ì„œ ë‚´ìš©ì˜ êµ¬ì²´ì  í™œìš©
- **ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ëª…ì¹­, ìœ„ì¹˜, ê·œëª¨ ë“±ì„ ë¶„ì„ì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”**
- **ë¬¸ì„œì˜ ë§¥ë½ê³¼ ë°°ê²½ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”**
- **ë¬¸ì„œì˜ ì•”ì‹œì  ì˜ë¯¸ë‚˜ ì—°ê´€ëœ ìš”êµ¬ì‚¬í•­ì„ ì¶”ë¡ í•˜ì—¬ ë¶„ì„ì„ í’ë¶€í•˜ê²Œ ë§Œë“¤ë˜, ì¶”ë¡ ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”**

**ìœ„ ì§€ì‹œì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶„ì„ì´ ë°˜ë³µë˜ê±°ë‚˜ ì¼ë°˜ë¡ ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.**
"""
            
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            if web_search_context:
                formatted_prompt = f"""{formatted_prompt}

{web_search_context}

**ì¤‘ìš”**: ìœ„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì‹  ì •ë³´ì™€ ì‹œì¥ ë™í–¥ì„ ë°˜ì˜í•œ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ë‹¨, ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ë³´ì™„í•˜ëŠ” ì—­í• ì´ë©°, ë¶„ì„ì˜ ì£¼ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–»ì€ ì •ë³´ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³ , ë¬¸ì„œ ë‚´ìš©ê³¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.

{document_based_instruction}
"""
            else:
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ ê°•ì¡°
                formatted_prompt = f"""{formatted_prompt}{document_based_instruction}"""
            
            # í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì¶”ê°€ (ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ ì ìš©)
            # ë¸”ë¡ í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ Chain of Thought ì§€ì‹œì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ë¸”ë¡ ëª©ë¡
            # (ì´ ë¸”ë¡ë“¤ì€ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ë ˆë²¨ ì§€ì‹œì‚¬í•­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
            blocks_with_builtin_cot = ['phase1_facility_program']
            
            # ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ì ìœ¼ë¡œ í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì ìš© (ì¤‘ë³µ ë°©ì§€ ì œì™¸)
            extended_thinking_note = ""
            if block_id and block_id not in blocks_with_builtin_cot:
                # ì‹œìŠ¤í…œ ë ˆë²¨ í™•ì¥ ì‚¬ê³  í…œí”Œë¦¿ ì‚¬ìš©
                extended_thinking_note = self._get_extended_thinking_template()
            
            # CoT ì»¨í…ìŠ¤íŠ¸ì™€ ë¸”ë¡ í”„ë¡¬í”„íŠ¸ ê²°í•©
            # ì¤‘ìš”: ë¸”ë¡ì˜ í”„ë¡¬í”„íŠ¸(formatted_prompt)ê°€ ì£¼ìš” ë¶„ì„ ë°©í–¥ì„ ê²°ì •í•˜ë¯€ë¡œ ëª…í™•í•˜ê²Œ í¬í•¨
            # ìºì‹œ ë¬´íš¨í™”ë¥¼ ìœ„í•œ ê³ ìœ  ID ìƒì„±
            import uuid
            cache_buster = str(uuid.uuid4())[:8]

            enhanced_prompt = f"""
{cot_context}

## ğŸ¯ ë¸”ë¡ë³„ ë¶„ì„ ì§€ì‹œì‚¬í•­ (í•µì‹¬)

**ì•„ë˜ ë¸”ë¡ì˜ êµ¬ì²´ì ì¸ ì—­í• , ì§€ì‹œì‚¬í•­, ë‹¨ê³„ë¥¼ ì •í™•íˆ ë”°ë¼ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.**
**ì´ ë¸”ë¡ì˜ ë‚´ìš©ì´ ì´ë²ˆ ë¶„ì„ì˜ ì£¼ìš” ë°©í–¥ê³¼ ëª©í‘œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.**

{formatted_prompt}{extended_thinking_note}

{self._get_output_format_template()}

<!-- analysis_id: {cache_buster} -->
"""
            
            # ë¸”ë¡ IDì— ë”°ë¼ ì ì ˆí•œ Signature ì„ íƒ (ë™ì  ìƒì„±)
            signature_map = self._build_signature_map()
            
            signature_class = signature_map.get(block_id, SimpleAnalysisSignature)
            
            # System Instruction ìƒì„±
            system_instruction = self._build_system_instruction(block_info)
            
            # Thinking Budgetê³¼ Temperatureê°€ ì„¤ì •ëœ ê²½ìš° LMì— ì ìš©
            lm_context = self._lm_context_with_system_instruction(system_instruction)
            if thinking_budget is not None or temperature is not None:
                # Thinking budgetê³¼ temperatureë¥¼ LMì— ì ìš©í•˜ê¸° ìœ„í•´ ë³„ë„ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
                lm_context = self._lm_context_with_params(
                    thinking_budget=thinking_budget,
                    temperature=temperature,
                    system_instruction=system_instruction
                )
            
            # Streamingì´ í™œì„±í™”ëœ ê²½ìš° streamify ì‚¬ìš©
            if enable_streaming and progress_callback:
                try:
                    # DSPy streamifyë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
                    stream_predict = dspy.streamify(dspy.Predict(signature_class))
                    
                    with lm_context:
                        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë³€í™˜)
                        accumulated_text = ""
                        final_result = None
                        
                        try:
                            # streamifyëŠ” async generatorë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
                            import asyncio
                            
                            async def collect_stream():
                                nonlocal accumulated_text, final_result
                                async for chunk in stream_predict(input=enhanced_prompt):
                                    if isinstance(chunk, dspy.Prediction):
                                        # ìµœì¢… ê²°ê³¼
                                        final_result = chunk
                                        if progress_callback:
                                            progress_callback(f"âœ… ë¶„ì„ ì™„ë£Œ")
                                        break
                                    elif hasattr(chunk, 'text') and chunk.text:
                                        # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
                                        accumulated_text += chunk.text
                                        if progress_callback and len(accumulated_text) % 100 == 0:  # 100ìë§ˆë‹¤ ì—…ë°ì´íŠ¸
                                            progress_callback(f"ğŸ“ ë¶„ì„ ì¤‘... ({len(accumulated_text)}ì)")
                                    elif isinstance(chunk, str):
                                        accumulated_text += chunk
                                        if progress_callback and len(accumulated_text) % 100 == 0:
                                            progress_callback(f"ğŸ“ ë¶„ì„ ì¤‘... ({len(accumulated_text)}ì)")
                                
                                # ìµœì¢… ê²°ê³¼ê°€ ì—†ìœ¼ë©´ accumulated_text ì‚¬ìš©
                                if final_result is None:
                                    class StreamResult:
                                        def __init__(self, output):
                                            self.output = output
                                    final_result = StreamResult(accumulated_text)
                            
                            # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
                            # Streamlit í™˜ê²½ì—ì„œëŠ” ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
                            # ë¨¼ì € í™•ì¸í•˜ê³ , ì‹¤í–‰ ì¤‘ì´ë©´ ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
                            try:
                                # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                                asyncio.get_running_loop()
                                # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
                                raise RuntimeError("Event loop already running")
                            except RuntimeError:
                                # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
                                loop = None
                                try:
                                    try:
                                        loop = asyncio.get_event_loop()
                                        if loop.is_running():
                                            raise RuntimeError("Event loop already running")
                                    except RuntimeError:
                                        # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ê±°ë‚˜ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ìƒˆë¡œ ìƒì„±
                                        loop = asyncio.new_event_loop()
                                        asyncio.set_event_loop(loop)
                                    
                                    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
                                    if loop and not loop.is_running():
                                        loop.run_until_complete(collect_stream())
                                    else:
                                        raise RuntimeError("Event loop already running")
                                finally:
                                    # ì´ë²¤íŠ¸ ë£¨í”„ ì •ë¦¬ (ìƒˆë¡œ ë§Œë“  ê²½ìš°ì—ë§Œ)
                                    if loop:
                                        try:
                                            if not loop.is_running():
                                                # ë³´ë¥˜ ì¤‘ì¸ íƒœìŠ¤í¬ ì •ë¦¬
                                                try:
                                                    pending = [t for t in asyncio.all_tasks(loop) if not t.done()]
                                                    if pending:
                                                        for task in pending:
                                                            if not task.done():
                                                                task.cancel()
                                                        # ì·¨ì†Œëœ íƒœìŠ¤í¬ë“¤ì„ ê¸°ë‹¤ë¦¼ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                                                        try:
                                                            loop.run_until_complete(asyncio.wait_for(
                                                                asyncio.gather(*pending, return_exceptions=True),
                                                                timeout=1.0
                                                            ))
                                                        except (asyncio.TimeoutError, Exception):
                                                            pass
                                                except Exception:
                                                    pass
                                                # ë£¨í”„ ë‹«ê¸°
                                                try:
                                                    loop.close()
                                                except Exception:
                                                    pass
                                        except Exception:
                                            pass
                            
                            result = final_result
                        except (RuntimeError, AttributeError) as stream_error:
                            # ìŠ¤íŠ¸ë¦¬ë°ì´ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ (ì˜ˆ: Streamlitì˜ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„)
                            print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° í™˜ê²½ ì œí•œ, ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜: {stream_error}")
                            if progress_callback:
                                progress_callback("ğŸ“Š ë¶„ì„ ì‹œì‘...")
                            # ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
                            result = dspy.Predict(signature_class)(input=enhanced_prompt)
                            if progress_callback:
                                progress_callback("âœ… ë¶„ì„ ì™„ë£Œ")
                except Exception as stream_error:
                    print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜, ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜: {stream_error}")
                    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
                    with lm_context:
                        result = dspy.Predict(signature_class)(input=enhanced_prompt)
            else:
                # ì¼ë°˜ ëª¨ë“œ (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
                with lm_context:
                    result = dspy.Predict(signature_class)(input=enhanced_prompt)
            
            return {
                "success": True,
                "analysis": result.output,
                "model": self._get_current_model_info(" (DSPy + CoT)"),
                "method": f"DSPy + {signature_class.__name__} + Block CoT",
                "block_id": block_id
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": f"DSPy + Block CoT",
                "block_id": block_id
            }
    
    def _get_file_search_client(self) -> Tuple[Optional[Any], Optional[Dict[str, Any]]]:
        """
        File Searchìš© Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            (client, error_dict) íŠœí”Œ
            - client: ì„±ê³µ ì‹œ genai.Client ê°ì²´, ì‹¤íŒ¨ ì‹œ None
            - error_dict: ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë”•ì…”ë„ˆë¦¬, ì„±ê³µ ì‹œ None
        """
        try:
            from google import genai
            
            current_provider = get_current_provider()
            api_key = get_api_key(current_provider)
            if not api_key:
                return None, {
                    "success": False,
                    "error": "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
            
            client = genai.Client(api_key=api_key)
            return client, None
        except Exception as e:
            return None, {
                "success": False,
                "error": f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"
            }
    
    def _validate_store_name(self, store_name: str) -> Optional[Dict[str, Any]]:
        """
        Store ì´ë¦„ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            store_name: ê²€ì¦í•  Store ì´ë¦„
            
        Returns:
            ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ ë”•ì…”ë„ˆë¦¬, ìœ íš¨í•˜ë©´ None
        """
        if not store_name or not isinstance(store_name, str):
            return {
                "success": False,
                "error": "Store ì´ë¦„ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }
        
        store_name = store_name.strip()
        if not store_name:
            return {
                "success": False,
                "error": "Store ì´ë¦„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            }
        
        return None
    
    def create_file_search_store(self, display_name: str) -> Dict[str, Any]:
        """
        File Search Store ìƒì„±
        
        Args:
            display_name: Store í‘œì‹œ ì´ë¦„
        
        Returns:
            Store ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not display_name or not display_name.strip():
            return {
                "success": False,
                "error": "Store í‘œì‹œ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            }
        
        client, error = self._get_file_search_client()
        if error:
            return error
        
        try:
            store = client.file_search_stores.create(
                config={'display_name': display_name.strip()}
            )
            
            return {
                "success": True,
                "store_name": store.name,
                "display_name": store.display_name,
                "create_time": store.create_time
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"File Search Store ìƒì„± ì˜¤ë¥˜: {str(e)}"
            }
    
    def list_file_search_stores(self) -> Dict[str, Any]:
        """
        File Search Store ëª©ë¡ ì¡°íšŒ
        
        Returns:
            Store ëª©ë¡ ë”•ì…”ë„ˆë¦¬
        """
        client, error = self._get_file_search_client()
        if error:
            return error
        
        try:
            stores = list(client.file_search_stores.list())
            
            return {
                "success": True,
                "stores": [
                    {
                        "name": store.name,
                        "display_name": store.display_name,
                        "create_time": store.create_time
                    }
                    for store in stores
                ]
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"File Search Store ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
            }
    
    def upload_to_file_search_store(
        self,
        file_path: Union[str, bytes],
        store_name: str,
        display_name: Optional[str] = None,
        chunking_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        íŒŒì¼ì„ File Search Storeì— ì—…ë¡œë“œ ë° ì¸ë±ì‹±
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë°”ì´íŠ¸ ë°ì´í„°
            store_name: File Search Store ì´ë¦„
            display_name: íŒŒì¼ í‘œì‹œ ì´ë¦„
            chunking_config: Chunking ì„¤ì • (ì„ íƒì‚¬í•­)
        
        Returns:
            ì—…ë¡œë“œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import os
        import tempfile
        import time
        
        # Store ì´ë¦„ ê²€ì¦
        validation_error = self._validate_store_name(store_name)
        if validation_error:
            return validation_error
        
        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client, error = self._get_file_search_client()
        if error:
            return error
        
        tmp_path = None
        
        try:
            
            # Config êµ¬ì„±
            config = {}
            if display_name:
                config['display_name'] = display_name
            if chunking_config:
                config['chunking_config'] = chunking_config
            
            # íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
            if isinstance(file_path, bytes):
                # ë°”ì´íŠ¸ ë°ì´í„°ì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                # ì—…ë¡œë“œ ì™„ë£Œê¹Œì§€ íŒŒì¼ì„ ìœ ì§€í•´ì•¼ í•¨
                file_ext = '.pdf'  # ê¸°ë³¸ í™•ì¥ì
                if display_name:
                    ext = os.path.splitext(display_name)[1]
                    if ext:
                        file_ext = ext
                
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_ext)
                tmp_file.write(file_path)
                tmp_path = tmp_file.name
                tmp_file.close()
                
                file_to_upload = tmp_path
            else:
                file_to_upload = file_path
            
            # íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° í™•ì¸
            if not os.path.exists(file_to_upload):
                return {
                    "success": False,
                    "error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_to_upload}"
                }
            
            file_size = os.path.getsize(file_to_upload)
            if file_size == 0:
                return {
                    "success": False,
                    "error": "ë¹ˆ íŒŒì¼ì€ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ì—…ë¡œë“œ ì‹œì‘
            try:
                operation = client.file_search_stores.upload_to_file_search_store(
                    file=file_to_upload,
                    file_search_store_name=store_name,
                    config=config if config else None
                )
            except Exception as upload_error:
                error_msg = str(upload_error)
                
                # "terminated" ë˜ëŠ” "already" ì˜¤ë¥˜ ì²˜ë¦¬
                if 'terminated' in error_msg.lower() or 'already' in error_msg.lower():
                    return {
                        "success": False,
                        "error": f"ì—…ë¡œë“œê°€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆê±°ë‚˜ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                                f"**ê°€ëŠ¥í•œ ì›ì¸:**\n"
                                f"1. ë™ì¼í•œ íŒŒì¼ëª…ì˜ íŒŒì¼ì´ ì´ë¯¸ Storeì— ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤\n"
                                f"2. ì´ì „ ì—…ë¡œë“œê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤\n"
                                f"3. ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¡œ ì—…ë¡œë“œê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤\n\n"
                                f"**í•´ê²° ë°©ë²•:**\n"
                                f"- íŒŒì¼ëª…ì„ ë³€ê²½í•˜ì—¬ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”\n"
                                f"- ë‹¤ë¥¸ Storeë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”\n"
                                f"- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”\n\n"
                                f"ìƒì„¸ ì˜¤ë¥˜: {error_msg}"
                    }
                return {
                    "success": False,
                    "error": f"ì—…ë¡œë“œ ì‹œì‘ ì‹¤íŒ¨: {error_msg}"
                }
            
            # Operation ì™„ë£Œ ëŒ€ê¸°
            max_wait_time = 600  # ìµœëŒ€ 10ë¶„
            start_time = time.time()
            check_interval = 2  # 2ì´ˆë§ˆë‹¤ í™•ì¸
            
            while not operation.done:
                if time.time() - start_time > max_wait_time:
                    return {
                        "success": False,
                        "error": "íŒŒì¼ ì¸ë±ì‹± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
                    }
                
                time.sleep(check_interval)
                try:
                    operation = client.operations.get(operation)
                except Exception as op_error:
                    return {
                        "success": False,
                        "error": f"Operation ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(op_error)}"
                    }
            
            # Operation ê²°ê³¼ í™•ì¸
            if hasattr(operation, 'error') and operation.error:
                error_detail = operation.error
                if isinstance(error_detail, dict):
                    error_msg = error_detail.get('message', str(error_detail))
                else:
                    error_msg = str(error_detail)
                
                return {
                    "success": False,
                    "error": f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨: {error_msg}"
                }
            
            return {
                "success": True,
                "operation_name": operation.name,
                "done": operation.done
            }
            
        except Exception as e:
            error_msg = str(e)
            if 'terminated' in error_msg.lower():
                return {
                    "success": False,
                    "error": f"ì—…ë¡œë“œê°€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì—…ë¡œë“œë˜ì—ˆê±°ë‚˜, ì´ì „ ì—…ë¡œë“œê°€ ì¤‘ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ íŒŒì¼ì„ ì—…ë¡œë“œí•´ë³´ì„¸ìš”. ìƒì„¸ ì˜¤ë¥˜: {error_msg}"
                }
            return {
                "success": False,
                "error": f"File Search Store ì—…ë¡œë“œ ì˜¤ë¥˜: {error_msg}"
            }
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì—…ë¡œë“œ ì™„ë£Œ í›„)
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except Exception as cleanup_error:
                    print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {cleanup_error}")
    
    def list_files_in_store(self, store_name: str) -> Dict[str, Any]:
        """
        File Search Store ë‚´ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        
        Args:
            store_name: File Search Store ì´ë¦„
        
        Returns:
            íŒŒì¼ ëª©ë¡ ë”•ì…”ë„ˆë¦¬
        """
        # Store ì´ë¦„ ê²€ì¦
        validation_error = self._validate_store_name(store_name)
        if validation_error:
            return validation_error
        
        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client, error = self._get_file_search_client()
        if error:
            return error
        
        try:
            # Store ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            store = client.file_search_stores.get(name=store_name)
            if not store:
                return {
                    "success": False,
                    "error": f"File Search Storeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {store_name}"
                }
            
            store_display_name = getattr(store, 'display_name', None) or store_name
            
            # Store ê°ì²´ì˜ files ì†ì„± ì‚¬ìš©
            files = []
            if hasattr(store, 'files') and store.files:
                for file in store.files:
                    file_info = {
                        "name": getattr(file, 'name', str(file)),
                        "display_name": getattr(file, 'display_name', None),
                        "create_time": getattr(file, 'create_time', None),
                        "mime_type": getattr(file, 'mime_type', None),
                        "size_bytes": getattr(file, 'size_bytes', None)
                    }
                    files.append(file_info)
            
            return {
                "success": True,
                "store_name": store_name,
                "store_display_name": store_display_name,
                "files": files,
                "file_count": len(files)
            }
        except Exception as e:
            error_msg = str(e)
            # Storeë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ëª…í™•í•œ ë©”ì‹œì§€
            if 'not found' in error_msg.lower() or '404' in error_msg:
                return {
                    "success": False,
                    "error": f"File Search Storeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {store_name}"
                }
            return {
                "success": False,
                "error": f"Store íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {error_msg}"
            }
    
    def delete_file_search_store(self, store_name: str, force: bool = True) -> Dict[str, Any]:
        """
        File Search Store ì‚­ì œ
        
        Args:
            store_name: Store ì´ë¦„
            force: ê°•ì œ ì‚­ì œ ì—¬ë¶€
        
        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        validation_error = self._validate_store_name(store_name)
        if validation_error:
            return validation_error
        
        client, error = self._get_file_search_client()
        if error:
            return error
        
        try:
            client.file_search_stores.delete(
                name=store_name,
                config={'force': force} if force else None
            )
            
            return {
                "success": True,
                "message": f"File Search Store ì‚­ì œ ì™„ë£Œ: {store_name}"
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"File Search Store ì‚­ì œ ì˜¤ë¥˜: {str(e)}"
            }
    
    def _extract_location_coordinates(self, project_info: Optional[Dict[str, Any]]) -> Optional[Dict[str, float]]:
        """
        project_infoì—ì„œ ìœ„ì¹˜ ì¢Œí‘œ ì¶”ì¶œ
        
        ìš°ì„ ìˆœìœ„:
        1. ì§ì ‘ ì…ë ¥ëœ ì¢Œí‘œ (latitude, longitude)
        2. geo_layersì˜ ì¤‘ì‹¬ì 
        3. location í…ìŠ¤íŠ¸ë¥¼ Geocoding (ì„ íƒì‚¬í•­)
        
        Returns:
            {'latitude': float, 'longitude': float} ë˜ëŠ” None
        """
        # 1. ì§ì ‘ ì¢Œí‘œ í™•ì¸
        if isinstance(project_info, dict):
            if 'latitude' in project_info and 'longitude' in project_info:
                try:
                    return {
                        'latitude': float(project_info['latitude']),
                        'longitude': float(project_info['longitude'])
                    }
                except (ValueError, TypeError):
                    pass
        
        # 2. geo_layers ì¤‘ì‹¬ì  ì‚¬ìš©
        try:
            import streamlit as st
            if st.session_state.get('geo_layers'):
                # ëª¨ë“  ë ˆì´ì–´ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
                all_coords = []
                for layer_name, layer_data in st.session_state.geo_layers.items():
                    gdf = layer_data.get('gdf')
                    if gdf is not None and len(gdf) > 0:
                        try:
                            centroid = gdf.geometry.centroid.iloc[0]
                            all_coords.append({
                                'lat': centroid.y,
                                'lon': centroid.x
                            })
                        except Exception as e:
                            print(f"âš ï¸ ë ˆì´ì–´ {layer_name} ì¤‘ì‹¬ì  ê³„ì‚° ì˜¤ë¥˜: {e}")
                
                if all_coords:
                    # í‰ê·  ì¢Œí‘œ ë°˜í™˜
                    avg_lat = sum(c['lat'] for c in all_coords) / len(all_coords)
                    avg_lon = sum(c['lon'] for c in all_coords) / len(all_coords)
                    return {
                        'latitude': avg_lat,
                        'longitude': avg_lon
                    }
        except Exception as e:
            print(f"âš ï¸ geo_layers ì¢Œí‘œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        # 3. location í…ìŠ¤íŠ¸ Geocoding (ì„ íƒì‚¬í•­, êµ¬í˜„ ìƒëµ)
        # í•„ìš”ì‹œ Google Geocoding API ì‚¬ìš©
        
        return None
    
    def _extract_pdf_data(self, project_info: Optional[Dict[str, Any]]) -> Tuple[Optional[bytes], Optional[str], Optional[int]]:
        """
        project_infoë‚˜ session_stateì—ì„œ PDF ë°”ì´íŠ¸ ë°ì´í„°, ê²½ë¡œ, íŒŒì¼ í¬ê¸° ì¶”ì¶œ
        
        Returns:
            (pdf_bytes, pdf_path, file_size) íŠœí”Œ
        """
        pdf_bytes = None
        pdf_path = None
        file_size = None
        
        try:
            # 1. project_infoì—ì„œ ì§ì ‘ PDF ë°”ì´íŠ¸ ë°ì´í„° í™•ì¸
            if isinstance(project_info, dict):
                pdf_bytes = project_info.get('pdf_bytes')
                pdf_path = project_info.get('pdf_path') or project_info.get('file_path')
            
            # 2. Streamlit session_stateì—ì„œ ì—…ë¡œë“œëœ íŒŒì¼ í™•ì¸
            if pdf_bytes is None:
                try:
                    import streamlit as st
                    uploaded_file = st.session_state.get('uploaded_file')
                    if uploaded_file is not None:
                        # íŒŒì¼ì´ ì—…ë¡œë“œë˜ì–´ ìˆê³  PDFì¸ ê²½ìš°
                        if hasattr(uploaded_file, 'getvalue'):
                            file_bytes = uploaded_file.getvalue()
                            # PDF ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (%PDF)
                            if file_bytes[:4] == b'%PDF':
                                pdf_bytes = file_bytes
                                file_size = len(file_bytes)
                                print(f"ğŸ“„ Session stateì—ì„œ PDF ë°”ì´íŠ¸ ë°ì´í„° ì¶”ì¶œ: {len(pdf_bytes)} bytes")
                except Exception:
                    pass
            
            # 3. íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ íŒŒì¼ì—ì„œ ì½ê¸°
            if pdf_bytes is None and pdf_path:
                from pathlib import Path
                pdf_path_obj = Path(pdf_path)
                if pdf_path_obj.exists():
                    with open(pdf_path_obj, 'rb') as f:
                        pdf_bytes = f.read()
                    file_size = len(pdf_bytes)
                    print(f"ğŸ“„ íŒŒì¼ ê²½ë¡œì—ì„œ PDF ì½ê¸°: {pdf_path} ({file_size} bytes)")
            
            return pdf_bytes, pdf_path, file_size
            
        except Exception as e:
            print(f"âš ï¸ PDF ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None, None, None
    
    def _analyze_block_with_pdf_direct(
        self,
        enhanced_prompt: str,
        pdf_bytes: bytes,
        pdf_path: Optional[str],
        block_info: Dict[str, Any],
        block_id: str,
        system_instruction: str,
        thinking_budget: Optional[int],
        temperature: Optional[float],
        enable_streaming: bool = False,
        progress_callback=None,
        thinking_level: Optional[str] = None,
        include_thoughts: bool = False,
        function_declarations: Optional[List[Union[Dict[str, Any], Callable]]] = None,
        function_implementations: Optional[Dict[str, Callable]] = None,
        automatic_function_calling: bool = False,
        max_function_iterations: int = 10,
        file_search_store_names: Optional[List[str]] = None,
        reference_urls: Optional[List[str]] = None,
        use_google_search: bool = False,
        use_google_maps: bool = False,
        enable_maps_widget: bool = False,
        location_coordinates: Optional[Dict[str, float]] = None,
        web_search_citations: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        PDFë¥¼ ì§ì ‘ Gemini APIì— ì „ë‹¬í•˜ì—¬ ë¶„ì„
        
        Args:
            enhanced_prompt: ëª¨ë“  í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ê°€ ê²°í•©ëœ í”„ë¡¬í”„íŠ¸
            pdf_bytes: PDF ë°”ì´íŠ¸ ë°ì´í„°
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ (Files API ì‚¬ìš© ì‹œ)
            block_info: ë¸”ë¡ ì •ë³´
            block_id: ë¸”ë¡ ID
            system_instruction: System Instruction
            thinking_budget: Thinking Budget
            temperature: Temperature
            enable_streaming: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            from google import genai
            from google.genai import types
            import io
            import time
            
            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            current_provider = get_current_provider()
            api_key = get_api_key(current_provider)
            if not api_key:
                return {
                    "success": False,
                    "error": "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
            
            # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            provider_config = PROVIDER_CONFIG.get(current_provider, {})
            model_name = provider_config.get('model', 'gemini-2.5-flash')
            clean_model = model_name.replace('models/', '').replace('model/', '')
            
            client = genai.Client(api_key=api_key)
            
            # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ (10MB ê¸°ì¤€)
            FILE_SIZE_THRESHOLD = 10 * 1024 * 1024  # 10MB
            file_size = len(pdf_bytes)
            use_files_api = file_size >= FILE_SIZE_THRESHOLD
            
            # PDF Part ì¤€ë¹„
            pdf_part = None
            if use_files_api:
                # Files API ì‚¬ìš© (ëŒ€ìš©ëŸ‰ íŒŒì¼)
                if pdf_path:
                    uploaded_file = client.files.upload(
                        file=pdf_path,
                        config=dict(mime_type='application/pdf')
                    )
                else:
                    # ë°”ì´íŠ¸ ë°ì´í„°ëŠ” BytesIOë¡œ ì—…ë¡œë“œ
                    pdf_io = io.BytesIO(pdf_bytes)
                    uploaded_file = client.files.upload(
                        file=pdf_io,
                        config=dict(mime_type='application/pdf')
                    )
                
                # íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°
                max_wait_time = 300  # ìµœëŒ€ 5ë¶„ ëŒ€ê¸°
                start_time = time.time()
                
                while uploaded_file.state == 'PROCESSING':
                    if time.time() - start_time > max_wait_time:
                        return {
                            "success": False,
                            "error": "íŒŒì¼ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
                        }
                    
                    uploaded_file = client.files.get(name=uploaded_file.name)
                    if progress_callback:
                        progress_callback(f"ğŸ“¤ PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘... ({uploaded_file.state})")
                    time.sleep(2)
                
                if uploaded_file.state == 'FAILED':
                    return {
                        "success": False,
                        "error": "íŒŒì¼ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    }
                
                # Files APIë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì€ URIë¡œ ì°¸ì¡°
                pdf_part = uploaded_file
                print(f"ğŸ“„ Files API ì‚¬ìš©: {uploaded_file.uri}")
            else:
                # ì¸ë¼ì¸ ì²˜ë¦¬ (ì‘ì€ íŒŒì¼)
                pdf_part = types.Part.from_bytes(
                    data=pdf_bytes,
                    mime_type='application/pdf',
                )
                print(f"ğŸ“„ ì¸ë¼ì¸ PDF ì²˜ë¦¬: {file_size} bytes")
            
            # Contents êµ¬ì„±: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ â†’ PDF Part
            # URL Contextë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° URLì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            prompt_with_urls = enhanced_prompt
            if reference_urls and len(reference_urls) > 0:
                urls_text = "\n\n## ì°¸ê³  URL:\n" + "\n".join([f"- {url}" for url in reference_urls])
                prompt_with_urls = enhanced_prompt + urls_text
            
            contents = [
                prompt_with_urls,
                pdf_part
            ]
            
            # Tools êµ¬ì„±
            tools = []
            
            # File Search tool ì¶”ê°€
            if file_search_store_names:
                try:
                    tools.append(types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=file_search_store_names
                        )
                    ))
                    print(f"ğŸ“š File Search í™œì„±í™”: {len(file_search_store_names)}ê°œ Store")
                except Exception as e:
                    print(f"âš ï¸ File Search tool ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            # URL Context tool ì¶”ê°€
            if reference_urls and len(reference_urls) > 0:
                try:
                    # URLì€ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ (toolì€ ìë™ìœ¼ë¡œ URL ê°ì§€)
                    tools.append(types.Tool(url_context={}))
                    print(f"ğŸ”— URL Context í™œì„±í™”: {len(reference_urls)}ê°œ URL")
                except Exception as e:
                    print(f"âš ï¸ URL Context tool ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            # Google Search tool ì¶”ê°€
            if use_google_search:
                try:
                    tools.append(types.Tool(google_search={}))
                    print(f"ğŸŒ Google Search í™œì„±í™”")
                except Exception as e:
                    print(f"âš ï¸ Google Search tool ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            # Google Maps tool ì¶”ê°€
            if use_google_maps:
                try:
                    tools.append(types.Tool(google_maps=types.GoogleMaps(enable_widget=enable_maps_widget)))
                    print(f"ğŸ—ºï¸ Google Maps í™œì„±í™” (Widget: {enable_maps_widget})")
                except Exception as e:
                    print(f"âš ï¸ Google Maps tool ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            # Function declarationsë¥¼ types.Tool í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if function_declarations:
                try:
                    # Python í•¨ìˆ˜ë¥¼ FunctionDeclarationìœ¼ë¡œ ë³€í™˜
                    converted_declarations = []
                    for func_decl in function_declarations:
                        if isinstance(func_decl, dict):
                            # ë”•ì…”ë„ˆë¦¬ë¥¼ FunctionDeclaration ê°ì²´ë¡œ ë³€í™˜
                            try:
                                func_decl_obj = types.FunctionDeclaration(**func_decl)
                                converted_declarations.append(func_decl_obj)
                            except Exception as e:
                                print(f"âš ï¸ ë”•ì…”ë„ˆë¦¬ë¥¼ FunctionDeclarationìœ¼ë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                                # ë”•ì…”ë„ˆë¦¬ ê·¸ëŒ€ë¡œ ì‚¬ìš© (APIê°€ ì²˜ë¦¬)
                                converted_declarations.append(func_decl)
                        elif callable(func_decl):
                            # Google GenAI SDKì˜ from_callable ì‚¬ìš©
                            try:
                                func_decl_obj = types.FunctionDeclaration.from_callable(
                                    client=client,
                                    callable=func_decl
                                )
                                converted_declarations.append(func_decl_obj)
                            except Exception as e:
                                print(f"âš ï¸ í•¨ìˆ˜ë¥¼ FunctionDeclarationìœ¼ë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                                # ìˆ˜ë™ ë³€í™˜ ì‹œë„
                                converted_dicts = self._convert_function_declarations([func_decl])
                                for decl_dict in converted_dicts:
                                    try:
                                        func_decl_obj = types.FunctionDeclaration(**decl_dict)
                                        converted_declarations.append(func_decl_obj)
                                    except Exception:
                                        converted_declarations.append(decl_dict)
                        else:
                            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” function declaration í˜•ì‹: {type(func_decl)}")
                    
                    if converted_declarations:
                        tools.append(types.Tool(function_declarations=converted_declarations))
                        print(f"ğŸ”§ Function Calling í™œì„±í™”: {len(converted_declarations)}ê°œ í•¨ìˆ˜")
                except Exception as e:
                    print(f"âš ï¸ Function declarations ë³€í™˜ ì˜¤ë¥˜: {e}")
            
            # GenerateContentConfig êµ¬ì„±
            config_dict = {}
            
            # System Instruction ì¶”ê°€
            if system_instruction:
                config_dict['system_instruction'] = system_instruction
            
            # Temperature ì¶”ê°€
            if temperature is not None:
                config_dict['temperature'] = max(0.0, min(1.0, temperature))
            
            # Thinking Config êµ¬ì„±
            is_thinking_model = (
                'gemini-2.5' in clean_model or 
                'gemini-3' in clean_model or
                'gemini-2.0' in clean_model
            )
            
            if is_thinking_model:
                is_gemini_3 = 'gemini-3' in clean_model
                is_gemini_3_pro = 'gemini-3-pro' in clean_model
                is_gemini_3_flash = 'gemini-3-flash' in clean_model
                is_gemini_25_pro = 'gemini-2.5-pro' in clean_model
                is_gemini_25_flash = 'gemini-2.5-flash' in clean_model or 'gemini-2.5-flash-lite' in clean_model
                
                thinking_config = {}
                
                # Gemini 3 ëª¨ë¸: thinking_level ì‚¬ìš© ê¶Œì¥
                if is_gemini_3:
                    if thinking_level:
                        # thinking_level ì§ì ‘ ì œê³µ
                        valid_levels = ['low', 'high']
                        if is_gemini_3_flash:
                            valid_levels.extend(['minimal', 'medium'])
                        
                        if thinking_level.lower() in valid_levels:
                            thinking_config['thinking_level'] = thinking_level.lower()
                            print(f"ğŸ§  Thinking Level: {thinking_level.lower()} (Gemini 3)")
                        else:
                            print(f"âš ï¸ ì˜ëª»ëœ thinking_level: {thinking_level}. ê¸°ë³¸ê°’ ì‚¬ìš©.")
                    elif thinking_budget is not None:
                        # thinking_budgetì„ thinking_levelë¡œ ë³€í™˜ (í˜¸í™˜ì„±)
                        if thinking_budget <= 1024:
                            thinking_config['thinking_level'] = "low"
                        else:
                            thinking_config['thinking_level'] = "high"
                        print(f"ğŸ§  Thinking Budget â†’ Level ë³€í™˜: {thinking_budget} â†’ {thinking_config['thinking_level']}")
                    
                    # include_thoughts ì˜µì…˜
                    if include_thoughts:
                        thinking_config['include_thoughts'] = True
                        print(f"ğŸ’­ Thought summaries í™œì„±í™”")
                
                # Gemini 2.5 ëª¨ë¸: thinking_budget ì‚¬ìš©
                elif is_gemini_25_pro:
                    if thinking_budget is not None:
                        if thinking_budget == -1:
                            # Dynamic thinking
                            thinking_config['thinking_budget'] = -1
                            print(f"ğŸ§  Dynamic Thinking í™œì„±í™” (Gemini 2.5 Pro)")
                        elif thinking_budget > 0:
                            thinking_config['thinking_budget'] = max(128, min(32768, thinking_budget))
                            print(f"ğŸ§  Thinking Budget: {thinking_config['thinking_budget']} (Gemini 2.5 Pro)")
                
                elif is_gemini_25_flash:
                    if thinking_budget is not None:
                        if thinking_budget == -1:
                            # Dynamic thinking
                            thinking_config['thinking_budget'] = -1
                            print(f"ğŸ§  Dynamic Thinking í™œì„±í™” (Gemini 2.5 Flash)")
                        elif thinking_budget == 0:
                            # Thinking ë¹„í™œì„±í™”
                            thinking_config['thinking_budget'] = 0
                            print(f"ğŸ§  Thinking ë¹„í™œì„±í™” (Gemini 2.5 Flash)")
                        else:
                            thinking_config['thinking_budget'] = max(0, min(24576, thinking_budget))
                            print(f"ğŸ§  Thinking Budget: {thinking_config['thinking_budget']} (Gemini 2.5 Flash)")
                    
                    # include_thoughts ì˜µì…˜
                    if include_thoughts:
                        thinking_config['include_thoughts'] = True
                        print(f"ğŸ’­ Thought summaries í™œì„±í™”")
                
                if thinking_config:
                    config_dict['thinking_config'] = types.ThinkingConfig(**thinking_config)
            
            # Tool Config êµ¬ì„± (Google Maps ìœ„ì¹˜ ì •ë³´)
            tool_config_dict = {}
            if use_google_maps and location_coordinates:
                try:
                    tool_config_dict['retrieval_config'] = types.RetrievalConfig(
                        lat_lng=types.LatLng(
                            latitude=location_coordinates['latitude'],
                            longitude=location_coordinates['longitude']
                        )
                    )
                    print(f"ğŸ—ºï¸ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì •: ({location_coordinates['latitude']}, {location_coordinates['longitude']})")
                except Exception as e:
                    print(f"âš ï¸ Tool Config ì„¤ì • ì˜¤ë¥˜: {e}")
            
            if tool_config_dict:
                config_dict['tool_config'] = types.ToolConfig(**tool_config_dict)
            
            # Tools ì¶”ê°€
            if tools:
                config_dict['tools'] = tools
            
            # GenerateContentConfig ìƒì„±
            config = types.GenerateContentConfig(**config_dict) if config_dict else None
            
            # Function Callingì´ ìˆëŠ” ê²½ìš° Compositional calling ì§€ì›
            if tools and function_implementations:
                return self._handle_function_calling_with_pdf(
                    client=client,
                    model=clean_model,
                    contents=contents,
                    config=config,
                    function_implementations=function_implementations,
                    automatic_function_calling=automatic_function_calling,
                    max_iterations=max_function_iterations,
                    enable_streaming=enable_streaming,
                    progress_callback=progress_callback,
                    include_thoughts=include_thoughts,
                    provider_config=provider_config,
                    model_name=model_name,
                    block_id=block_id,
                    use_files_api=use_files_api
                )
            
            # ë¶„ì„ ìš”ì²­
            if progress_callback:
                progress_callback("ğŸ“Š PDF ì§ì ‘ ë¶„ì„ ì‹œì‘...")
            
            thought_summary = ""
            analysis_text = ""
            
            if enable_streaming and progress_callback:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                response_stream = client.models.generate_content_stream(
                    model=clean_model,
                    contents=contents,
                    config=config
                )
                
                accumulated_text = ""
                accumulated_thoughts = ""
                
                for chunk in response_stream:
                    if hasattr(chunk, 'candidates') and chunk.candidates:
                        for part in chunk.candidates[0].content.parts:
                            if not part.text:
                                continue
                            
                            # Thought summaries ì²˜ë¦¬
                            if include_thoughts and hasattr(part, 'thought') and part.thought:
                                accumulated_thoughts += part.text
                                if progress_callback and len(accumulated_thoughts) % 100 == 0:
                                    progress_callback(f"ğŸ’­ ì¶”ë¡  ì¤‘... ({len(accumulated_thoughts)}ì)")
                            else:
                                accumulated_text += part.text
                                if progress_callback and len(accumulated_text) % 100 == 0:
                                    progress_callback(f"ğŸ“ ë¶„ì„ ì¤‘... ({len(accumulated_text)}ì)")
                
                analysis_text = accumulated_text
                thought_summary = accumulated_thoughts
                if progress_callback:
                    progress_callback("âœ… ë¶„ì„ ì™„ë£Œ")
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = client.models.generate_content(
                    model=clean_model,
                    contents=contents,
                    config=config
                )
                
                # Thought summariesì™€ ì¼ë°˜ ì‘ë‹µ ë¶„ë¦¬
                if include_thoughts and hasattr(response, 'candidates') and response.candidates:
                    for part in response.candidates[0].content.parts:
                        if not part.text:
                            continue
                        
                        if hasattr(part, 'thought') and part.thought:
                            thought_summary += part.text + "\n"
                        else:
                            analysis_text += part.text
                else:
                    analysis_text = response.text
            
            result = {
                "success": True,
                "analysis": analysis_text,
                "model": f"{provider_config.get('display_name', model_name)} (PDF Direct)",
                "method": "Gemini API Direct + PDF Native",
                "block_id": block_id,
                "pdf_method": "files_api" if use_files_api else "inline"
            }
            
            # Thought summaries ì¶”ê°€
            if include_thoughts and thought_summary:
                result["thought_summary"] = thought_summary.strip()
            
            # Grounding metadata ì¶”ì¶œ (Google Search)
            grounding_supports = []
            if use_google_search:
                try:
                    if enable_streaming:
                        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ë§ˆì§€ë§‰ chunkì—ì„œ metadata ì¶”ì¶œ
                        # (ì‹¤ì œë¡œëŠ” ì¼ë°˜ ëª¨ë“œì—ì„œë§Œ metadataê°€ ì™„ì „íˆ ì œê³µë¨)
                        pass
                    else:
                        if hasattr(response, 'candidates') and response.candidates:
                            grounding_metadata = response.candidates[0].grounding_metadata
                            if grounding_metadata:
                                # Citations ì¶”ì¶œ
                                citations = []
                                if hasattr(grounding_metadata, 'grounding_chunks'):
                                    for chunk in grounding_metadata.grounding_chunks:
                                        if hasattr(chunk, 'web'):
                                            citations.append({
                                                'uri': chunk.web.uri,
                                                'title': chunk.web.title
                                            })
                                
                                # Grounding supports ì¶”ì¶œ (ì¸ë¼ì¸ ì¸ìš©ìš©)
                                if hasattr(grounding_metadata, 'grounding_supports'):
                                    for support in grounding_metadata.grounding_supports:
                                        support_dict = {}
                                        if hasattr(support, 'segment'):
                                            segment = support.segment
                                            support_dict["text"] = segment.text if hasattr(segment, 'text') else ""
                                            support_dict["start_index"] = segment.start_index if hasattr(segment, 'start_index') else 0
                                            support_dict["end_index"] = segment.end_index if hasattr(segment, 'end_index') else 0
                                        if hasattr(support, 'grounding_chunk_indices'):
                                            support_dict["chunk_indices"] = list(support.grounding_chunk_indices) if support.grounding_chunk_indices else []
                                        grounding_supports.append(support_dict)
                                
                                # ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ
                                search_queries = []
                                if hasattr(grounding_metadata, 'web_search_queries'):
                                    search_queries = list(grounding_metadata.web_search_queries)
                                
                                result['citations'] = citations
                                result['grounding_supports'] = grounding_supports
                                result['search_queries'] = search_queries
                                print(f"ğŸ“š Citations: {len(citations)}ê°œ, Grounding Supports: {len(grounding_supports)}ê°œ, ê²€ìƒ‰ ì¿¼ë¦¬: {len(search_queries)}ê°œ")
                except Exception as e:
                    print(f"âš ï¸ Grounding metadata ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # URL Context metadata ì¶”ì¶œ
            if reference_urls and len(reference_urls) > 0:
                try:
                    if not enable_streaming and hasattr(response, 'candidates') and response.candidates:
                        url_context_metadata = response.candidates[0].url_context_metadata
                        if url_context_metadata:
                            url_metadata = []
                            if hasattr(url_context_metadata, 'url_metadata'):
                                for url_meta in url_context_metadata.url_metadata:
                                    url_metadata.append({
                                        'retrieved_url': url_meta.retrieved_url,
                                        'url_retrieval_status': url_meta.url_retrieval_status
                                    })
                            result['url_context_metadata'] = url_metadata
                            print(f"ğŸ”— URL Context: {len(url_metadata)}ê°œ URL ì²˜ë¦¬")
                except Exception as e:
                    print(f"âš ï¸ URL Context metadata ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # File Search citations ì¶”ì¶œ
            if file_search_store_names:
                try:
                    if not enable_streaming and hasattr(response, 'candidates') and response.candidates:
                        grounding_metadata = response.candidates[0].grounding_metadata
                        if grounding_metadata and hasattr(grounding_metadata, 'grounding_chunks'):
                            file_citations = []
                            for chunk in grounding_metadata.grounding_chunks:
                                if hasattr(chunk, 'file'):
                                    file_citations.append({
                                        'file_uri': chunk.file.file_uri,
                                        'display_name': chunk.file.display_name
                                    })
                            if file_citations:
                                result['file_citations'] = file_citations
                                print(f"ğŸ“š File Search Citations: {len(file_citations)}ê°œ")
                except Exception as e:
                    print(f"âš ï¸ File Search citations ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # Google Maps metadata ì¶”ì¶œ
            if use_google_maps:
                try:
                    if not enable_streaming and hasattr(response, 'candidates') and response.candidates:
                        grounding_metadata = response.candidates[0].grounding_metadata
                        if grounding_metadata:
                            # Maps citations ì¶”ì¶œ
                            maps_citations = []
                            if hasattr(grounding_metadata, 'grounding_chunks'):
                                for chunk in grounding_metadata.grounding_chunks:
                                    if hasattr(chunk, 'maps'):
                                        maps_citations.append({
                                            'uri': chunk.maps.uri,
                                            'title': chunk.maps.title,
                                            'place_id': chunk.maps.place_id if hasattr(chunk.maps, 'place_id') else None
                                        })
                            
                            # Widget token ì¶”ì¶œ
                            widget_token = None
                            if hasattr(grounding_metadata, 'google_maps_widget_context_token'):
                                widget_token = grounding_metadata.google_maps_widget_context_token
                            
                            if maps_citations:
                                result['maps_citations'] = maps_citations
                                print(f"ğŸ—ºï¸ Maps Citations: {len(maps_citations)}ê°œ")
                            
                            if widget_token:
                                result['google_maps_widget_token'] = widget_token
                                print(f"ğŸ—ºï¸ Google Maps Widget Token ì¶”ì¶œë¨")
                except Exception as e:
                    print(f"âš ï¸ Maps metadata ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # ëª¨ë“  citations í†µí•©
            all_citations = []
            
            # Google Search tool citations
            if result.get('citations'):
                for cit in result['citations']:
                    cit['source_type'] = 'google_search'
                    all_citations.append(cit)
            
            # Custom Search API citations
            if web_search_citations:
                for cit in web_search_citations:
                    cit['source_type'] = 'custom_search'
                    all_citations.append(cit)
            
            # File Search citations
            if result.get('file_citations'):
                for cit in result['file_citations']:
                    cit['source_type'] = 'file_search'
                    # file_urië¥¼ urië¡œ ë³€í™˜
                    if 'file_uri' in cit:
                        cit['uri'] = cit.pop('file_uri')
                    all_citations.append(cit)
            
            # Maps citations
            if result.get('maps_citations'):
                for cit in result['maps_citations']:
                    cit['source_type'] = 'google_maps'
                    all_citations.append(cit)
            
            # URL Context citations
            if result.get('url_context_metadata'):
                for url_meta in result['url_context_metadata']:
                    if url_meta.get('retrieved_url'):
                        all_citations.append({
                            'uri': url_meta['retrieved_url'],
                            'title': url_meta.get('retrieved_url', 'URL'),
                            'source_type': 'url_context',
                            'status': url_meta.get('url_retrieval_status', 'unknown')
                        })
            
            # í†µí•©ëœ citations ì €ì¥
            if all_citations:
                result['all_citations'] = all_citations
                print(f"ğŸ“š í†µí•© Citations: {len(all_citations)}ê°œ")
            
            # ì¸ë¼ì¸ ì¸ìš© ì¶”ê°€ (Google Search toolì˜ grounding supports ì‚¬ìš©)
            if result.get('grounding_supports') and result.get('citations'):
                try:
                    from maps_grounding_helper import format_grounding_supports_for_display
                    # Google Search citationsë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    sources_for_inline = []
                    for cit in result['citations']:
                        sources_for_inline.append({
                            'title': cit.get('title', 'Unknown'),
                            'uri': cit.get('uri', '')
                        })
                    
                    # ì¸ë¼ì¸ ì¸ìš©ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ìƒì„±
                    analysis_with_citations = format_grounding_supports_for_display(
                        text=analysis_text,
                        grounding_supports=result['grounding_supports'],
                        sources=sources_for_inline
                    )
                    
                    # ì¸ë¼ì¸ ì¸ìš©ì´ ì¶”ê°€ëœ ê²½ìš° ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if analysis_with_citations != analysis_text:
                        result['analysis'] = analysis_with_citations
                        result['has_inline_citations'] = True
                        print(f"ğŸ“ ì¸ë¼ì¸ ì¸ìš© ì¶”ê°€ë¨")
                except Exception as e:
                    print(f"âš ï¸ ì¸ë¼ì¸ ì¸ìš© ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            return result
            
        except ImportError:
            return {
                "success": False,
                "error": "google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-genaië¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"PDF ì§ì ‘ ì „ë‹¬ ë¶„ì„ ì˜¤ë¥˜: {str(e)}",
                "model": self._get_current_model_info(" (PDF Direct)"),
                "method": "PDF Direct (Failed)"
            }
    
    def _handle_function_calling_with_pdf(
        self,
        client,
        model: str,
        contents: List,
        config: Any,  # types.GenerateContentConfig
        function_implementations: Dict[str, Callable],
        automatic_function_calling: bool,
        max_iterations: int,
        enable_streaming: bool,
        progress_callback,
        include_thoughts: bool,
        provider_config: Dict,
        model_name: str,
        block_id: str,
        use_files_api: bool
    ) -> Dict[str, Any]:
        """
        PDF ì§ì ‘ ì „ë‹¬ê³¼ Function Callingì„ í•¨ê»˜ ì²˜ë¦¬
        
        Compositional function calling ì§€ì› ë° Thought Signatures ì²˜ë¦¬
        """
        try:
            from google.genai import types
            
            conversation_history = contents.copy()
            iteration = 0
            function_calls_history = []
            thought_signatures = []  # Thought signatures ì €ì¥
            
            while iteration < max_iterations:
                iteration += 1
                print(f"ğŸ”„ Function calling ë°˜ë³µ {iteration}/{max_iterations}")
                
                if progress_callback:
                    progress_callback(f"ğŸ”„ Function calling ë°˜ë³µ {iteration}/{max_iterations}")
                
                # API í˜¸ì¶œ
                if enable_streaming and progress_callback:
                    response_stream = client.models.generate_content_stream(
                        model=model,
                        contents=conversation_history,
                        config=config
                    )
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
                    response_parts = []
                    for chunk in response_stream:
                        if hasattr(chunk, 'candidates') and chunk.candidates:
                            response_parts.append(chunk)
                    
                    # ë§ˆì§€ë§‰ chunkì—ì„œ response êµ¬ì„±
                    if response_parts:
                        response = response_parts[-1]
                    else:
                        break
                else:
                    response = client.models.generate_content(
                        model=model,
                        contents=conversation_history,
                        config=config
                    )
                
                # Function calls ì¶”ì¶œ
                function_calls = []
                thought_summary = ""
                analysis_text = ""
                
                if hasattr(response, 'candidates') and response.candidates:
                    for part in response.candidates[0].content.parts:
                        # Thought summaries ì²˜ë¦¬
                        if include_thoughts and hasattr(part, 'thought') and part.thought:
                            thought_summary += part.text + "\n"
                        # Function call ì²˜ë¦¬
                        elif hasattr(part, 'function_call') and part.function_call:
                            func_call = part.function_call
                            # func_call ì „ì²´ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            try:
                                # name ì¶”ì¶œ
                                try:
                                    func_name = func_call.name if hasattr(func_call, 'name') else str(func_call)
                                except (AttributeError, Exception) as name_error:
                                    print(f"[WARNING] func_call.name ì ‘ê·¼ ì‹¤íŒ¨: {name_error}, func_call íƒ€ì…: {type(func_call)}")
                                    func_name = "unknown_function"

                                # args ì¶”ì¶œ
                                try:
                                    if hasattr(func_call, 'args'):
                                        if hasattr(func_call.args, 'items'):
                                            args_value = dict(func_call.args)
                                        else:
                                            args_value = func_call.args
                                    else:
                                        args_value = {}
                                except (AttributeError, Exception) as args_error:
                                    print(f"[WARNING] func_call.args ì ‘ê·¼ ì‹¤íŒ¨: {args_error}, func_call íƒ€ì…: {type(func_call)}")
                                    args_value = {}

                                function_calls.append({
                                    'name': func_name,
                                    'args': args_value
                                })
                            except Exception as func_call_error:
                                print(f"[WARNING] func_call ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {func_call_error}, func_call íƒ€ì…: {type(func_call)}")
                                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            
                            # Thought signature ì¶”ì¶œ (Gemini 3 í•„ìˆ˜)
                            if hasattr(part, 'thought_signature') and part.thought_signature:
                                thought_signatures.append({
                                    'function_call': func_call,
                                    'signature': part.thought_signature
                                })
                                print(f"ğŸ” Thought signature ì¶”ì¶œ: {func_call.name}")
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
                        elif hasattr(part, 'text') and part.text:
                            analysis_text += part.text
                
                # Function callsê°€ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µ
                if not function_calls:
                    result = {
                        "success": True,
                        "analysis": analysis_text,
                        "model": f"{provider_config.get('display_name', model_name)} (PDF Direct + Function Calling)",
                        "method": "Gemini API Direct + PDF Native + Function Calling",
                        "block_id": block_id,
                        "pdf_method": "files_api" if use_files_api else "inline",
                        "function_calls": function_calls_history
                    }
                    
                    if include_thoughts and thought_summary:
                        result["thought_summary"] = thought_summary.strip()
                    
                    return result
                
                # Function calls ì‹¤í–‰
                function_responses = []
                for func_call in function_calls:
                    function_name = func_call['name']
                    arguments = func_call.get('args', {})
                    
                    print(f"ğŸ”§ Function í˜¸ì¶œ: {function_name}({arguments})")
                    
                    if automatic_function_calling and function_name in function_implementations:
                        try:
                            # í•¨ìˆ˜ ì‹¤í–‰
                            func_result = function_implementations[function_name](**arguments)
                            function_responses.append({
                                'name': function_name,
                                'response': func_result
                            })
                            function_calls_history.append({
                                'name': function_name,
                                'args': arguments,
                                'result': func_result
                            })
                        except Exception as e:
                            print(f"âš ï¸ Function ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                            function_responses.append({
                                'name': function_name,
                                'response': {'error': str(e)}
                            })
                
                # Function responsesë¥¼ conversation historyì— ì¶”ê°€
                # 1. Model response ì¶”ê°€ (thought signatures í¬í•¨)
                model_content_parts = []
                
                # Responseì—ì„œ ì›ë³¸ parts ê°€ì ¸ì˜¤ê¸° (thought signatures ë³´ì¡´)
                if hasattr(response, 'candidates') and response.candidates:
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'function_call') and part.function_call:
                            # ì›ë³¸ partë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (thought signature í¬í•¨)
                            model_content_parts.append(part)
                
                conversation_history.append(
                    types.Content(
                        role="model",
                        parts=model_content_parts
                    )
                )
                
                # 2. Function responses ì¶”ê°€
                function_response_parts = []
                for func_response in function_responses:
                    function_response_parts.append(
                        types.Part.from_function_response(
                            name=func_response['name'],
                            response=func_response['response']
                        )
                    )
                
                if function_response_parts:
                    conversation_history.append(
                        types.Content(
                            role="user",
                            parts=function_response_parts
                        )
                    )
            
            # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬
            return {
                "success": False,
                "error": f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜({max_iterations})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.",
                "model": f"{provider_config.get('display_name', model_name)} (PDF Direct + Function Calling)",
                "method": "PDF Direct + Function Calling (Max Iterations)",
                "block_id": block_id,
                "function_calls": function_calls_history
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Function calling ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                "model": f"{provider_config.get('display_name', model_name)} (PDF Direct + Function Calling)",
                "method": "PDF Direct + Function Calling (Failed)",
                "block_id": block_id
            }
    
    def _analyze_block_with_pdf_direct_wrapper(
        self,
        cot_context: str,
        block_info: Dict[str, Any],
        block_id: str,
        project_info: Optional[Dict[str, Any]],
        pdf_bytes: bytes,
        pdf_path: Optional[str],
        thinking_budget: Optional[int],
        temperature: Optional[float],
        enable_streaming: bool,
        progress_callback
    ) -> Dict[str, Any]:
        """
        PDF ì§ì ‘ ì „ë‹¬ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
        ëª¨ë“  í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ê³  PDFì™€ í•¨ê»˜ ì „ë‹¬
        """
        try:
            # ìµœì í™”ëœ thinking_budget ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if thinking_budget is None:
                current_provider = get_current_provider()
                provider_config = PROVIDER_CONFIG.get(current_provider, {})
                model_name = provider_config.get('model', '')
                thinking_budget = self._get_optimal_thinking_budget(block_id, block_info, model_name)
                if thinking_budget:
                    print(f"ğŸ§  ë¸”ë¡ë³„ ìµœì í™”ëœ Thinking Budget: {thinking_budget} (ë¸”ë¡: {block_id})")
            
            # ìµœì í™”ëœ temperature ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if temperature is None:
                temperature = self._get_optimal_temperature(block_id, block_info)
                print(f"ğŸŒ¡ï¸ ë¸”ë¡ë³„ ìµœì í™”ëœ Temperature: {temperature:.2f} (ë¸”ë¡: {block_id})")
            
            # PDF í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì • (PDF ì§ì ‘ ì „ë‹¬ì´ë¯€ë¡œ)
            pdf_text = ""
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (PDF í…ìŠ¤íŠ¸ ì—†ì´)
            formatted_prompt = self._format_prompt_template(block_info, cot_context, pdf_text)
            
            # File Search Store ì´ë¦„ ì¶”ì¶œ (project_infoì—ì„œ)
            file_search_store_names = None
            if isinstance(project_info, dict):
                file_search_store_names = project_info.get('file_search_store_names')
                if isinstance(file_search_store_names, str):
                    file_search_store_names = [file_search_store_names]
            
            # ì°¸ê³  URL ì¶”ì¶œ (project_infoì—ì„œ)
            reference_urls = None
            if isinstance(project_info, dict):
                reference_urls = project_info.get('reference_urls')
                if isinstance(reference_urls, str):
                    reference_urls = [reference_urls]
                elif reference_urls and len(reference_urls) > 20:
                    reference_urls = reference_urls[:20]  # ìµœëŒ€ 20ê°œ ì œí•œ
                    print(f"âš ï¸ URL ê°œìˆ˜ê°€ 20ê°œë¥¼ ì´ˆê³¼í•˜ì—¬ ì²˜ìŒ 20ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # Google Search ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            # ê¸°ì¡´ ì›¹ ê²€ìƒ‰ì„ Google Search toolë¡œ ëŒ€ì²´ (ì˜µì…˜)
            use_google_search = False
            if block_id and project_info:
                # ë¸”ë¡ë³„ë¡œ Google Search ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                # ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•œ ë¸”ë¡ì—ì„œ Google Search tool ì‚¬ìš©
                blocks_with_google_search = [
                    'phase1_candidate_evaluation',
                    'legal_analysis',
                    'feasibility_analysis',
                    'market_research_analysis',  # ì‹œì¥ ì¡°ì‚¬ ë¶„ì„
                    'business_model_development',  # ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ê°œë°œ
                    'revenue_model_design',  # ìˆ˜ìµ ëª¨ë¸ ì„¤ê³„
                    'competitive_analysis',  # ê²½ìŸ ë¶„ì„
                    'trend_analysis',  # íŠ¸ë Œë“œ ë¶„ì„
                    'benchmarking_analysis'  # ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„
                ]
                use_google_search = block_id in blocks_with_google_search
                
                # ê¸°ì¡´ ì›¹ ê²€ìƒ‰ì€ fallbackìœ¼ë¡œ ìœ ì§€ (Google Search tool ì‚¬ìš© ì‹œì—ë„ Custom Search APIë¡œ citations ìˆ˜ì§‘)
                web_search_context = ""
                web_search_citations = []
                try:
                    if not use_google_search:
                        # Google Search toolì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° Custom Search API ì‚¬ìš©
                        web_search_context = get_web_search_context(block_id, project_info, "")
                        if web_search_context:
                            print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ (CoT): {block_id}")
                    # Google Search tool ì‚¬ìš© ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ Custom Search APIë¡œ citations ìˆ˜ì§‘
                    if WEB_SEARCH_CITATIONS_AVAILABLE and get_web_search_citations:
                        web_search_citations = get_web_search_citations(block_id, project_info, "")
                        if web_search_citations:
                            print(f"ğŸ“š Custom Search API Citations: {len(web_search_citations)}ê°œ")
                except Exception as e:
                    print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            
            # Google Maps ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            # ìœ„ì¹˜ ê¸°ë°˜ ë¸”ë¡ì—ì„œ ìë™ í™œì„±í™”
            use_google_maps = False
            enable_maps_widget = False
            location_coordinates = None
            
            if block_id and project_info:
                # ìœ„ì¹˜ ê¸°ë°˜ ë¸”ë¡ ì‹ë³„
                location_based_blocks = [
                    'phase1_site_analysis',
                    'phase1_facility_program',
                    'phase1_candidate_evaluation',
                    'transportation_analysis',
                    'facility_analysis'
                ]
                
                # Gemini 3 ëª¨ë¸ì—ì„œëŠ” Google Maps ì‚¬ìš© ë¶ˆê°€
                current_provider = get_current_provider()
                provider_config = PROVIDER_CONFIG.get(current_provider, {})
                model_name = provider_config.get('model', '')
                clean_model = model_name.replace('models/', '').replace('model/', '')
                is_gemini_3 = 'gemini-3' in clean_model
                
                if block_id in location_based_blocks and not is_gemini_3:
                    use_google_maps = True
                    # ìœ„ì¹˜ ì¢Œí‘œ ì¶”ì¶œ
                    location_coordinates = self._extract_location_coordinates(project_info)
                    if location_coordinates:
                        print(f"ğŸ—ºï¸ ìœ„ì¹˜ ì¢Œí‘œ ì¶”ì¶œë¨: ({location_coordinates['latitude']}, {location_coordinates['longitude']})")
                    else:
                        print(f"âš ï¸ ìœ„ì¹˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ Google Mapsë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                        use_google_maps = False
            
            # ë¬¸ì„œ ê¸°ë°˜ ì¶”ë¡  ê°•ì¡° ì§€ì‹œì‚¬í•­ ì¶”ê°€ (PDF ì§ì ‘ ì „ë‹¬ìš©)
            document_based_instruction = f"""

## ğŸ“„ PDF ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ í•„ìˆ˜ ì§€ì‹œì‚¬í•­

**âš ï¸ ë§¤ìš° ì¤‘ìš”**: ì•„ë˜ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

### 1. PDF ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì¶”ë¡  í•„ìˆ˜
- **ì•„ë˜ì— ì œê³µëœ PDF ë¬¸ì„œë¥¼ ì •í™•íˆ ì½ê³  ì´í•´í•œ í›„ ë¶„ì„í•˜ì„¸ìš”**
- **PDF ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„ì— í™œìš©í•˜ì„¸ìš”**
- **PDFì˜ ì´ë¯¸ì§€, ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸, í…Œì´ë¸”ë„ ëª¨ë‘ ë¶„ì„ì— í¬í•¨í•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ í…œí”Œë¦¿ì´ë‚˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì´ ì•„ë‹Œ, ì´ íŠ¹ì • í”„ë¡œì íŠ¸ PDF ë¬¸ì„œì˜ ì‹¤ì œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”**

### 2. PDF ë¬¸ì„œ ì¸ìš© ë° ê·¼ê±° ì œì‹œ í•„ìˆ˜
- **ë¶„ì„ ê²°ê³¼ì˜ ëª¨ë“  ì£¼ìš” ì£¼ì¥ì€ PDF ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë’·ë°›ì¹¨í•˜ì„¸ìš”**
- **ì˜ˆì‹œ**: "PDF ë¬¸ì„œì˜ Xí˜ì´ì§€ì— 'ëŒ€ì§€ë©´ì  5,000ã¡'ë¼ê³  ëª…ì‹œë˜ì–´ ìˆì–´..." í˜•ì‹ìœ¼ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
- **ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ì„ ì œì‹œí•  ë•ŒëŠ” ë°˜ë“œì‹œ PDF ë¬¸ì„œì˜ ì¶œì²˜(í˜ì´ì§€ ë²ˆí˜¸ ë“±)ë¥¼ ëª…ì‹œí•˜ì„¸ìš”**

### 3. PDF ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ìƒì„±í•˜ì§€ ë§ ê²ƒ
- **PDF ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”**
- **ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 'PDF ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ë˜ëŠ” 'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¡œ í‘œì‹œí•˜ì„¸ìš”**
- **ì¼ë°˜ì ì¸ ê±´ì¶• í”„ë¡œì íŠ¸ì˜ ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”**

### 4. PDF ë¬¸ì„œ ë‚´ìš©ì˜ êµ¬ì²´ì  í™œìš©
- **PDF ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ëª…ì¹­, ìœ„ì¹˜, ê·œëª¨ ë“±ì„ ë¶„ì„ì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”**
- **PDFì˜ ë ˆì´ì•„ì›ƒ, êµ¬ì¡°, ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸ë¥¼ ì´í•´í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”**
- **PDFì˜ ì•”ì‹œì  ì˜ë¯¸ë‚˜ ì—°ê´€ëœ ìš”êµ¬ì‚¬í•­ì„ ì¶”ë¡ í•˜ì—¬ ë¶„ì„ì„ í’ë¶€í•˜ê²Œ ë§Œë“¤ë˜, ì¶”ë¡ ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”**

**ìœ„ ì§€ì‹œì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶„ì„ì´ ë°˜ë³µë˜ê±°ë‚˜ ì¼ë°˜ë¡ ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ì•„ë˜ PDF ë¬¸ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.**
"""
            
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            if web_search_context:
                formatted_prompt = f"""{formatted_prompt}

{web_search_context}

**ì¤‘ìš”**: ìœ„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì‹  ì •ë³´ì™€ ì‹œì¥ ë™í–¥ì„ ë°˜ì˜í•œ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ë‹¨, ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” PDF ë¬¸ì„œ ë‚´ìš©ì„ ë³´ì™„í•˜ëŠ” ì—­í• ì´ë©°, ë¶„ì„ì˜ ì£¼ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì— ì œê³µëœ PDF ë¬¸ì„œ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–»ì€ ì •ë³´ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³ , PDF ë¬¸ì„œ ë‚´ìš©ê³¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.

{document_based_instruction}
"""
            else:
                formatted_prompt = f"""{formatted_prompt}{document_based_instruction}"""
            
            # í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì¶”ê°€
            blocks_with_builtin_cot = ['phase1_facility_program']
            extended_thinking_note = ""
            if block_id and block_id not in blocks_with_builtin_cot:
                extended_thinking_note = self._get_extended_thinking_template()
            
            # CoT ì»¨í…ìŠ¤íŠ¸ì™€ ë¸”ë¡ í”„ë¡¬í”„íŠ¸ ê²°í•©
            enhanced_prompt = f"""
{cot_context}

## ğŸ¯ ë¸”ë¡ë³„ ë¶„ì„ ì§€ì‹œì‚¬í•­ (í•µì‹¬)

**ì•„ë˜ ë¸”ë¡ì˜ êµ¬ì²´ì ì¸ ì—­í• , ì§€ì‹œì‚¬í•­, ë‹¨ê³„ë¥¼ ì •í™•íˆ ë”°ë¼ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.**
**ì´ ë¸”ë¡ì˜ ë‚´ìš©ì´ ì´ë²ˆ ë¶„ì„ì˜ ì£¼ìš” ë°©í–¥ê³¼ ëª©í‘œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.**

{formatted_prompt}{extended_thinking_note}

{self._get_output_format_template()}
"""
            
            # System Instruction ìƒì„±
            system_instruction = self._build_system_instruction(block_info)
            
            # PDF ì§ì ‘ ì „ë‹¬ ë¶„ì„ ì‹¤í–‰
            return self._analyze_block_with_pdf_direct(
                enhanced_prompt=enhanced_prompt,
                pdf_bytes=pdf_bytes,
                pdf_path=pdf_path,
                block_info=block_info,
                block_id=block_id,
                system_instruction=system_instruction,
                thinking_budget=thinking_budget,
                temperature=temperature,
                enable_streaming=enable_streaming,
                progress_callback=progress_callback,
                thinking_level=None,  # ê¸°ë³¸ê°’, í•„ìš”ì‹œ ì¶”ê°€ ê°€ëŠ¥
                include_thoughts=False,  # ê¸°ë³¸ê°’, í•„ìš”ì‹œ ì¶”ê°€ ê°€ëŠ¥
                file_search_store_names=file_search_store_names,
                reference_urls=reference_urls,
                use_google_search=use_google_search,
                use_google_maps=use_google_maps,
                enable_maps_widget=enable_maps_widget,
                location_coordinates=location_coordinates,
                web_search_citations=web_search_citations
            )
            
        except Exception as e:
            print(f"âš ï¸ PDF ì§ì ‘ ì „ë‹¬ ë˜í¼ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ì „í™˜
            return self._analyze_block_with_cot_context(
                cot_context, block_info, block_id, project_info,
                thinking_budget, temperature, enable_streaming, progress_callback,
                use_pdf_direct=False  # ì¬ê·€ ë°©ì§€
            )
    
    def _build_system_instruction(self, block_info: Dict[str, Any]) -> str:
        """ë¸”ë¡ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ System Instruction ìƒì„±"""
        role = block_info.get('role', 'ê±´ì¶• í”„ë¡œì íŠ¸ ë¶„ì„ ì „ë¬¸ê°€')
        instructions = block_info.get('instructions', '')
        end_goal = block_info.get('end_goal', '')
        
        system_instruction = f"""ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤.

{instructions}

ìµœì¢… ëª©í‘œ: {end_goal}

ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
1. ë¬¸ì„œì— ëª…ì‹œëœ ì‚¬ì‹¤ê³¼ ìˆ˜ì¹˜ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
2. ì¶”ë¡  ê³¼ì •ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš” (Chain of Thought ë°©ì‹)
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‹¨ìœ„, ì‚°ì • ê·¼ê±°ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ë˜ëŠ” 'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¡œ í‘œì‹œí•˜ì„¸ìš”
5. ì¼ë°˜ë¡ ì ì¸ ë‚´ìš©ë³´ë‹¤ëŠ” ì´ íŠ¹ì • í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
"""
        return system_instruction.strip()
    
    def _get_optimal_thinking_budget(self, block_id: str, block_info: Dict[str, Any], model_name: str = "") -> Optional[int]:
        """ë¸”ë¡ì˜ ë³µì¡ë„ì™€ ìœ í˜•ì— ë”°ë¼ ìµœì í™”ëœ thinking_budget ê³„ì‚°"""
        # ë¸”ë¡ ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ thinking_budget ë§¤í•‘
        THINKING_BUDGET_MAP = {
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ: ë‚®ì€ thinking
            'basic_info': 1024,
            'phase1_data_inventory': 1024,
            
            # ìš”êµ¬ì‚¬í•­ ë¶„ì„: ì¤‘ê°„ thinking
            'requirements_analysis': 4096,
            'phase1_requirements_structuring': 4096,
            'accessibility_analysis': 4096,
            
            # ë³µì¡í•œ ë¶„ì„: ë†’ì€ thinking
            'legal_analysis': 8192,
            'feasibility_analysis': 16384,
            'capacity_analysis': 16384,
            'phase1_facility_program': 8192,
            'phase1_facility_area_calculation': 8192,
            'phase1_candidate_generation': 12288,
            'phase1_candidate_evaluation': 16384,
            
            # ë„ì‹œì¬ê°œë°œ ì‚¬íšŒê²½ì œì  ì˜í–¥ ë¶„ì„: ë§¤ìš° ë†’ì€ thinking
            'ë„ì‹œì¬ê°œë°œì‚¬íšŒê²½ì œì ì˜í–¥ë¶„ì„': 16384,
        }
        
        # ë¸”ë¡ IDë¡œ ì§ì ‘ ë§¤í•‘
        if block_id in THINKING_BUDGET_MAP:
            return THINKING_BUDGET_MAP[block_id]
        
        # ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘ ì‹œë„
        category = block_info.get('category', '').lower()
        if 'ê¸°ë³¸' in category or 'ì •ë³´' in category:
            return 1024
        elif 'ìš”êµ¬ì‚¬í•­' in category or 'ì ‘ê·¼ì„±' in category:
            return 4096
        elif 'ë²•ê·œ' in category or 'ë²•ì ' in category:
            return 8192
        elif 'ìˆ˜ìš©ë ¥' in category or 'íƒ€ë‹¹ì„±' in category or 'ë³µí•©' in category:
            return 16384
        
        # ë¸”ë¡ì˜ steps ìˆ˜ë¡œ ë³µì¡ë„ ì¶”ì •
        steps = block_info.get('steps', [])
        if len(steps) <= 3:
            return 2048  # ë‹¨ìˆœí•œ ë¶„ì„
        elif len(steps) <= 5:
            return 4096  # ì¤‘ê°„ ë³µì¡ë„
        elif len(steps) <= 7:
            return 8192  # ë³µì¡í•œ ë¶„ì„
        else:
            return 12288  # ë§¤ìš° ë³µì¡í•œ ë¶„ì„
        
        # ê¸°ë³¸ê°’: None (ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©)
        return None
    
    def _get_optimal_temperature(self, block_id: str, block_info: Dict[str, Any]) -> float:
        """ë¸”ë¡ ìœ í˜•ì— ë”°ë¼ ìµœì í™”ëœ temperature ê³„ì‚°"""
        # ë¸”ë¡ë³„ temperature ë§¤í•‘
        TEMPERATURE_MAP = {
            # ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„: ë‚®ì€ temperature
            'basic_info': 0.1,
            'phase1_data_inventory': 0.1,
            'legal_analysis': 0.2,
            'phase1_facility_area_calculation': 0.2,
            
            # ì¼ë°˜ ë¶„ì„: ì¤‘ê°„ temperature
            'requirements_analysis': 0.3,
            'phase1_requirements_structuring': 0.3,
            'accessibility_analysis': 0.3,
            'phase1_facility_program': 0.4,
            'phase1_facility_area_reference': 0.3,
            
            # ì°½ì˜ì  ë¶„ì„: ë†’ì€ temperature
            'phase1_candidate_generation': 0.7,
            'phase1_candidate_evaluation': 0.6,
            'feasibility_analysis': 0.5,
            'capacity_analysis': 0.5,
            'ë„ì‹œì¬ê°œë°œì‚¬íšŒê²½ì œì ì˜í–¥ë¶„ì„': 0.6,
        }
        
        # ë¸”ë¡ IDë¡œ ì§ì ‘ ë§¤í•‘
        if block_id in TEMPERATURE_MAP:
            return TEMPERATURE_MAP[block_id]
        
        # ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘ ì‹œë„
        category = block_info.get('category', '').lower()
        name = block_info.get('name', '').lower()
        
        # ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„
        if any(keyword in category or keyword in name for keyword in ['ê¸°ë³¸', 'ì •ë³´', 'ë²•ê·œ', 'ë²•ì ', 'ê³„ì‚°', 'ìˆ˜ì¹˜']):
            return 0.2
        
        # ì°½ì˜ì  ë¶„ì„
        if any(keyword in category or keyword in name for keyword in ['í›„ë³´', 'ì œì•ˆ', 'ìƒì„±', 'ì•„ì´ë””ì–´', 'ì°½ì˜', 'ëŒ€ì•ˆ']):
            return 0.7
        
        # ë³µí•© ë¶„ì„
        if any(keyword in category or keyword in name for keyword in ['íƒ€ë‹¹ì„±', 'ìˆ˜ìš©ë ¥', 'ì˜í–¥', 'ë³µí•©', 'ì¢…í•©']):
            return 0.5
        
        # ê¸°ë³¸ê°’: ì¤‘ê°„ temperature
        return 0.3
    
    def _extract_key_insights(self, analysis_text, max_length=200):
        """ë¶„ì„ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë¡œì§
            # "í•µì‹¬", "ì£¼ìš”", "ì¤‘ìš”", "ê²°ë¡ " ë“±ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            import re
            
            # í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì°¾ê¸°
            key_patterns = [
                r'í•µì‹¬[^.]*[.]',
                r'ì£¼ìš”[^.]*[.]',
                r'ì¤‘ìš”[^.]*[.]',
                r'ê²°ë¡ [^.]*[.]',
                r'ë°œê²¬[^.]*[.]',
                r'ì¸ì‚¬ì´íŠ¸[^.]*[.]'
            ]
            
            insights = []
            for pattern in key_patterns:
                matches = re.findall(pattern, analysis_text)
                insights.extend(matches[:2])  # íŒ¨í„´ë‹¹ ìµœëŒ€ 2ê°œ
            
            # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
            unique_insights = []
            for insight in insights:
                if insight not in unique_insights and len(insight) <= max_length:
                    unique_insights.append(insight)
            
            # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ë°˜í™˜
            return unique_insights[:3]
            
        except:
            # ì˜¤ë¥˜ ì‹œ ê°„ë‹¨íˆ ì•ë¶€ë¶„ ë°˜í™˜
            return [analysis_text[:max_length] + "..."] if analysis_text else []
    
    def batch_analyze_blocks(self, projects: List[Dict[str, Any]], block_ids: List[str], 
                           block_infos: Dict[str, Dict], progress_callback=None):
        """
        ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            projects: í”„ë¡œì íŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'project_info': {...}, 'pdf_text': '...'}, ...]
            block_ids: ë¶„ì„í•  ë¸”ë¡ ID ë¦¬ìŠ¤íŠ¸
            block_infos: ë¸”ë¡ ì •ë³´ ë”•ì…”ë„ˆë¦¬
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
        
        Returns:
            ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import concurrent.futures
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        try:
            total_tasks = len(projects) * len(block_ids)
            completed_tasks = 0
            batch_results = {}
            
            print(f"ğŸ”„ ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(projects)}ê°œ í”„ë¡œì íŠ¸ Ã— {len(block_ids)}ê°œ ë¸”ë¡ = {total_tasks}ê°œ ì‘ì—…")
            if progress_callback:
                progress_callback(f"ğŸ”„ ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {total_tasks}ê°œ ì‘ì—…")
            
            # ê° í”„ë¡œì íŠ¸ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³€ê²½ ê°€ëŠ¥í•˜ì§€ë§Œ, API ì œí•œ ê³ ë ¤)
            for project_idx, project_data in enumerate(projects):
                project_info = project_data.get('project_info', {})
                pdf_text = project_data.get('pdf_text', '')
                project_name = project_info.get('project_name', f'í”„ë¡œì íŠ¸ {project_idx + 1}')
                
                print(f"ğŸ“Š í”„ë¡œì íŠ¸ {project_idx + 1}/{len(projects)}: {project_name}")
                if progress_callback:
                    progress_callback(f"ğŸ“Š í”„ë¡œì íŠ¸ {project_idx + 1}/{len(projects)}: {project_name}")
                
                project_results = {}
                
                # ë¸”ë¡ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬ (ë™ì¼ í”„ë¡œì íŠ¸ ë‚´ì—ì„œëŠ” ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)
                for block_idx, block_id in enumerate(block_ids):
                    block_name = block_infos.get(block_id, {}).get('name', block_id)
                    
                    print(f"  ğŸ“‹ ë¸”ë¡ {block_idx + 1}/{len(block_ids)}: {block_name}")
                    if progress_callback:
                        progress_callback(f"  ğŸ“‹ ë¸”ë¡ {block_idx + 1}/{len(block_ids)}: {block_name}")
                    
                    # ë¸”ë¡ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    block_info = block_infos.get(block_id)
                    if not block_info:
                        print(f"  âŒ ë¸”ë¡ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {block_id}")
                        continue
                    
                    # ë¸”ë¡ ë¶„ì„ ìˆ˜í–‰
                    try:
                        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
                        formatted_prompt = self._format_prompt_template(
                            block_info, ""
                        )
                        
                        # PDF í…ìŠ¤íŠ¸ ì¹˜í™˜
                        if "{pdf_text}" in formatted_prompt:
                            formatted_prompt = formatted_prompt.replace(
                                "{pdf_text}", 
                                pdf_text[:4000] if pdf_text else "PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                            )
                        
                        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                        web_search_context = ""
                        try:
                            web_search_context = get_web_search_context(block_id, project_info, pdf_text)
                        except Exception as e:
                            print(f"  âš ï¸ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
                        
                        # í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì¶”ê°€ (ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ ì ìš©)
                        # ë¸”ë¡ í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ Chain of Thought ì§€ì‹œì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ë¸”ë¡ ëª©ë¡
                        # (ì´ ë¸”ë¡ë“¤ì€ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ë ˆë²¨ ì§€ì‹œì‚¬í•­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
                        blocks_with_builtin_cot = ['phase1_facility_program']
                        
                        # ëª¨ë“  ë¸”ë¡ì— ê¸°ë³¸ì ìœ¼ë¡œ í™•ì¥ ì‚¬ê³  ì§€ì‹œì‚¬í•­ ì ìš© (ì¤‘ë³µ ë°©ì§€ ì œì™¸)
                        extended_thinking_note = ""
                        if block_id and block_id not in blocks_with_builtin_cot:
                            # ì‹œìŠ¤í…œ ë ˆë²¨ í™•ì¥ ì‚¬ê³  í…œí”Œë¦¿ ì‚¬ìš©
                            extended_thinking_note = self._get_extended_thinking_template()
                        
                        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                        enhanced_prompt = f"""
{formatted_prompt}
{web_search_context if web_search_context else ""}
{extended_thinking_note}

{self._get_output_format_template()}
"""
                        
                        # Signature ì„ íƒ (ë™ì  ìƒì„±)
                        signature_map = self._build_signature_map()
                        signature_class = signature_map.get(block_id, SimpleAnalysisSignature)
                        
                        # DSPy ë¶„ì„ ìˆ˜í–‰
                        with self._lm_context():
                            result = dspy.Predict(signature_class)(input=enhanced_prompt)
                        
                        project_results[block_id] = {
                            'success': True,
                            'analysis': result.output,
                            'block_name': block_name
                        }
                        
                        completed_tasks += 1
                        print(f"  âœ… {block_name} ì™„ë£Œ ({completed_tasks}/{total_tasks})")
                        if progress_callback:
                            progress = completed_tasks / total_tasks
                            progress_callback(f"  âœ… {block_name} ì™„ë£Œ ({completed_tasks}/{total_tasks})")
                    
                    except Exception as e:
                        print(f"  âŒ {block_name} ì‹¤íŒ¨: {e}")
                        project_results[block_id] = {
                            'success': False,
                            'error': str(e),
                            'block_name': block_name
                        }
                        completed_tasks += 1
                    
                    # API í˜¸ì¶œ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                    time.sleep(0.5)
                
                batch_results[project_name] = project_results
            
            print(f"ğŸ‰ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {completed_tasks}/{total_tasks}ê°œ ì‘ì—… ì™„ë£Œ")
            if progress_callback:
                progress_callback(f"ğŸ‰ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {completed_tasks}/{total_tasks}ê°œ ì‘ì—… ì™„ë£Œ")
            
            return {
                "success": True,
                "batch_results": batch_results,
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "model": self._get_current_model_info(" (DSPy + Batch)"),
                "method": "Batch Processing"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self._get_current_model_info(" (DSPy)"),
                "method": "Batch Processing"
            }